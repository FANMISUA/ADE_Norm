{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOMq7buPEWSRlRCyRvoA9j+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"u4Ymm56BAtwR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722754418563,"user_tz":300,"elapsed":12800,"user":{"displayName":"F D","userId":"09431385063825136827"}},"outputId":"0f42ffa7-f478-4264-9807-5b1bf5b38aaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","<bound method NDFrame.head of                             ade  soc_code\n","0                           ade      <NA>\n","1                     allergies  10021428\n","2               HURT YOUR Liver  10019805\n","3                            AD  10037175\n","4                         focus  10029205\n","...                         ...       ...\n","1707                     orgasm  10037175\n","1708  never have another orgasm  10037175\n","1709                       coma  10029205\n","1710        gain so much weight  10022891\n","1711         increase my weight  10022891\n","\n","[1712 rows x 2 columns]>\n","(1712, 2)\n","(1107, 2)\n","SOC count in CADEC:  soc_code\n","10037175    287\n","10018065    235\n","10029205    212\n","10017947     63\n","10028395     58\n","10022891     54\n","10027433     48\n","10040785     28\n","10038738     22\n","10022117     16\n","10015919     16\n","10038604     10\n","10047065     10\n","10021428      8\n","10007541      7\n","10041244      7\n","10038359      6\n","10021881      5\n","10013993      4\n","10019805      2\n","10042613      2\n","10029104      2\n","10010331      1\n","10077536      1\n","0             1\n","10014698      1\n","Name: count, dtype: Int64\n","CADEC top3 in SMM4H:                             ade  soc_code\n","3                            AD  10037175\n","4                         focus  10029205\n","5                          died  10018065\n","8                        dreams  10037175\n","10                   withdrawal  10018065\n","...                         ...       ...\n","1695       talk a mile a minute  10037175\n","1698     can't go back to sleep  10037175\n","1703                 chest hurt  10018065\n","1704   got ten minutes of sleep  10037175\n","1708  never have another orgasm  10037175\n","\n","[734 rows x 2 columns]\n","                            ade  soc_code  label\n","3                            AD  10037175      0\n","4                         focus  10029205      2\n","5                          died  10018065      1\n","8                        dreams  10037175      0\n","10                   withdrawal  10018065      1\n","...                         ...       ...    ...\n","1695       talk a mile a minute  10037175      0\n","1698     can't go back to sleep  10037175      0\n","1703                 chest hurt  10018065      1\n","1704   got ten minutes of sleep  10037175      0\n","1708  never have another orgasm  10037175      0\n","\n","[734 rows x 3 columns]\n"]}],"source":["#install packages\n","!pip install transformers\n","#import packages\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from tqdm.notebook import tqdm\n","\n","from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset\n","\n","from transformers import BertForSequenceClassification\n","\n","from sklearn.metrics import f1_score\n","\n","from sklearn.model_selection import train_test_split\n","\n","import numpy as np\n","import random\n","\n","#Read smm4h data from git:\n","#https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\n","# URL of the CSV file\n","# csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n","csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n","\n","# Read the CSV file into a pandas DataFrame\n","column_names = [\"ade\", \"soc_code\"]\n","smm4h_all = pd.read_csv(csv_url,names=column_names, sep = '\\t', header=None)\n","\n","smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n","smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n","\n","print(smm4h_all.head)\n","# Display the first few rows of the DataFrame\n","print(smm4h_all.shape)\n","\n","\n","# Remove duplicate rows based on the 'ade' column\n","# smm4h_unique = smm4h_all\n","smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n","\n","\n","# Display the resulting DataFrame\n","print(smm4h_unique.shape)\n","# Count occurrences of each 'soc_code'\n","soc_code_counts = smm4h_unique['soc_code'].value_counts()\n","# Sort the counts from high to low and print the result\n","print(\"SOC count in CADEC: \",soc_code_counts)\n","\n","#get top 3 of the SMM4H list\n","#['10037175','10018065','10029205','10017947''10028395','10022891']\n","# top6SMM4H = [10018065,10037175,10029205,10022891,10028395,10017947]\n","top3SMM4H = [10037175, 10018065,10029205]\n","# top3SMM4H = ['10018065', '10037175', '10029205']\n","\n","top3label_dict = {\n","    'Label': [0, 1, 2],\n","    'soc_code': [10037175,10018065,10029205]\n","}\n","top3label_dict = {\n","    10037175: 0,\n","    10018065: 1,\n","    10029205: 2\n","}\n","\n","\n","# Filter DataFrame\n","filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(top3SMM4H)]\n","# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n","\n","# Select only the Term and SOC columns\n","top3inSMM4H = filtered_data3[['ade', 'soc_code']]\n","# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n","\n","print(\"CADEC top3 in SMM4H:\",top3inSMM4H)\n","data = top3inSMM4H\n","# Convert to DataFrame\n","df = pd.DataFrame(top3inSMM4H)\n","\n","#mapping dictionary for soc to label\n","# soc_code_to_label = dict(zip(top3label_dict['soc_code'], top3label_dict['Label']))\n","df['label'] = df['soc_code'].map(top3label_dict)\n","\n","# Replace `soc_code` values with corresponding labels\n","# df['label'] = df['soc_code'].replace(soc_code_to_label)\n","# print(top3label_dict)\n","print(df)\n","\n","\n","def custom_train_test_split(X, y, test_size=0.2, random_state=None):\n","    classes, counts = np.unique(y, return_counts=True)\n","    min_class_count = min(counts)\n","\n","    # Find classes with only one or two instances\n","    single_or_double_instance_classes = classes[np.logical_or(counts == 1, counts == 2)]\n","\n","    # Remove instances of single-instance or two-instance classes\n","    X_filtered = X[~np.isin(y, single_or_double_instance_classes)]\n","    y_filtered = y[~np.isin(y, single_or_double_instance_classes)]\n","\n","    if len(y_filtered) < 2:\n","        raise ValueError(\"No classes have more than two instances after filtering.\")\n","\n","    # Perform stratified split on the filtered dataset\n","    X_train, X_val, y_train, y_val = train_test_split(X_filtered, y_filtered, test_size=test_size, random_state=random_state, stratify=y_filtered)\n","\n","    # Randomly assign instances of single-instance classes to training or testing sets\n","    for class_label in single_or_double_instance_classes:\n","        class_indices = np.where(y == class_label)[0]\n","        np.random.shuffle(class_indices)\n","\n","        if len(class_indices) <= 2:\n","            # Only one instance, randomly assign to training or testing set\n","            if np.random.rand() < test_size:\n","                X_val = np.concatenate((X_val, X[class_indices]))\n","                y_val = np.concatenate((y_val, y[class_indices]))\n","            else:\n","                X_train = np.concatenate((X_train, X[class_indices]))\n","                y_train = np.concatenate((y_train, y[class_indices]))\n","\n","    return X_train, X_val, y_train, y_val\n","\n","\n","#evaluation\n","from sklearn.metrics import f1_score\n","\n","def f1_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, preds_flat, average='weighted')\n","\n","\n","def accuracy_per_class(predictions, true_vals):\n","    pred_flat = np.argmax(predictions, axis=1).flatten()\n","    labels_flat = true_vals.flatten()\n","\n","    accuracy_dict = {}\n","    count_dict = {}\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = pred_flat[labels_flat == label]\n","        y_true = labels_flat[labels_flat == label]\n","        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n","        count_dict[label] = len(y_true)\n","\n","    return accuracy_dict, count_dict"]},{"cell_type":"code","source":["import logging\n","import random\n","import numpy as np\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from sklearn.metrics import f1_score as f1_score_func\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","# Configure logging\n","logging.basicConfig(filename='smm4h_top3_20times_training_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger()\n","\n","class TQDMLoggingWrapper(tqdm):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.logger = logger\n","\n","    def display(self, msg=None, pos=None):\n","        if msg is not None:\n","            self.logger.info(msg)\n","        super().display(msg, pos)\n","\n","    def update(self, n=1):\n","        super().update(n)\n","        desc = self.format_dict.get('desc', 'No description')\n","        postfix = self.format_dict.get('postfix', '')\n","        self.logger.info(f'{desc} - {postfix}')\n","\n","    def set_description(self, desc=None, refresh=True):\n","        super().set_description(desc, refresh)\n","        if desc:\n","            self.logger.info(f'Set description: {desc}')\n","\n","\n","# Define the random seeds and other parameters\n","seed_values = list(range(2, 4, 2))\n","# batch_size = 8\n","# epochs = 10\n","# Define parameter grid\n","learning_rates = [1e-5, 1e-4, 1e-3]\n","batch_sizes = [4, 8, 32, 64]\n","epochs_list = [5, 10, 15, 20]\n","\n","# Results storage\n","results = []\n","\n","# Placeholder for accuracies\n","all_accuracies = {label: [] for label in range(len(top3label_dict))}\n","\n","# Function to evaluate the model\n","def evaluate(dataloader_val):\n","    model.eval()\n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","\n","    for batch in dataloader_val:\n","        batch = tuple(b.to(device) for b in batch)\n","        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","\n","    loss_val_avg = loss_val_total / len(dataloader_val)\n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","\n","    return loss_val_avg, predictions, true_vals\n","\n","# Main loop over seed values\n","for seed_val in seed_values:\n","  # Set seeds\n","  random.seed(seed_val)\n","  np.random.seed(seed_val)\n","  torch.manual_seed(seed_val)\n","  torch.cuda.manual_seed_all(seed_val)\n","\n","  # Data preparation\n","  X_train, X_val, y_train, y_val = custom_train_test_split(df.index.values, df.label.values, test_size=0.2, random_state=seed_val)\n","  df['data_type'] = ['not_set'] * df.shape[0]\n","  df.loc[X_train, 'data_type'] = 'train'\n","  df.loc[X_val, 'data_type'] = 'val'\n","  # logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n","\n","  # Training loop for grid search\n","  for lr in learning_rates:\n","      for batch_size in batch_sizes:\n","          for epochs in epochs_list:\n","            logger.info(f\"Seed: {seed_val}, Learning Rate: {lr}, Batch Size: {batch_size}, Epochs: {epochs}\")\n","            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","            encoded_data_train = tokenizer.batch_encode_plus(\n","                df[df.data_type == 'train'].ade.values,\n","                add_special_tokens=True,\n","                return_attention_mask=True,\n","                pad_to_max_length=True,\n","                max_length=256,\n","                return_tensors='pt'\n","            )\n","\n","            encoded_data_val = tokenizer.batch_encode_plus(\n","                df[df.data_type == 'val'].ade.values,\n","                add_special_tokens=True,\n","                return_attention_mask=True,\n","                pad_to_max_length=True,\n","                max_length=256,\n","                return_tensors='pt'\n","            )\n","\n","            input_ids_train = encoded_data_train['input_ids']\n","            attention_masks_train = encoded_data_train['attention_mask']\n","            labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n","\n","            input_ids_val = encoded_data_val['input_ids']\n","            attention_masks_val = encoded_data_val['attention_mask']\n","            labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n","\n","            dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","            dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n","\n","            model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top3label_dict), output_attentions=False, output_hidden_states=False)\n","\n","            dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n","            dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n","\n","            optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n","            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n","\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            model.to(device)\n","            logger.info(f\"Device used: {device}\")\n","\n","            # Training loop\n","            for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n","                model.train()\n","                loss_train_total = 0\n","\n","                progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n","                for batch in progress_bar:\n","                    model.zero_grad()\n","                    batch = tuple(b.to(device) for b in batch)\n","                    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n","\n","                    outputs = model(**inputs)\n","                    loss = outputs[0]\n","                    loss_train_total += loss.item()\n","                    loss.backward()\n","\n","                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","                    optimizer.step()\n","                    scheduler.step()\n","\n","                    progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n","\n","                # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n","\n","                # logger.info(f'\\nEpoch {epoch}')\n","                loss_train_avg = loss_train_total / len(dataloader_train)\n","                # logger.info(f'Training loss: {loss_train_avg}')\n","\n","                val_loss, predictions, true_vals = evaluate(dataloader_validation)\n","                val_f1 = f1_score_func(true_vals, np.argmax(predictions, axis=1), average='weighted')\n","                # logger.info(f'Validation loss: {val_loss}')\n","                # logger.info(f'F1 Score (Weighted): {val_f1}')\n","\n","            _, predictions, true_vals = evaluate(dataloader_validation)\n","            accuracy_dict, count_dict = accuracy_per_class(predictions, true_vals)\n","\n","            for label, accuracy in accuracy_dict.items():\n","                all_accuracies[label].append(accuracy)\n","\n","\n","            # Calculate the average accuracy for each label\n","            avg_accuracy = {label: np.mean(accs) for label, accs in all_accuracies.items()}\n","\n","            # Calculate the overall average accuracy across all labels\n","            overall_avg_accuracy = np.mean(list(avg_accuracy.values()))\n","\n","            logger.info(f'Seed {seed_val} - Accuracy: {overall_avg_accuracy} - Count: {count_dict} - lr: {lr} -batchsize:{batch_size} -epochs:{epochs}')\n","            #store results\n","            results.append((lr, batch_size, epochs, overall_avg_accuracy))\n","\n","\n","# Extract each parameter and accuracy for plotting\n","learning_rates = [result[0] for result in results]\n","batch_sizes = [result[1] for result in results]\n","epochs = [result[2] for result in results]\n","accuracies = [result[3] for result in results]\n","\n","# Find the best result based on accuracy\n","best_result = max(results, key=lambda x: x[3])\n","print(f\"Best result: LR={best_result[0]}, Batch={best_result[1]}, Epoch={best_result[2]}, Accuracy={best_result[3]:.4f}\")\n","\n","# Create a 3D plot\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","\n","# Plot the data points\n","sc = ax.scatter(learning_rates, batch_sizes, epochs, c=accuracies, cmap='viridis', s=100, edgecolors='k')\n","\n","# Add color bar\n","cbar = plt.colorbar(sc)\n","cbar.set_label('Overall Average Accuracy')\n","\n","# Set labels\n","ax.set_xlabel('Learning Rate')\n","ax.set_ylabel('Batch Size')\n","ax.set_zlabel('Epochs')\n","\n","# Title\n","ax.set_title('Hyperparameter Tuning Results for smm4h top3')\n","\n","# Save the plot to a file\n","plt.savefig(\"hyperparameter_tuning_3d_plot.png\", dpi=300, bbox_inches='tight')\n","\n","# Show the plot\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vRUOcg2DXcZ","outputId":"58270a1e-29e5-4d9a-dd5a-86a94807c9e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch Progress:   0%|          | 0/5 [00:00<?, ?it/s]\n","Epoch 1:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\n","Epoch 1:   0%|          | 0/147 [00:00<?, ?it/s, training_loss=0.371]\u001b[A\n","Epoch 1:   1%|          | 1/147 [00:00<00:44,  3.25it/s, training_loss=0.371]\u001b[A\n","Epoch 1:   1%|          | 1/147 [00:00<00:44,  3.25it/s, training_loss=0.324]\u001b[A\n","Epoch 1:   1%|▏         | 2/147 [00:00<00:38,  3.74it/s, training_loss=0.324]\u001b[A\n","Epoch 1:   1%|▏         | 2/147 [00:00<00:38,  3.74it/s, training_loss=0.419]\u001b[A\n","Epoch 1:   2%|▏         | 3/147 [00:00<00:37,  3.83it/s, training_loss=0.419]\u001b[A\n","Epoch 1:   2%|▏         | 3/147 [00:01<00:37,  3.83it/s, training_loss=0.369]\u001b[A\n","Epoch 1:   3%|▎         | 4/147 [00:01<00:36,  3.90it/s, training_loss=0.369]\u001b[A\n","Epoch 1:   3%|▎         | 4/147 [00:01<00:36,  3.90it/s, training_loss=0.354]\u001b[A\n","Epoch 1:   3%|▎         | 5/147 [00:01<00:35,  4.00it/s, training_loss=0.354]\u001b[A\n","Epoch 1:   3%|▎         | 5/147 [00:01<00:35,  4.00it/s, training_loss=0.294]\u001b[A\n","Epoch 1:   4%|▍         | 6/147 [00:01<00:34,  4.14it/s, training_loss=0.294]\u001b[A\n","Epoch 1:   4%|▍         | 6/147 [00:01<00:34,  4.14it/s, training_loss=0.379]\u001b[A\n","Epoch 1:   5%|▍         | 7/147 [00:01<00:32,  4.26it/s, training_loss=0.379]\u001b[A\n","Epoch 1:   5%|▍         | 7/147 [00:01<00:32,  4.26it/s, training_loss=0.409]\u001b[A\n","Epoch 1:   5%|▌         | 8/147 [00:01<00:32,  4.29it/s, training_loss=0.409]\u001b[A\n","Epoch 1:   5%|▌         | 8/147 [00:02<00:32,  4.29it/s, training_loss=0.288]\u001b[A\n","Epoch 1:   6%|▌         | 9/147 [00:02<00:32,  4.30it/s, training_loss=0.288]\u001b[A\n","Epoch 1:   6%|▌         | 9/147 [00:02<00:32,  4.30it/s, training_loss=0.345]\u001b[A\n","Epoch 1:   7%|▋         | 10/147 [00:02<00:33,  4.06it/s, training_loss=0.345]\u001b[A\n","Epoch 1:   7%|▋         | 10/147 [00:02<00:33,  4.06it/s, training_loss=0.388]\u001b[A\n","Epoch 1:   7%|▋         | 11/147 [00:02<00:32,  4.16it/s, training_loss=0.388]\u001b[A\n","Epoch 1:   7%|▋         | 11/147 [00:02<00:32,  4.16it/s, training_loss=0.420]\u001b[A\n","Epoch 1:   8%|▊         | 12/147 [00:02<00:32,  4.22it/s, training_loss=0.420]\u001b[A\n","Epoch 1:   8%|▊         | 12/147 [00:03<00:32,  4.22it/s, training_loss=0.373]\u001b[A\n","Epoch 1:   9%|▉         | 13/147 [00:03<00:31,  4.24it/s, training_loss=0.373]\u001b[A\n","Epoch 1:   9%|▉         | 13/147 [00:03<00:31,  4.24it/s, training_loss=0.340]\u001b[A\n","Epoch 1:  10%|▉         | 14/147 [00:03<00:30,  4.32it/s, training_loss=0.340]\u001b[A\n","Epoch 1:  10%|▉         | 14/147 [00:03<00:30,  4.32it/s, training_loss=0.371]\u001b[A\n","Epoch 1:  10%|█         | 15/147 [00:03<00:29,  4.43it/s, training_loss=0.371]\u001b[A\n","Epoch 1:  10%|█         | 15/147 [00:03<00:29,  4.43it/s, training_loss=0.412]\u001b[A\n","Epoch 1:  11%|█         | 16/147 [00:03<00:28,  4.52it/s, training_loss=0.412]\u001b[A\n","Epoch 1:  11%|█         | 16/147 [00:04<00:28,  4.52it/s, training_loss=0.346]\u001b[A\n","Epoch 1:  12%|█▏        | 17/147 [00:04<00:28,  4.59it/s, training_loss=0.346]\u001b[A\n","Epoch 1:  12%|█▏        | 17/147 [00:04<00:28,  4.59it/s, training_loss=0.375]\u001b[A\n","Epoch 1:  12%|█▏        | 18/147 [00:04<00:27,  4.62it/s, training_loss=0.375]\u001b[A\n","Epoch 1:  12%|█▏        | 18/147 [00:04<00:27,  4.62it/s, training_loss=0.335]\u001b[A\n","Epoch 1:  13%|█▎        | 19/147 [00:04<00:27,  4.64it/s, training_loss=0.335]\u001b[A\n","Epoch 1:  13%|█▎        | 19/147 [00:04<00:27,  4.64it/s, training_loss=0.393]\u001b[A\n","Epoch 1:  14%|█▎        | 20/147 [00:04<00:27,  4.67it/s, training_loss=0.393]\u001b[A\n","Epoch 1:  14%|█▎        | 20/147 [00:04<00:27,  4.67it/s, training_loss=0.373]\u001b[A\n","Epoch 1:  14%|█▍        | 21/147 [00:04<00:26,  4.71it/s, training_loss=0.373]\u001b[A\n","Epoch 1:  14%|█▍        | 21/147 [00:05<00:26,  4.71it/s, training_loss=0.347]\u001b[A\n","Epoch 1:  15%|█▍        | 22/147 [00:05<00:26,  4.73it/s, training_loss=0.347]\u001b[A\n","Epoch 1:  15%|█▍        | 22/147 [00:05<00:26,  4.73it/s, training_loss=0.383]\u001b[A\n","Epoch 1:  16%|█▌        | 23/147 [00:05<00:26,  4.76it/s, training_loss=0.383]\u001b[A\n","Epoch 1:  16%|█▌        | 23/147 [00:05<00:26,  4.76it/s, training_loss=0.338]\u001b[A\n","Epoch 1:  16%|█▋        | 24/147 [00:05<00:25,  4.75it/s, training_loss=0.338]\u001b[A\n","Epoch 1:  16%|█▋        | 24/147 [00:05<00:25,  4.75it/s, training_loss=0.373]\u001b[A\n","Epoch 1:  17%|█▋        | 25/147 [00:05<00:25,  4.77it/s, training_loss=0.373]\u001b[A\n","Epoch 1:  17%|█▋        | 25/147 [00:05<00:25,  4.77it/s, training_loss=0.338]\u001b[A\n","Epoch 1:  18%|█▊        | 26/147 [00:05<00:25,  4.75it/s, training_loss=0.338]\u001b[A\n","Epoch 1:  18%|█▊        | 26/147 [00:06<00:25,  4.75it/s, training_loss=0.294]\u001b[A\n","Epoch 1:  18%|█▊        | 27/147 [00:06<00:25,  4.75it/s, training_loss=0.294]\u001b[A\n","Epoch 1:  18%|█▊        | 27/147 [00:06<00:25,  4.75it/s, training_loss=0.337]\u001b[A\n","Epoch 1:  19%|█▉        | 28/147 [00:06<00:24,  4.78it/s, training_loss=0.337]\u001b[A\n","Epoch 1:  19%|█▉        | 28/147 [00:06<00:24,  4.78it/s, training_loss=0.274]\u001b[A\n","Epoch 1:  20%|█▉        | 29/147 [00:06<00:24,  4.75it/s, training_loss=0.274]\u001b[A\n","Epoch 1:  20%|█▉        | 29/147 [00:06<00:24,  4.75it/s, training_loss=0.315]\u001b[A\n","Epoch 1:  20%|██        | 30/147 [00:06<00:24,  4.74it/s, training_loss=0.315]\u001b[A\n","Epoch 1:  20%|██        | 30/147 [00:06<00:24,  4.74it/s, training_loss=0.319]\u001b[A\n","Epoch 1:  21%|██        | 31/147 [00:06<00:24,  4.74it/s, training_loss=0.319]\u001b[A\n","Epoch 1:  21%|██        | 31/147 [00:07<00:24,  4.74it/s, training_loss=0.323]\u001b[A\n","Epoch 1:  22%|██▏       | 32/147 [00:07<00:24,  4.74it/s, training_loss=0.323]\u001b[A\n","Epoch 1:  22%|██▏       | 32/147 [00:07<00:24,  4.74it/s, training_loss=0.432]\u001b[A\n","Epoch 1:  22%|██▏       | 33/147 [00:07<00:24,  4.75it/s, training_loss=0.432]\u001b[A\n","Epoch 1:  22%|██▏       | 33/147 [00:07<00:24,  4.75it/s, training_loss=0.363]\u001b[A\n","Epoch 1:  23%|██▎       | 34/147 [00:07<00:23,  4.75it/s, training_loss=0.363]\u001b[A\n","Epoch 1:  23%|██▎       | 34/147 [00:07<00:23,  4.75it/s, training_loss=0.394]\u001b[A\n","Epoch 1:  24%|██▍       | 35/147 [00:07<00:23,  4.77it/s, training_loss=0.394]\u001b[A\n","Epoch 1:  24%|██▍       | 35/147 [00:08<00:23,  4.77it/s, training_loss=0.333]\u001b[A\n","Epoch 1:  24%|██▍       | 36/147 [00:08<00:23,  4.78it/s, training_loss=0.333]\u001b[A\n","Epoch 1:  24%|██▍       | 36/147 [00:08<00:23,  4.78it/s, training_loss=0.293]\u001b[A\n","Epoch 1:  25%|██▌       | 37/147 [00:08<00:23,  4.74it/s, training_loss=0.293]\u001b[A\n","Epoch 1:  25%|██▌       | 37/147 [00:08<00:23,  4.74it/s, training_loss=0.341]\u001b[A\n","Epoch 1:  26%|██▌       | 38/147 [00:08<00:22,  4.74it/s, training_loss=0.341]\u001b[A\n","Epoch 1:  26%|██▌       | 38/147 [00:08<00:22,  4.74it/s, training_loss=0.329]\u001b[A\n","Epoch 1:  27%|██▋       | 39/147 [00:08<00:22,  4.73it/s, training_loss=0.329]\u001b[A\n","Epoch 1:  27%|██▋       | 39/147 [00:08<00:22,  4.73it/s, training_loss=0.378]\u001b[A\n","Epoch 1:  27%|██▋       | 40/147 [00:08<00:22,  4.71it/s, training_loss=0.378]\u001b[A\n","Epoch 1:  27%|██▋       | 40/147 [00:09<00:22,  4.71it/s, training_loss=0.370]\u001b[A\n","Epoch 1:  28%|██▊       | 41/147 [00:09<00:22,  4.70it/s, training_loss=0.370]\u001b[A\n","Epoch 1:  28%|██▊       | 41/147 [00:09<00:22,  4.70it/s, training_loss=0.378]\u001b[A\n","Epoch 1:  29%|██▊       | 42/147 [00:09<00:22,  4.69it/s, training_loss=0.378]\u001b[A\n","Epoch 1:  29%|██▊       | 42/147 [00:09<00:22,  4.69it/s, training_loss=0.316]\u001b[A\n","Epoch 1:  29%|██▉       | 43/147 [00:09<00:22,  4.67it/s, training_loss=0.316]\u001b[A\n","Epoch 1:  29%|██▉       | 43/147 [00:09<00:22,  4.67it/s, training_loss=0.361]\u001b[A\n","Epoch 1:  30%|██▉       | 44/147 [00:09<00:21,  4.69it/s, training_loss=0.361]\u001b[A\n","Epoch 1:  30%|██▉       | 44/147 [00:09<00:21,  4.69it/s, training_loss=0.368]\u001b[A\n","Epoch 1:  31%|███       | 45/147 [00:09<00:21,  4.69it/s, training_loss=0.368]\u001b[A\n","Epoch 1:  31%|███       | 45/147 [00:10<00:21,  4.69it/s, training_loss=0.398]\u001b[A\n","Epoch 1:  31%|███▏      | 46/147 [00:10<00:21,  4.70it/s, training_loss=0.398]\u001b[A\n","Epoch 1:  31%|███▏      | 46/147 [00:10<00:21,  4.70it/s, training_loss=0.323]\u001b[A\n","Epoch 1:  32%|███▏      | 47/147 [00:10<00:21,  4.71it/s, training_loss=0.323]\u001b[A\n","Epoch 1:  32%|███▏      | 47/147 [00:10<00:21,  4.71it/s, training_loss=0.275]\u001b[A\n","Epoch 1:  33%|███▎      | 48/147 [00:10<00:21,  4.70it/s, training_loss=0.275]\u001b[A\n","Epoch 1:  33%|███▎      | 48/147 [00:10<00:21,  4.70it/s, training_loss=0.340]\u001b[A\n","Epoch 1:  33%|███▎      | 49/147 [00:10<00:21,  4.65it/s, training_loss=0.340]\u001b[A\n","Epoch 1:  33%|███▎      | 49/147 [00:10<00:21,  4.65it/s, training_loss=0.450]\u001b[A\n","Epoch 1:  34%|███▍      | 50/147 [00:11<00:20,  4.66it/s, training_loss=0.450]\u001b[A\n","Epoch 1:  34%|███▍      | 50/147 [00:11<00:20,  4.66it/s, training_loss=0.369]\u001b[A\n","Epoch 1:  35%|███▍      | 51/147 [00:11<00:20,  4.69it/s, training_loss=0.369]\u001b[A\n","Epoch 1:  35%|███▍      | 51/147 [00:11<00:20,  4.69it/s, training_loss=0.353]\u001b[A\n","Epoch 1:  35%|███▌      | 52/147 [00:11<00:20,  4.70it/s, training_loss=0.353]\u001b[A\n","Epoch 1:  35%|███▌      | 52/147 [00:11<00:20,  4.70it/s, training_loss=0.398]\u001b[A\n","Epoch 1:  36%|███▌      | 53/147 [00:11<00:19,  4.72it/s, training_loss=0.398]\u001b[A\n","Epoch 1:  36%|███▌      | 53/147 [00:11<00:19,  4.72it/s, training_loss=0.367]\u001b[A\n","Epoch 1:  37%|███▋      | 54/147 [00:11<00:19,  4.67it/s, training_loss=0.367]\u001b[A\n","Epoch 1:  37%|███▋      | 54/147 [00:12<00:19,  4.67it/s, training_loss=0.425]\u001b[A\n","Epoch 1:  37%|███▋      | 55/147 [00:12<00:19,  4.69it/s, training_loss=0.425]\u001b[A\n","Epoch 1:  37%|███▋      | 55/147 [00:12<00:19,  4.69it/s, training_loss=0.405]\u001b[A\n","Epoch 1:  38%|███▊      | 56/147 [00:12<00:19,  4.68it/s, training_loss=0.405]\u001b[A\n","Epoch 1:  38%|███▊      | 56/147 [00:12<00:19,  4.68it/s, training_loss=0.413]\u001b[A\n","Epoch 1:  39%|███▉      | 57/147 [00:12<00:19,  4.67it/s, training_loss=0.413]\u001b[A\n","Epoch 1:  39%|███▉      | 57/147 [00:12<00:19,  4.67it/s, training_loss=0.308]\u001b[A\n","Epoch 1:  39%|███▉      | 58/147 [00:12<00:19,  4.68it/s, training_loss=0.308]\u001b[A\n","Epoch 1:  39%|███▉      | 58/147 [00:12<00:19,  4.68it/s, training_loss=0.398]\u001b[A\n","Epoch 1:  40%|████      | 59/147 [00:12<00:18,  4.70it/s, training_loss=0.398]\u001b[A\n","Epoch 1:  40%|████      | 59/147 [00:13<00:18,  4.70it/s, training_loss=0.335]\u001b[A\n","Epoch 1:  41%|████      | 60/147 [00:13<00:18,  4.68it/s, training_loss=0.335]\u001b[A\n","Epoch 1:  41%|████      | 60/147 [00:13<00:18,  4.68it/s, training_loss=0.306]\u001b[A\n","Epoch 1:  41%|████▏     | 61/147 [00:13<00:18,  4.69it/s, training_loss=0.306]\u001b[A\n","Epoch 1:  41%|████▏     | 61/147 [00:13<00:18,  4.69it/s, training_loss=0.311]\u001b[A\n","Epoch 1:  42%|████▏     | 62/147 [00:13<00:18,  4.66it/s, training_loss=0.311]\u001b[A\n","Epoch 1:  42%|████▏     | 62/147 [00:13<00:18,  4.66it/s, training_loss=0.284]\u001b[A\n","Epoch 1:  43%|████▎     | 63/147 [00:13<00:17,  4.68it/s, training_loss=0.284]\u001b[A\n","Epoch 1:  43%|████▎     | 63/147 [00:13<00:17,  4.68it/s, training_loss=0.303]\u001b[A\n","Epoch 1:  44%|████▎     | 64/147 [00:13<00:17,  4.67it/s, training_loss=0.303]\u001b[A\n","Epoch 1:  44%|████▎     | 64/147 [00:14<00:17,  4.67it/s, training_loss=0.392]\u001b[A\n","Epoch 1:  44%|████▍     | 65/147 [00:14<00:17,  4.66it/s, training_loss=0.392]\u001b[A\n","Epoch 1:  44%|████▍     | 65/147 [00:14<00:17,  4.66it/s, training_loss=0.403]\u001b[A\n","Epoch 1:  45%|████▍     | 66/147 [00:14<00:17,  4.68it/s, training_loss=0.403]\u001b[A\n","Epoch 1:  45%|████▍     | 66/147 [00:14<00:17,  4.68it/s, training_loss=0.365]\u001b[A\n","Epoch 1:  46%|████▌     | 67/147 [00:14<00:17,  4.66it/s, training_loss=0.365]\u001b[A\n","Epoch 1:  46%|████▌     | 67/147 [00:14<00:17,  4.66it/s, training_loss=0.391]\u001b[A\n","Epoch 1:  46%|████▋     | 68/147 [00:14<00:17,  4.64it/s, training_loss=0.391]\u001b[A\n","Epoch 1:  46%|████▋     | 68/147 [00:15<00:17,  4.64it/s, training_loss=0.276]\u001b[A\n","Epoch 1:  47%|████▋     | 69/147 [00:15<00:16,  4.62it/s, training_loss=0.276]\u001b[A\n","Epoch 1:  47%|████▋     | 69/147 [00:15<00:16,  4.62it/s, training_loss=0.359]\u001b[A\n","Epoch 1:  48%|████▊     | 70/147 [00:15<00:16,  4.65it/s, training_loss=0.359]\u001b[A\n","Epoch 1:  48%|████▊     | 70/147 [00:15<00:16,  4.65it/s, training_loss=0.417]\u001b[A\n","Epoch 1:  48%|████▊     | 71/147 [00:15<00:16,  4.62it/s, training_loss=0.417]\u001b[A\n","Epoch 1:  48%|████▊     | 71/147 [00:15<00:16,  4.62it/s, training_loss=0.406]\u001b[A\n","Epoch 1:  49%|████▉     | 72/147 [00:15<00:16,  4.66it/s, training_loss=0.406]\u001b[A\n","Epoch 1:  49%|████▉     | 72/147 [00:15<00:16,  4.66it/s, training_loss=0.390]\u001b[A\n","Epoch 1:  50%|████▉     | 73/147 [00:15<00:16,  4.62it/s, training_loss=0.390]\u001b[A\n","Epoch 1:  50%|████▉     | 73/147 [00:16<00:16,  4.62it/s, training_loss=0.349]\u001b[A\n","Epoch 1:  50%|█████     | 74/147 [00:16<00:15,  4.63it/s, training_loss=0.349]\u001b[A\n","Epoch 1:  50%|█████     | 74/147 [00:16<00:15,  4.63it/s, training_loss=0.290]\u001b[A\n","Epoch 1:  51%|█████     | 75/147 [00:16<00:15,  4.64it/s, training_loss=0.290]\u001b[A\n","Epoch 1:  51%|█████     | 75/147 [00:16<00:15,  4.64it/s, training_loss=0.429]\u001b[A\n","Epoch 1:  52%|█████▏    | 76/147 [00:16<00:15,  4.64it/s, training_loss=0.429]\u001b[A\n","Epoch 1:  52%|█████▏    | 76/147 [00:16<00:15,  4.64it/s, training_loss=0.360]\u001b[A\n","Epoch 1:  52%|█████▏    | 77/147 [00:16<00:14,  4.67it/s, training_loss=0.360]\u001b[A\n","Epoch 1:  52%|█████▏    | 77/147 [00:17<00:14,  4.67it/s, training_loss=0.288]\u001b[A\n","Epoch 1:  53%|█████▎    | 78/147 [00:17<00:14,  4.67it/s, training_loss=0.288]\u001b[A\n","Epoch 1:  53%|█████▎    | 78/147 [00:17<00:14,  4.67it/s, training_loss=0.366]\u001b[A\n","Epoch 1:  54%|█████▎    | 79/147 [00:17<00:14,  4.67it/s, training_loss=0.366]\u001b[A\n","Epoch 1:  54%|█████▎    | 79/147 [00:17<00:14,  4.67it/s, training_loss=0.331]\u001b[A\n","Epoch 1:  54%|█████▍    | 80/147 [00:17<00:14,  4.62it/s, training_loss=0.331]\u001b[A\n","Epoch 1:  54%|█████▍    | 80/147 [00:17<00:14,  4.62it/s, training_loss=0.295]\u001b[A\n","Epoch 1:  55%|█████▌    | 81/147 [00:17<00:14,  4.62it/s, training_loss=0.295]\u001b[A\n","Epoch 1:  55%|█████▌    | 81/147 [00:17<00:14,  4.62it/s, training_loss=0.304]\u001b[A\n","Epoch 1:  56%|█████▌    | 82/147 [00:17<00:14,  4.64it/s, training_loss=0.304]\u001b[A\n","Epoch 1:  56%|█████▌    | 82/147 [00:18<00:14,  4.64it/s, training_loss=0.407]\u001b[A\n","Epoch 1:  56%|█████▋    | 83/147 [00:18<00:13,  4.63it/s, training_loss=0.407]\u001b[A\n","Epoch 1:  56%|█████▋    | 83/147 [00:18<00:13,  4.63it/s, training_loss=0.411]\u001b[A\n","Epoch 1:  57%|█████▋    | 84/147 [00:18<00:13,  4.63it/s, training_loss=0.411]\u001b[A\n","Epoch 1:  57%|█████▋    | 84/147 [00:18<00:13,  4.63it/s, training_loss=0.330]\u001b[A\n","Epoch 1:  58%|█████▊    | 85/147 [00:18<00:13,  4.63it/s, training_loss=0.330]\u001b[A\n","Epoch 1:  58%|█████▊    | 85/147 [00:18<00:13,  4.63it/s, training_loss=0.319]\u001b[A\n","Epoch 1:  59%|█████▊    | 86/147 [00:18<00:13,  4.62it/s, training_loss=0.319]\u001b[A\n","Epoch 1:  59%|█████▊    | 86/147 [00:18<00:13,  4.62it/s, training_loss=0.382]\u001b[A\n","Epoch 1:  59%|█████▉    | 87/147 [00:18<00:12,  4.64it/s, training_loss=0.382]\u001b[A\n","Epoch 1:  59%|█████▉    | 87/147 [00:19<00:12,  4.64it/s, training_loss=0.415]\u001b[A\n","Epoch 1:  60%|█████▉    | 88/147 [00:19<00:12,  4.65it/s, training_loss=0.415]\u001b[A\n","Epoch 1:  60%|█████▉    | 88/147 [00:19<00:12,  4.65it/s, training_loss=0.401]\u001b[A\n","Epoch 1:  61%|██████    | 89/147 [00:19<00:12,  4.66it/s, training_loss=0.401]\u001b[A\n","Epoch 1:  61%|██████    | 89/147 [00:19<00:12,  4.66it/s, training_loss=0.412]\u001b[A\n","Epoch 1:  61%|██████    | 90/147 [00:19<00:12,  4.68it/s, training_loss=0.412]\u001b[A\n","Epoch 1:  61%|██████    | 90/147 [00:19<00:12,  4.68it/s, training_loss=0.362]\u001b[A\n","Epoch 1:  62%|██████▏   | 91/147 [00:19<00:12,  4.66it/s, training_loss=0.362]\u001b[A\n","Epoch 1:  62%|██████▏   | 91/147 [00:20<00:12,  4.66it/s, training_loss=0.314]\u001b[A\n","Epoch 1:  63%|██████▎   | 92/147 [00:20<00:11,  4.64it/s, training_loss=0.314]\u001b[A\n","Epoch 1:  63%|██████▎   | 92/147 [00:20<00:11,  4.64it/s, training_loss=0.325]\u001b[A\n","Epoch 1:  63%|██████▎   | 93/147 [00:20<00:11,  4.67it/s, training_loss=0.325]\u001b[A\n","Epoch 1:  63%|██████▎   | 93/147 [00:20<00:11,  4.67it/s, training_loss=0.293]\u001b[A\n","Epoch 1:  64%|██████▍   | 94/147 [00:20<00:11,  4.64it/s, training_loss=0.293]\u001b[A\n","Epoch 1:  64%|██████▍   | 94/147 [00:20<00:11,  4.64it/s, training_loss=0.293]\u001b[A\n","Epoch 1:  65%|██████▍   | 95/147 [00:20<00:11,  4.67it/s, training_loss=0.293]\u001b[A\n","Epoch 1:  65%|██████▍   | 95/147 [00:20<00:11,  4.67it/s, training_loss=0.261]\u001b[A\n","Epoch 1:  65%|██████▌   | 96/147 [00:20<00:10,  4.65it/s, training_loss=0.261]\u001b[A\n","Epoch 1:  65%|██████▌   | 96/147 [00:21<00:10,  4.65it/s, training_loss=0.354]\u001b[A\n","Epoch 1:  66%|██████▌   | 97/147 [00:21<00:10,  4.64it/s, training_loss=0.354]\u001b[A\n","Epoch 1:  66%|██████▌   | 97/147 [00:21<00:10,  4.64it/s, training_loss=0.357]\u001b[A\n","Epoch 1:  67%|██████▋   | 98/147 [00:21<00:10,  4.63it/s, training_loss=0.357]\u001b[A\n","Epoch 1:  67%|██████▋   | 98/147 [00:21<00:10,  4.63it/s, training_loss=0.399]\u001b[A\n","Epoch 1:  67%|██████▋   | 99/147 [00:21<00:10,  4.65it/s, training_loss=0.399]\u001b[A\n","Epoch 1:  67%|██████▋   | 99/147 [00:21<00:10,  4.65it/s, training_loss=0.408]\u001b[A\n","Epoch 1:  68%|██████▊   | 100/147 [00:21<00:10,  4.64it/s, training_loss=0.408]\u001b[A\n","Epoch 1:  68%|██████▊   | 100/147 [00:21<00:10,  4.64it/s, training_loss=0.362]\u001b[A\n","Epoch 1:  69%|██████▊   | 101/147 [00:21<00:09,  4.65it/s, training_loss=0.362]\u001b[A\n","Epoch 1:  69%|██████▊   | 101/147 [00:22<00:09,  4.65it/s, training_loss=0.419]\u001b[A\n","Epoch 1:  69%|██████▉   | 102/147 [00:22<00:09,  4.65it/s, training_loss=0.419]\u001b[A\n","Epoch 1:  69%|██████▉   | 102/147 [00:22<00:09,  4.65it/s, training_loss=0.428]\u001b[A\n","Epoch 1:  70%|███████   | 103/147 [00:22<00:09,  4.63it/s, training_loss=0.428]\u001b[A\n","Epoch 1:  70%|███████   | 103/147 [00:22<00:09,  4.63it/s, training_loss=0.439]\u001b[A\n","Epoch 1:  71%|███████   | 104/147 [00:22<00:09,  4.61it/s, training_loss=0.439]\u001b[A\n","Epoch 1:  71%|███████   | 104/147 [00:22<00:09,  4.61it/s, training_loss=0.308]\u001b[A\n","Epoch 1:  71%|███████▏  | 105/147 [00:22<00:09,  4.58it/s, training_loss=0.308]\u001b[A\n","Epoch 1:  71%|███████▏  | 105/147 [00:23<00:09,  4.58it/s, training_loss=0.376]\u001b[A\n","Epoch 1:  72%|███████▏  | 106/147 [00:23<00:08,  4.58it/s, training_loss=0.376]\u001b[A\n","Epoch 1:  72%|███████▏  | 106/147 [00:23<00:08,  4.58it/s, training_loss=0.353]\u001b[A\n","Epoch 1:  73%|███████▎  | 107/147 [00:23<00:08,  4.59it/s, training_loss=0.353]\u001b[A\n","Epoch 1:  73%|███████▎  | 107/147 [00:23<00:08,  4.59it/s, training_loss=0.383]\u001b[A\n","Epoch 1:  73%|███████▎  | 108/147 [00:23<00:08,  4.61it/s, training_loss=0.383]\u001b[A\n","Epoch 1:  73%|███████▎  | 108/147 [00:23<00:08,  4.61it/s, training_loss=0.423]\u001b[A\n","Epoch 1:  74%|███████▍  | 109/147 [00:23<00:08,  4.62it/s, training_loss=0.423]\u001b[A\n","Epoch 1:  74%|███████▍  | 109/147 [00:23<00:08,  4.62it/s, training_loss=0.364]\u001b[A\n","Epoch 1:  75%|███████▍  | 110/147 [00:23<00:07,  4.63it/s, training_loss=0.364]\u001b[A\n","Epoch 1:  75%|███████▍  | 110/147 [00:24<00:07,  4.63it/s, training_loss=0.340]\u001b[A\n","Epoch 1:  76%|███████▌  | 111/147 [00:24<00:07,  4.62it/s, training_loss=0.340]\u001b[A\n","Epoch 1:  76%|███████▌  | 111/147 [00:24<00:07,  4.62it/s, training_loss=0.275]\u001b[A\n","Epoch 1:  76%|███████▌  | 112/147 [00:24<00:07,  4.61it/s, training_loss=0.275]\u001b[A\n","Epoch 1:  76%|███████▌  | 112/147 [00:24<00:07,  4.61it/s, training_loss=0.328]\u001b[A\n","Epoch 1:  77%|███████▋  | 113/147 [00:24<00:07,  4.62it/s, training_loss=0.328]\u001b[A\n","Epoch 1:  77%|███████▋  | 113/147 [00:24<00:07,  4.62it/s, training_loss=0.318]\u001b[A\n","Epoch 1:  78%|███████▊  | 114/147 [00:24<00:07,  4.60it/s, training_loss=0.318]\u001b[A\n","Epoch 1:  78%|███████▊  | 114/147 [00:24<00:07,  4.60it/s, training_loss=0.385]\u001b[A\n","Epoch 1:  78%|███████▊  | 115/147 [00:24<00:06,  4.63it/s, training_loss=0.385]\u001b[A\n","Epoch 1:  78%|███████▊  | 115/147 [00:25<00:06,  4.63it/s, training_loss=0.373]\u001b[A\n","Epoch 1:  79%|███████▉  | 116/147 [00:25<00:06,  4.64it/s, training_loss=0.373]\u001b[A\n","Epoch 1:  79%|███████▉  | 116/147 [00:25<00:06,  4.64it/s, training_loss=0.403]\u001b[A\n","Epoch 1:  80%|███████▉  | 117/147 [00:25<00:06,  4.66it/s, training_loss=0.403]\u001b[A\n","Epoch 1:  80%|███████▉  | 117/147 [00:25<00:06,  4.66it/s, training_loss=0.298]\u001b[A\n","Epoch 1:  80%|████████  | 118/147 [00:25<00:06,  4.67it/s, training_loss=0.298]\u001b[A\n","Epoch 1:  80%|████████  | 118/147 [00:25<00:06,  4.67it/s, training_loss=0.387]\u001b[A\n","Epoch 1:  81%|████████  | 119/147 [00:25<00:06,  4.61it/s, training_loss=0.387]\u001b[A\n","Epoch 1:  81%|████████  | 119/147 [00:26<00:06,  4.61it/s, training_loss=0.377]\u001b[A\n","Epoch 1:  82%|████████▏ | 120/147 [00:26<00:05,  4.62it/s, training_loss=0.377]\u001b[A\n","Epoch 1:  82%|████████▏ | 120/147 [00:26<00:05,  4.62it/s, training_loss=0.326]\u001b[A\n","Epoch 1:  82%|████████▏ | 121/147 [00:26<00:05,  4.65it/s, training_loss=0.326]\u001b[A\n","Epoch 1:  82%|████████▏ | 121/147 [00:26<00:05,  4.65it/s, training_loss=0.343]\u001b[A\n","Epoch 1:  83%|████████▎ | 122/147 [00:26<00:05,  4.64it/s, training_loss=0.343]\u001b[A\n","Epoch 1:  83%|████████▎ | 122/147 [00:26<00:05,  4.64it/s, training_loss=0.380]\u001b[A\n","Epoch 1:  84%|████████▎ | 123/147 [00:26<00:05,  4.64it/s, training_loss=0.380]\u001b[A\n","Epoch 1:  84%|████████▎ | 123/147 [00:26<00:05,  4.64it/s, training_loss=0.386]\u001b[A\n","Epoch 1:  84%|████████▍ | 124/147 [00:26<00:04,  4.62it/s, training_loss=0.386]\u001b[A\n","Epoch 1:  84%|████████▍ | 124/147 [00:27<00:04,  4.62it/s, training_loss=0.304]\u001b[A\n","Epoch 1:  85%|████████▌ | 125/147 [00:27<00:04,  4.66it/s, training_loss=0.304]\u001b[A\n","Epoch 1:  85%|████████▌ | 125/147 [00:27<00:04,  4.66it/s, training_loss=0.351]\u001b[A\n","Epoch 1:  86%|████████▌ | 126/147 [00:27<00:04,  4.66it/s, training_loss=0.351]\u001b[A\n","Epoch 1:  86%|████████▌ | 126/147 [00:27<00:04,  4.66it/s, training_loss=0.380]\u001b[A\n","Epoch 1:  86%|████████▋ | 127/147 [00:27<00:04,  4.67it/s, training_loss=0.380]\u001b[A\n","Epoch 1:  86%|████████▋ | 127/147 [00:27<00:04,  4.67it/s, training_loss=0.370]\u001b[A\n","Epoch 1:  87%|████████▋ | 128/147 [00:27<00:04,  4.67it/s, training_loss=0.370]\u001b[A\n","Epoch 1:  87%|████████▋ | 128/147 [00:27<00:04,  4.67it/s, training_loss=0.297]\u001b[A\n","Epoch 1:  88%|████████▊ | 129/147 [00:28<00:03,  4.66it/s, training_loss=0.297]\u001b[A\n","Epoch 1:  88%|████████▊ | 129/147 [00:28<00:03,  4.66it/s, training_loss=0.414]\u001b[A\n","Epoch 1:  88%|████████▊ | 130/147 [00:28<00:03,  4.62it/s, training_loss=0.414]\u001b[A\n","Epoch 1:  88%|████████▊ | 130/147 [00:28<00:03,  4.62it/s, training_loss=0.322]\u001b[A\n","Epoch 1:  89%|████████▉ | 131/147 [00:28<00:03,  4.61it/s, training_loss=0.322]\u001b[A\n","Epoch 1:  89%|████████▉ | 131/147 [00:28<00:03,  4.61it/s, training_loss=0.336]\u001b[A\n","Epoch 1:  90%|████████▉ | 132/147 [00:28<00:03,  4.63it/s, training_loss=0.336]\u001b[A\n","Epoch 1:  90%|████████▉ | 132/147 [00:28<00:03,  4.63it/s, training_loss=0.332]\u001b[A\n","Epoch 1:  90%|█████████ | 133/147 [00:28<00:03,  4.63it/s, training_loss=0.332]\u001b[A\n","Epoch 1:  90%|█████████ | 133/147 [00:29<00:03,  4.63it/s, training_loss=0.446]\u001b[A\n","Epoch 1:  91%|█████████ | 134/147 [00:29<00:02,  4.65it/s, training_loss=0.446]\u001b[A\n","Epoch 1:  91%|█████████ | 134/147 [00:29<00:02,  4.65it/s, training_loss=0.304]\u001b[A\n","Epoch 1:  92%|█████████▏| 135/147 [00:29<00:02,  4.61it/s, training_loss=0.304]\u001b[A\n","Epoch 1:  92%|█████████▏| 135/147 [00:29<00:02,  4.61it/s, training_loss=0.357]\u001b[A\n","Epoch 1:  93%|█████████▎| 136/147 [00:29<00:02,  4.60it/s, training_loss=0.357]\u001b[A\n","Epoch 1:  93%|█████████▎| 136/147 [00:29<00:02,  4.60it/s, training_loss=0.386]\u001b[A\n","Epoch 1:  93%|█████████▎| 137/147 [00:29<00:02,  4.65it/s, training_loss=0.386]\u001b[A\n","Epoch 1:  93%|█████████▎| 137/147 [00:29<00:02,  4.65it/s, training_loss=0.316]\u001b[A\n","Epoch 1:  94%|█████████▍| 138/147 [00:29<00:01,  4.65it/s, training_loss=0.316]\u001b[A\n","Epoch 1:  94%|█████████▍| 138/147 [00:30<00:01,  4.65it/s, training_loss=0.370]\u001b[A\n","Epoch 1:  95%|█████████▍| 139/147 [00:30<00:01,  4.67it/s, training_loss=0.370]\u001b[A\n","Epoch 1:  95%|█████████▍| 139/147 [00:30<00:01,  4.67it/s, training_loss=0.342]\u001b[A\n","Epoch 1:  95%|█████████▌| 140/147 [00:30<00:01,  4.62it/s, training_loss=0.342]\u001b[A\n","Epoch 1:  95%|█████████▌| 140/147 [00:30<00:01,  4.62it/s, training_loss=0.357]\u001b[A\n","Epoch 1:  96%|█████████▌| 141/147 [00:30<00:01,  4.65it/s, training_loss=0.357]\u001b[A\n","Epoch 1:  96%|█████████▌| 141/147 [00:30<00:01,  4.65it/s, training_loss=0.324]\u001b[A\n","Epoch 1:  97%|█████████▋| 142/147 [00:30<00:01,  4.70it/s, training_loss=0.324]\u001b[A\n","Epoch 1:  97%|█████████▋| 142/147 [00:31<00:01,  4.70it/s, training_loss=0.325]\u001b[A\n","Epoch 1:  97%|█████████▋| 143/147 [00:31<00:00,  4.70it/s, training_loss=0.325]\u001b[A\n","Epoch 1:  97%|█████████▋| 143/147 [00:31<00:00,  4.70it/s, training_loss=0.249]\u001b[A\n","Epoch 1:  98%|█████████▊| 144/147 [00:31<00:00,  4.66it/s, training_loss=0.249]\u001b[A\n","Epoch 1:  98%|█████████▊| 144/147 [00:31<00:00,  4.66it/s, training_loss=0.326]\u001b[A\n","Epoch 1:  99%|█████████▊| 145/147 [00:31<00:00,  4.62it/s, training_loss=0.326]\u001b[A\n","Epoch 1:  99%|█████████▊| 145/147 [00:31<00:00,  4.62it/s, training_loss=0.422]\u001b[A\n","Epoch 1:  99%|█████████▉| 146/147 [00:31<00:00,  4.65it/s, training_loss=0.422]\u001b[A\n","Epoch 1:  99%|█████████▉| 146/147 [00:31<00:00,  4.65it/s, training_loss=0.369]\u001b[A\n","Epoch 1: 100%|██████████| 147/147 [00:31<00:00,  4.89it/s, training_loss=0.369]\u001b[A\n","Epoch Progress:  20%|██        | 1/5 [00:34<02:16, 34.21s/it]\n","Epoch 2:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\n","Epoch 2:   0%|          | 0/147 [00:00<?, ?it/s, training_loss=0.261]\u001b[A\n","Epoch 2:   1%|          | 1/147 [00:00<00:31,  4.65it/s, training_loss=0.261]\u001b[A\n","Epoch 2:   1%|          | 1/147 [00:00<00:31,  4.65it/s, training_loss=0.257]\u001b[A\n","Epoch 2:   1%|▏         | 2/147 [00:00<00:30,  4.70it/s, training_loss=0.257]\u001b[A\n","Epoch 2:   1%|▏         | 2/147 [00:00<00:30,  4.70it/s, training_loss=0.388]\u001b[A\n","Epoch 2:   2%|▏         | 3/147 [00:00<00:30,  4.68it/s, training_loss=0.388]\u001b[A\n","Epoch 2:   2%|▏         | 3/147 [00:00<00:30,  4.68it/s, training_loss=0.336]\u001b[A\n","Epoch 2:   3%|▎         | 4/147 [00:00<00:30,  4.71it/s, training_loss=0.336]\u001b[A\n","Epoch 2:   3%|▎         | 4/147 [00:01<00:30,  4.71it/s, training_loss=0.345]\u001b[A\n","Epoch 2:   3%|▎         | 5/147 [00:01<00:29,  4.75it/s, training_loss=0.345]\u001b[A\n","Epoch 2:   3%|▎         | 5/147 [00:01<00:29,  4.75it/s, training_loss=0.316]\u001b[A\n","Epoch 2:   4%|▍         | 6/147 [00:01<00:29,  4.75it/s, training_loss=0.316]\u001b[A\n","Epoch 2:   4%|▍         | 6/147 [00:01<00:29,  4.75it/s, training_loss=0.374]\u001b[A\n","Epoch 2:   5%|▍         | 7/147 [00:01<00:29,  4.75it/s, training_loss=0.374]\u001b[A\n","Epoch 2:   5%|▍         | 7/147 [00:01<00:29,  4.75it/s, training_loss=0.331]\u001b[A\n","Epoch 2:   5%|▌         | 8/147 [00:01<00:29,  4.73it/s, training_loss=0.331]\u001b[A\n","Epoch 2:   5%|▌         | 8/147 [00:01<00:29,  4.73it/s, training_loss=0.353]\u001b[A\n","Epoch 2:   6%|▌         | 9/147 [00:01<00:29,  4.72it/s, training_loss=0.353]\u001b[A\n","Epoch 2:   6%|▌         | 9/147 [00:02<00:29,  4.72it/s, training_loss=0.323]\u001b[A\n","Epoch 2:   7%|▋         | 10/147 [00:02<00:29,  4.72it/s, training_loss=0.323]\u001b[A\n","Epoch 2:   7%|▋         | 10/147 [00:02<00:29,  4.72it/s, training_loss=0.324]\u001b[A\n","Epoch 2:   7%|▋         | 11/147 [00:02<00:28,  4.73it/s, training_loss=0.324]\u001b[A\n","Epoch 2:   7%|▋         | 11/147 [00:02<00:28,  4.73it/s, training_loss=0.296]\u001b[A\n","Epoch 2:   8%|▊         | 12/147 [00:02<00:28,  4.72it/s, training_loss=0.296]\u001b[A\n","Epoch 2:   8%|▊         | 12/147 [00:02<00:28,  4.72it/s, training_loss=0.362]\u001b[A\n","Epoch 2:   9%|▉         | 13/147 [00:02<00:28,  4.71it/s, training_loss=0.362]\u001b[A\n","Epoch 2:   9%|▉         | 13/147 [00:02<00:28,  4.71it/s, training_loss=0.401]\u001b[A\n","Epoch 2:  10%|▉         | 14/147 [00:02<00:28,  4.71it/s, training_loss=0.401]\u001b[A\n","Epoch 2:  10%|▉         | 14/147 [00:03<00:28,  4.71it/s, training_loss=0.328]\u001b[A\n","Epoch 2:  10%|█         | 15/147 [00:03<00:28,  4.68it/s, training_loss=0.328]\u001b[A\n","Epoch 2:  10%|█         | 15/147 [00:03<00:28,  4.68it/s, training_loss=0.344]\u001b[A\n","Epoch 2:  11%|█         | 16/147 [00:03<00:27,  4.69it/s, training_loss=0.344]\u001b[A\n","Epoch 2:  11%|█         | 16/147 [00:03<00:27,  4.69it/s, training_loss=0.292]\u001b[A\n","Epoch 2:  12%|█▏        | 17/147 [00:03<00:27,  4.68it/s, training_loss=0.292]\u001b[A\n","Epoch 2:  12%|█▏        | 17/147 [00:03<00:27,  4.68it/s, training_loss=0.398]\u001b[A\n","Epoch 2:  12%|█▏        | 18/147 [00:03<00:27,  4.70it/s, training_loss=0.398]\u001b[A\n","Epoch 2:  12%|█▏        | 18/147 [00:04<00:27,  4.70it/s, training_loss=0.351]\u001b[A\n","Epoch 2:  13%|█▎        | 19/147 [00:04<00:27,  4.70it/s, training_loss=0.351]\u001b[A\n","Epoch 2:  13%|█▎        | 19/147 [00:04<00:27,  4.70it/s, training_loss=0.322]\u001b[A\n","Epoch 2:  14%|█▎        | 20/147 [00:04<00:27,  4.70it/s, training_loss=0.322]\u001b[A\n","Epoch 2:  14%|█▎        | 20/147 [00:04<00:27,  4.70it/s, training_loss=0.336]\u001b[A\n","Epoch 2:  14%|█▍        | 21/147 [00:04<00:26,  4.70it/s, training_loss=0.336]\u001b[A\n","Epoch 2:  14%|█▍        | 21/147 [00:04<00:26,  4.70it/s, training_loss=0.347]\u001b[A\n","Epoch 2:  15%|█▍        | 22/147 [00:04<00:26,  4.73it/s, training_loss=0.347]\u001b[A\n","Epoch 2:  15%|█▍        | 22/147 [00:04<00:26,  4.73it/s, training_loss=0.313]\u001b[A\n","Epoch 2:  16%|█▌        | 23/147 [00:04<00:26,  4.75it/s, training_loss=0.313]\u001b[A\n","Epoch 2:  16%|█▌        | 23/147 [00:05<00:26,  4.75it/s, training_loss=0.315]\u001b[A\n","Epoch 2:  16%|█▋        | 24/147 [00:05<00:25,  4.74it/s, training_loss=0.315]\u001b[A\n","Epoch 2:  16%|█▋        | 24/147 [00:05<00:25,  4.74it/s, training_loss=0.316]\u001b[A\n","Epoch 2:  17%|█▋        | 25/147 [00:05<00:25,  4.73it/s, training_loss=0.316]\u001b[A\n","Epoch 2:  17%|█▋        | 25/147 [00:05<00:25,  4.73it/s, training_loss=0.350]\u001b[A\n","Epoch 2:  18%|█▊        | 26/147 [00:05<00:25,  4.71it/s, training_loss=0.350]\u001b[A\n","Epoch 2:  18%|█▊        | 26/147 [00:05<00:25,  4.71it/s, training_loss=0.295]\u001b[A\n","Epoch 2:  18%|█▊        | 27/147 [00:05<00:25,  4.74it/s, training_loss=0.295]\u001b[A\n","Epoch 2:  18%|█▊        | 27/147 [00:05<00:25,  4.74it/s, training_loss=0.267]\u001b[A\n","Epoch 2:  19%|█▉        | 28/147 [00:05<00:25,  4.74it/s, training_loss=0.267]\u001b[A\n","Epoch 2:  19%|█▉        | 28/147 [00:06<00:25,  4.74it/s, training_loss=0.313]\u001b[A\n","Epoch 2:  20%|█▉        | 29/147 [00:06<00:24,  4.74it/s, training_loss=0.313]\u001b[A\n","Epoch 2:  20%|█▉        | 29/147 [00:06<00:24,  4.74it/s, training_loss=0.302]\u001b[A\n","Epoch 2:  20%|██        | 30/147 [00:06<00:24,  4.74it/s, training_loss=0.302]\u001b[A\n","Epoch 2:  20%|██        | 30/147 [00:06<00:24,  4.74it/s, training_loss=0.364]\u001b[A\n","Epoch 2:  21%|██        | 31/147 [00:06<00:24,  4.74it/s, training_loss=0.364]\u001b[A\n","Epoch 2:  21%|██        | 31/147 [00:06<00:24,  4.74it/s, training_loss=0.277]\u001b[A\n","Epoch 2:  22%|██▏       | 32/147 [00:06<00:24,  4.67it/s, training_loss=0.277]\u001b[A\n","Epoch 2:  22%|██▏       | 32/147 [00:06<00:24,  4.67it/s, training_loss=0.328]\u001b[A\n","Epoch 2:  22%|██▏       | 33/147 [00:06<00:24,  4.73it/s, training_loss=0.328]\u001b[A\n","Epoch 2:  22%|██▏       | 33/147 [00:07<00:24,  4.73it/s, training_loss=0.315]\u001b[A\n","Epoch 2:  23%|██▎       | 34/147 [00:07<00:23,  4.75it/s, training_loss=0.315]\u001b[A\n","Epoch 2:  23%|██▎       | 34/147 [00:07<00:23,  4.75it/s, training_loss=0.289]\u001b[A\n","Epoch 2:  24%|██▍       | 35/147 [00:07<00:23,  4.74it/s, training_loss=0.289]\u001b[A\n","Epoch 2:  24%|██▍       | 35/147 [00:07<00:23,  4.74it/s, training_loss=0.387]\u001b[A\n","Epoch 2:  24%|██▍       | 36/147 [00:07<00:23,  4.70it/s, training_loss=0.387]\u001b[A\n","Epoch 2:  24%|██▍       | 36/147 [00:07<00:23,  4.70it/s, training_loss=0.353]\u001b[A\n","Epoch 2:  25%|██▌       | 37/147 [00:07<00:23,  4.67it/s, training_loss=0.353]\u001b[A\n","Epoch 2:  25%|██▌       | 37/147 [00:08<00:23,  4.67it/s, training_loss=0.375]\u001b[A\n","Epoch 2:  26%|██▌       | 38/147 [00:08<00:23,  4.67it/s, training_loss=0.375]\u001b[A\n","Epoch 2:  26%|██▌       | 38/147 [00:08<00:23,  4.67it/s, training_loss=0.294]\u001b[A\n","Epoch 2:  27%|██▋       | 39/147 [00:08<00:23,  4.66it/s, training_loss=0.294]\u001b[A\n","Epoch 2:  27%|██▋       | 39/147 [00:08<00:23,  4.66it/s, training_loss=0.353]\u001b[A\n","Epoch 2:  27%|██▋       | 40/147 [00:08<00:23,  4.64it/s, training_loss=0.353]\u001b[A\n","Epoch 2:  27%|██▋       | 40/147 [00:08<00:23,  4.64it/s, training_loss=0.383]\u001b[A\n","Epoch 2:  28%|██▊       | 41/147 [00:08<00:22,  4.62it/s, training_loss=0.383]\u001b[A\n","Epoch 2:  28%|██▊       | 41/147 [00:08<00:22,  4.62it/s, training_loss=0.300]\u001b[A\n","Epoch 2:  29%|██▊       | 42/147 [00:08<00:22,  4.68it/s, training_loss=0.300]\u001b[A\n","Epoch 2:  29%|██▊       | 42/147 [00:09<00:22,  4.68it/s, training_loss=0.339]\u001b[A\n","Epoch 2:  29%|██▉       | 43/147 [00:09<00:22,  4.72it/s, training_loss=0.339]\u001b[A\n","Epoch 2:  29%|██▉       | 43/147 [00:09<00:22,  4.72it/s, training_loss=0.264]\u001b[A\n","Epoch 2:  30%|██▉       | 44/147 [00:09<00:21,  4.69it/s, training_loss=0.264]\u001b[A\n","Epoch 2:  30%|██▉       | 44/147 [00:09<00:21,  4.69it/s, training_loss=0.252]\u001b[A\n","Epoch 2:  31%|███       | 45/147 [00:09<00:21,  4.71it/s, training_loss=0.252]\u001b[A\n","Epoch 2:  31%|███       | 45/147 [00:09<00:21,  4.71it/s, training_loss=0.296]\u001b[A\n","Epoch 2:  31%|███▏      | 46/147 [00:09<00:21,  4.70it/s, training_loss=0.296]\u001b[A\n","Epoch 2:  31%|███▏      | 46/147 [00:09<00:21,  4.70it/s, training_loss=0.328]\u001b[A\n","Epoch 2:  32%|███▏      | 47/147 [00:09<00:21,  4.73it/s, training_loss=0.328]\u001b[A\n","Epoch 2:  32%|███▏      | 47/147 [00:10<00:21,  4.73it/s, training_loss=0.344]\u001b[A\n","Epoch 2:  33%|███▎      | 48/147 [00:10<00:21,  4.71it/s, training_loss=0.344]\u001b[A\n","Epoch 2:  33%|███▎      | 48/147 [00:10<00:21,  4.71it/s, training_loss=0.324]\u001b[A\n","Epoch 2:  33%|███▎      | 49/147 [00:10<00:20,  4.68it/s, training_loss=0.324]\u001b[A\n","Epoch 2:  33%|███▎      | 49/147 [00:10<00:20,  4.68it/s, training_loss=0.299]\u001b[A\n","Epoch 2:  34%|███▍      | 50/147 [00:10<00:20,  4.70it/s, training_loss=0.299]\u001b[A\n","Epoch 2:  34%|███▍      | 50/147 [00:10<00:20,  4.70it/s, training_loss=0.299]\u001b[A\n","Epoch 2:  35%|███▍      | 51/147 [00:10<00:20,  4.71it/s, training_loss=0.299]\u001b[A\n","Epoch 2:  35%|███▍      | 51/147 [00:11<00:20,  4.71it/s, training_loss=0.423]\u001b[A\n","Epoch 2:  35%|███▌      | 52/147 [00:11<00:20,  4.71it/s, training_loss=0.423]\u001b[A\n","Epoch 2:  35%|███▌      | 52/147 [00:11<00:20,  4.71it/s, training_loss=0.316]\u001b[A\n","Epoch 2:  36%|███▌      | 53/147 [00:11<00:19,  4.71it/s, training_loss=0.316]\u001b[A\n","Epoch 2:  36%|███▌      | 53/147 [00:11<00:19,  4.71it/s, training_loss=0.249]\u001b[A\n","Epoch 2:  37%|███▋      | 54/147 [00:11<00:19,  4.71it/s, training_loss=0.249]\u001b[A\n","Epoch 2:  37%|███▋      | 54/147 [00:11<00:19,  4.71it/s, training_loss=0.384]\u001b[A\n","Epoch 2:  37%|███▋      | 55/147 [00:11<00:19,  4.70it/s, training_loss=0.384]\u001b[A\n","Epoch 2:  37%|███▋      | 55/147 [00:11<00:19,  4.70it/s, training_loss=0.331]\u001b[A\n","Epoch 2:  38%|███▊      | 56/147 [00:11<00:19,  4.70it/s, training_loss=0.331]\u001b[A\n","Epoch 2:  38%|███▊      | 56/147 [00:12<00:19,  4.70it/s, training_loss=0.297]\u001b[A\n","Epoch 2:  39%|███▉      | 57/147 [00:12<00:19,  4.73it/s, training_loss=0.297]\u001b[A\n","Epoch 2:  39%|███▉      | 57/147 [00:12<00:19,  4.73it/s, training_loss=0.253]\u001b[A\n","Epoch 2:  39%|███▉      | 58/147 [00:12<00:18,  4.76it/s, training_loss=0.253]\u001b[A\n","Epoch 2:  39%|███▉      | 58/147 [00:12<00:18,  4.76it/s, training_loss=0.294]\u001b[A\n","Epoch 2:  40%|████      | 59/147 [00:12<00:18,  4.78it/s, training_loss=0.294]\u001b[A\n","Epoch 2:  40%|████      | 59/147 [00:12<00:18,  4.78it/s, training_loss=0.293]\u001b[A\n","Epoch 2:  41%|████      | 60/147 [00:12<00:18,  4.74it/s, training_loss=0.293]\u001b[A\n","Epoch 2:  41%|████      | 60/147 [00:12<00:18,  4.74it/s, training_loss=0.283]\u001b[A\n","Epoch 2:  41%|████▏     | 61/147 [00:12<00:18,  4.75it/s, training_loss=0.283]\u001b[A\n","Epoch 2:  41%|████▏     | 61/147 [00:13<00:18,  4.75it/s, training_loss=0.278]\u001b[A\n","Epoch 2:  42%|████▏     | 62/147 [00:13<00:17,  4.75it/s, training_loss=0.278]\u001b[A\n","Epoch 2:  42%|████▏     | 62/147 [00:13<00:17,  4.75it/s, training_loss=0.246]\u001b[A\n","Epoch 2:  43%|████▎     | 63/147 [00:13<00:17,  4.76it/s, training_loss=0.246]\u001b[A\n","Epoch 2:  43%|████▎     | 63/147 [00:13<00:17,  4.76it/s, training_loss=0.329]\u001b[A\n","Epoch 2:  44%|████▎     | 64/147 [00:13<00:17,  4.76it/s, training_loss=0.329]\u001b[A\n","Epoch 2:  44%|████▎     | 64/147 [00:13<00:17,  4.76it/s, training_loss=0.332]\u001b[A\n","Epoch 2:  44%|████▍     | 65/147 [00:13<00:17,  4.75it/s, training_loss=0.332]\u001b[A\n","Epoch 2:  44%|████▍     | 65/147 [00:13<00:17,  4.75it/s, training_loss=0.284]\u001b[A\n","Epoch 2:  45%|████▍     | 66/147 [00:13<00:17,  4.76it/s, training_loss=0.284]\u001b[A\n","Epoch 2:  45%|████▍     | 66/147 [00:14<00:17,  4.76it/s, training_loss=0.247]\u001b[A\n","Epoch 2:  46%|████▌     | 67/147 [00:14<00:16,  4.75it/s, training_loss=0.247]\u001b[A\n","Epoch 2:  46%|████▌     | 67/147 [00:14<00:16,  4.75it/s, training_loss=0.338]\u001b[A\n","Epoch 2:  46%|████▋     | 68/147 [00:14<00:16,  4.78it/s, training_loss=0.338]\u001b[A\n","Epoch 2:  46%|████▋     | 68/147 [00:14<00:16,  4.78it/s, training_loss=0.292]\u001b[A\n","Epoch 2:  47%|████▋     | 69/147 [00:14<00:16,  4.77it/s, training_loss=0.292]\u001b[A\n","Epoch 2:  47%|████▋     | 69/147 [00:14<00:16,  4.77it/s, training_loss=0.297]\u001b[A\n","Epoch 2:  48%|████▊     | 70/147 [00:14<00:16,  4.74it/s, training_loss=0.297]\u001b[A\n","Epoch 2:  48%|████▊     | 70/147 [00:15<00:16,  4.74it/s, training_loss=0.283]\u001b[A\n","Epoch 2:  48%|████▊     | 71/147 [00:15<00:16,  4.74it/s, training_loss=0.283]\u001b[A\n","Epoch 2:  48%|████▊     | 71/147 [00:15<00:16,  4.74it/s, training_loss=0.214]\u001b[A\n","Epoch 2:  49%|████▉     | 72/147 [00:15<00:15,  4.72it/s, training_loss=0.214]\u001b[A\n","Epoch 2:  49%|████▉     | 72/147 [00:15<00:15,  4.72it/s, training_loss=0.264]\u001b[A\n","Epoch 2:  50%|████▉     | 73/147 [00:15<00:15,  4.78it/s, training_loss=0.264]\u001b[A\n","Epoch 2:  50%|████▉     | 73/147 [00:15<00:15,  4.78it/s, training_loss=0.275]\u001b[A\n","Epoch 2:  50%|█████     | 74/147 [00:15<00:15,  4.77it/s, training_loss=0.275]\u001b[A\n","Epoch 2:  50%|█████     | 74/147 [00:15<00:15,  4.77it/s, training_loss=0.329]\u001b[A\n","Epoch 2:  51%|█████     | 75/147 [00:15<00:15,  4.78it/s, training_loss=0.329]\u001b[A\n","Epoch 2:  51%|█████     | 75/147 [00:16<00:15,  4.78it/s, training_loss=0.253]\u001b[A\n","Epoch 2:  52%|█████▏    | 76/147 [00:16<00:14,  4.78it/s, training_loss=0.253]\u001b[A\n","Epoch 2:  52%|█████▏    | 76/147 [00:16<00:14,  4.78it/s, training_loss=0.409]\u001b[A\n","Epoch 2:  52%|█████▏    | 77/147 [00:16<00:14,  4.79it/s, training_loss=0.409]\u001b[A\n","Epoch 2:  52%|█████▏    | 77/147 [00:16<00:14,  4.79it/s, training_loss=0.307]\u001b[A\n","Epoch 2:  53%|█████▎    | 78/147 [00:16<00:14,  4.82it/s, training_loss=0.307]\u001b[A\n","Epoch 2:  53%|█████▎    | 78/147 [00:16<00:14,  4.82it/s, training_loss=0.214]\u001b[A\n","Epoch 2:  54%|█████▎    | 79/147 [00:16<00:14,  4.82it/s, training_loss=0.214]\u001b[A\n","Epoch 2:  54%|█████▎    | 79/147 [00:16<00:14,  4.82it/s, training_loss=0.265]\u001b[A\n","Epoch 2:  54%|█████▍    | 80/147 [00:16<00:13,  4.80it/s, training_loss=0.265]\u001b[A\n","Epoch 2:  54%|█████▍    | 80/147 [00:17<00:13,  4.80it/s, training_loss=0.229]\u001b[A\n","Epoch 2:  55%|█████▌    | 81/147 [00:17<00:13,  4.76it/s, training_loss=0.229]\u001b[A\n","Epoch 2:  55%|█████▌    | 81/147 [00:17<00:13,  4.76it/s, training_loss=0.331]\u001b[A\n","Epoch 2:  56%|█████▌    | 82/147 [00:17<00:13,  4.77it/s, training_loss=0.331]\u001b[A\n","Epoch 2:  56%|█████▌    | 82/147 [00:17<00:13,  4.77it/s, training_loss=0.295]\u001b[A\n","Epoch 2:  56%|█████▋    | 83/147 [00:17<00:13,  4.80it/s, training_loss=0.295]\u001b[A\n","Epoch 2:  56%|█████▋    | 83/147 [00:17<00:13,  4.80it/s, training_loss=0.343]\u001b[A\n","Epoch 2:  57%|█████▋    | 84/147 [00:17<00:13,  4.80it/s, training_loss=0.343]\u001b[A\n","Epoch 2:  57%|█████▋    | 84/147 [00:17<00:13,  4.80it/s, training_loss=0.445]\u001b[A\n","Epoch 2:  58%|█████▊    | 85/147 [00:17<00:12,  4.81it/s, training_loss=0.445]\u001b[A\n","Epoch 2:  58%|█████▊    | 85/147 [00:18<00:12,  4.81it/s, training_loss=0.268]\u001b[A\n","Epoch 2:  59%|█████▊    | 86/147 [00:18<00:12,  4.78it/s, training_loss=0.268]\u001b[A\n","Epoch 2:  59%|█████▊    | 86/147 [00:18<00:12,  4.78it/s, training_loss=0.290]\u001b[A\n","Epoch 2:  59%|█████▉    | 87/147 [00:18<00:12,  4.78it/s, training_loss=0.290]\u001b[A\n","Epoch 2:  59%|█████▉    | 87/147 [00:18<00:12,  4.78it/s, training_loss=0.376]\u001b[A\n","Epoch 2:  60%|█████▉    | 88/147 [00:18<00:12,  4.80it/s, training_loss=0.376]\u001b[A\n","Epoch 2:  60%|█████▉    | 88/147 [00:18<00:12,  4.80it/s, training_loss=0.261]\u001b[A\n","Epoch 2:  61%|██████    | 89/147 [00:18<00:12,  4.77it/s, training_loss=0.261]\u001b[A\n","Epoch 2:  61%|██████    | 89/147 [00:19<00:12,  4.77it/s, training_loss=0.247]\u001b[A\n","Epoch 2:  61%|██████    | 90/147 [00:19<00:11,  4.77it/s, training_loss=0.247]\u001b[A\n","Epoch 2:  61%|██████    | 90/147 [00:19<00:11,  4.77it/s, training_loss=0.302]\u001b[A\n","Epoch 2:  62%|██████▏   | 91/147 [00:19<00:11,  4.77it/s, training_loss=0.302]\u001b[A\n","Epoch 2:  62%|██████▏   | 91/147 [00:19<00:11,  4.77it/s, training_loss=0.318]\u001b[A\n","Epoch 2:  63%|██████▎   | 92/147 [00:19<00:11,  4.76it/s, training_loss=0.318]\u001b[A\n","Epoch 2:  63%|██████▎   | 92/147 [00:19<00:11,  4.76it/s, training_loss=0.365]\u001b[A\n","Epoch 2:  63%|██████▎   | 93/147 [00:19<00:11,  4.75it/s, training_loss=0.365]\u001b[A\n","Epoch 2:  63%|██████▎   | 93/147 [00:19<00:11,  4.75it/s, training_loss=0.243]\u001b[A\n","Epoch 2:  64%|██████▍   | 94/147 [00:19<00:11,  4.70it/s, training_loss=0.243]\u001b[A\n","Epoch 2:  64%|██████▍   | 94/147 [00:20<00:11,  4.70it/s, training_loss=0.220]\u001b[A\n","Epoch 2:  65%|██████▍   | 95/147 [00:20<00:10,  4.74it/s, training_loss=0.220]\u001b[A\n","Epoch 2:  65%|██████▍   | 95/147 [00:20<00:10,  4.74it/s, training_loss=0.437]\u001b[A\n","Epoch 2:  65%|██████▌   | 96/147 [00:20<00:10,  4.75it/s, training_loss=0.437]\u001b[A\n","Epoch 2:  65%|██████▌   | 96/147 [00:20<00:10,  4.75it/s, training_loss=0.389]\u001b[A\n","Epoch 2:  66%|██████▌   | 97/147 [00:20<00:10,  4.73it/s, training_loss=0.389]\u001b[A\n","Epoch 2:  66%|██████▌   | 97/147 [00:20<00:10,  4.73it/s, training_loss=0.327]\u001b[A\n","Epoch 2:  67%|██████▋   | 98/147 [00:20<00:10,  4.71it/s, training_loss=0.327]\u001b[A\n","Epoch 2:  67%|██████▋   | 98/147 [00:20<00:10,  4.71it/s, training_loss=0.336]\u001b[A\n","Epoch 2:  67%|██████▋   | 99/147 [00:20<00:10,  4.73it/s, training_loss=0.336]\u001b[A\n","Epoch 2:  67%|██████▋   | 99/147 [00:21<00:10,  4.73it/s, training_loss=0.291]\u001b[A\n","Epoch 2:  68%|██████▊   | 100/147 [00:21<00:09,  4.75it/s, training_loss=0.291]\u001b[A\n","Epoch 2:  68%|██████▊   | 100/147 [00:21<00:09,  4.75it/s, training_loss=0.289]\u001b[A\n","Epoch 2:  69%|██████▊   | 101/147 [00:21<00:09,  4.74it/s, training_loss=0.289]\u001b[A\n","Epoch 2:  69%|██████▊   | 101/147 [00:21<00:09,  4.74it/s, training_loss=0.249]\u001b[A\n","Epoch 2:  69%|██████▉   | 102/147 [00:21<00:09,  4.73it/s, training_loss=0.249]\u001b[A\n","Epoch 2:  69%|██████▉   | 102/147 [00:21<00:09,  4.73it/s, training_loss=0.271]\u001b[A\n","Epoch 2:  70%|███████   | 103/147 [00:21<00:09,  4.71it/s, training_loss=0.271]\u001b[A\n","Epoch 2:  70%|███████   | 103/147 [00:21<00:09,  4.71it/s, training_loss=0.291]\u001b[A\n","Epoch 2:  71%|███████   | 104/147 [00:21<00:09,  4.73it/s, training_loss=0.291]\u001b[A\n","Epoch 2:  71%|███████   | 104/147 [00:22<00:09,  4.73it/s, training_loss=0.341]\u001b[A\n","Epoch 2:  71%|███████▏  | 105/147 [00:22<00:08,  4.72it/s, training_loss=0.341]\u001b[A\n","Epoch 2:  71%|███████▏  | 105/147 [00:22<00:08,  4.72it/s, training_loss=0.277]\u001b[A\n","Epoch 2:  72%|███████▏  | 106/147 [00:22<00:08,  4.77it/s, training_loss=0.277]\u001b[A\n","Epoch 2:  72%|███████▏  | 106/147 [00:22<00:08,  4.77it/s, training_loss=0.328]\u001b[A\n","Epoch 2:  73%|███████▎  | 107/147 [00:22<00:08,  4.82it/s, training_loss=0.328]\u001b[A\n","Epoch 2:  73%|███████▎  | 107/147 [00:22<00:08,  4.82it/s, training_loss=0.207]\u001b[A\n","Epoch 2:  73%|███████▎  | 108/147 [00:22<00:08,  4.82it/s, training_loss=0.207]\u001b[A\n","Epoch 2:  73%|███████▎  | 108/147 [00:23<00:08,  4.82it/s, training_loss=0.325]\u001b[A\n","Epoch 2:  74%|███████▍  | 109/147 [00:23<00:07,  4.82it/s, training_loss=0.325]\u001b[A\n","Epoch 2:  74%|███████▍  | 109/147 [00:23<00:07,  4.82it/s, training_loss=0.257]\u001b[A\n","Epoch 2:  75%|███████▍  | 110/147 [00:23<00:07,  4.82it/s, training_loss=0.257]\u001b[A\n","Epoch 2:  75%|███████▍  | 110/147 [00:23<00:07,  4.82it/s, training_loss=0.273]\u001b[A\n","Epoch 2:  76%|███████▌  | 111/147 [00:23<00:07,  4.81it/s, training_loss=0.273]\u001b[A\n","Epoch 2:  76%|███████▌  | 111/147 [00:23<00:07,  4.81it/s, training_loss=0.359]\u001b[A\n","Epoch 2:  76%|███████▌  | 112/147 [00:23<00:07,  4.81it/s, training_loss=0.359]\u001b[A\n","Epoch 2:  76%|███████▌  | 112/147 [00:23<00:07,  4.81it/s, training_loss=0.300]\u001b[A\n","Epoch 2:  77%|███████▋  | 113/147 [00:23<00:07,  4.82it/s, training_loss=0.300]\u001b[A\n","Epoch 2:  77%|███████▋  | 113/147 [00:24<00:07,  4.82it/s, training_loss=0.257]\u001b[A\n","Epoch 2:  78%|███████▊  | 114/147 [00:24<00:06,  4.82it/s, training_loss=0.257]\u001b[A\n","Epoch 2:  78%|███████▊  | 114/147 [00:24<00:06,  4.82it/s, training_loss=0.319]\u001b[A\n","Epoch 2:  78%|███████▊  | 115/147 [00:24<00:06,  4.80it/s, training_loss=0.319]\u001b[A\n","Epoch 2:  78%|███████▊  | 115/147 [00:24<00:06,  4.80it/s, training_loss=0.396]\u001b[A\n","Epoch 2:  79%|███████▉  | 116/147 [00:24<00:06,  4.82it/s, training_loss=0.396]\u001b[A\n","Epoch 2:  79%|███████▉  | 116/147 [00:24<00:06,  4.82it/s, training_loss=0.351]\u001b[A\n","Epoch 2:  80%|███████▉  | 117/147 [00:24<00:06,  4.78it/s, training_loss=0.351]\u001b[A\n","Epoch 2:  80%|███████▉  | 117/147 [00:24<00:06,  4.78it/s, training_loss=0.258]\u001b[A\n","Epoch 2:  80%|████████  | 118/147 [00:24<00:06,  4.81it/s, training_loss=0.258]\u001b[A\n","Epoch 2:  80%|████████  | 118/147 [00:25<00:06,  4.81it/s, training_loss=0.379]\u001b[A\n","Epoch 2:  81%|████████  | 119/147 [00:25<00:05,  4.81it/s, training_loss=0.379]\u001b[A\n","Epoch 2:  81%|████████  | 119/147 [00:25<00:05,  4.81it/s, training_loss=0.346]\u001b[A\n","Epoch 2:  82%|████████▏ | 120/147 [00:25<00:05,  4.81it/s, training_loss=0.346]\u001b[A\n","Epoch 2:  82%|████████▏ | 120/147 [00:25<00:05,  4.81it/s, training_loss=0.330]\u001b[A\n","Epoch 2:  82%|████████▏ | 121/147 [00:25<00:05,  4.81it/s, training_loss=0.330]\u001b[A\n","Epoch 2:  82%|████████▏ | 121/147 [00:25<00:05,  4.81it/s, training_loss=0.239]\u001b[A\n","Epoch 2:  83%|████████▎ | 122/147 [00:25<00:05,  4.79it/s, training_loss=0.239]\u001b[A\n","Epoch 2:  83%|████████▎ | 122/147 [00:25<00:05,  4.79it/s, training_loss=0.288]\u001b[A\n","Epoch 2:  84%|████████▎ | 123/147 [00:25<00:05,  4.80it/s, training_loss=0.288]\u001b[A\n","Epoch 2:  84%|████████▎ | 123/147 [00:26<00:05,  4.80it/s, training_loss=0.213]\u001b[A\n","Epoch 2:  84%|████████▍ | 124/147 [00:26<00:04,  4.81it/s, training_loss=0.213]\u001b[A\n","Epoch 2:  84%|████████▍ | 124/147 [00:26<00:04,  4.81it/s, training_loss=0.373]\u001b[A\n","Epoch 2:  85%|████████▌ | 125/147 [00:26<00:04,  4.79it/s, training_loss=0.373]\u001b[A\n","Epoch 2:  85%|████████▌ | 125/147 [00:26<00:04,  4.79it/s, training_loss=0.163]\u001b[A\n","Epoch 2:  86%|████████▌ | 126/147 [00:26<00:04,  4.80it/s, training_loss=0.163]\u001b[A\n","Epoch 2:  86%|████████▌ | 126/147 [00:26<00:04,  4.80it/s, training_loss=0.262]\u001b[A\n","Epoch 2:  86%|████████▋ | 127/147 [00:26<00:04,  4.81it/s, training_loss=0.262]\u001b[A\n","Epoch 2:  86%|████████▋ | 127/147 [00:26<00:04,  4.81it/s, training_loss=0.357]\u001b[A\n","Epoch 2:  87%|████████▋ | 128/147 [00:26<00:03,  4.83it/s, training_loss=0.357]\u001b[A\n","Epoch 2:  87%|████████▋ | 128/147 [00:27<00:03,  4.83it/s, training_loss=0.400]\u001b[A\n","Epoch 2:  88%|████████▊ | 129/147 [00:27<00:03,  4.80it/s, training_loss=0.400]\u001b[A\n","Epoch 2:  88%|████████▊ | 129/147 [00:27<00:03,  4.80it/s, training_loss=0.258]\u001b[A\n","Epoch 2:  88%|████████▊ | 130/147 [00:27<00:03,  4.79it/s, training_loss=0.258]\u001b[A\n","Epoch 2:  88%|████████▊ | 130/147 [00:27<00:03,  4.79it/s, training_loss=0.315]\u001b[A\n","Epoch 2:  89%|████████▉ | 131/147 [00:27<00:03,  4.79it/s, training_loss=0.315]\u001b[A\n","Epoch 2:  89%|████████▉ | 131/147 [00:27<00:03,  4.79it/s, training_loss=0.333]\u001b[A\n","Epoch 2:  90%|████████▉ | 132/147 [00:27<00:03,  4.82it/s, training_loss=0.333]\u001b[A\n","Epoch 2:  90%|████████▉ | 132/147 [00:27<00:03,  4.82it/s, training_loss=0.347]\u001b[A\n","Epoch 2:  90%|█████████ | 133/147 [00:28<00:02,  4.82it/s, training_loss=0.347]\u001b[A\n","Epoch 2:  90%|█████████ | 133/147 [00:28<00:02,  4.82it/s, training_loss=0.355]\u001b[A\n","Epoch 2:  91%|█████████ | 134/147 [00:28<00:02,  4.83it/s, training_loss=0.355]\u001b[A\n","Epoch 2:  91%|█████████ | 134/147 [00:28<00:02,  4.83it/s, training_loss=0.310]\u001b[A\n","Epoch 2:  92%|█████████▏| 135/147 [00:28<00:02,  4.85it/s, training_loss=0.310]\u001b[A\n","Epoch 2:  92%|█████████▏| 135/147 [00:28<00:02,  4.85it/s, training_loss=0.382]\u001b[A\n","Epoch 2:  93%|█████████▎| 136/147 [00:28<00:02,  4.88it/s, training_loss=0.382]\u001b[A\n","Epoch 2:  93%|█████████▎| 136/147 [00:28<00:02,  4.88it/s, training_loss=0.458]\u001b[A\n","Epoch 2:  93%|█████████▎| 137/147 [00:28<00:02,  4.85it/s, training_loss=0.458]\u001b[A\n","Epoch 2:  93%|█████████▎| 137/147 [00:29<00:02,  4.85it/s, training_loss=0.195]\u001b[A\n","Epoch 2:  94%|█████████▍| 138/147 [00:29<00:01,  4.83it/s, training_loss=0.195]\u001b[A\n","Epoch 2:  94%|█████████▍| 138/147 [00:29<00:01,  4.83it/s, training_loss=0.278]\u001b[A\n","Epoch 2:  95%|█████████▍| 139/147 [00:29<00:01,  4.83it/s, training_loss=0.278]\u001b[A\n","Epoch 2:  95%|█████████▍| 139/147 [00:29<00:01,  4.83it/s, training_loss=0.225]\u001b[A\n","Epoch 2:  95%|█████████▌| 140/147 [00:29<00:01,  4.81it/s, training_loss=0.225]\u001b[A\n","Epoch 2:  95%|█████████▌| 140/147 [00:29<00:01,  4.81it/s, training_loss=0.434]\u001b[A\n","Epoch 2:  96%|█████████▌| 141/147 [00:29<00:01,  4.83it/s, training_loss=0.434]\u001b[A\n","Epoch 2:  96%|█████████▌| 141/147 [00:29<00:01,  4.83it/s, training_loss=0.286]\u001b[A\n","Epoch 2:  97%|█████████▋| 142/147 [00:29<00:01,  4.82it/s, training_loss=0.286]\u001b[A\n","Epoch 2:  97%|█████████▋| 142/147 [00:30<00:01,  4.82it/s, training_loss=0.195]\u001b[A\n","Epoch 2:  97%|█████████▋| 143/147 [00:30<00:00,  5.07it/s, training_loss=0.195]\u001b[A\n","Epoch 2:  97%|█████████▋| 143/147 [00:30<00:00,  5.07it/s, training_loss=0.326]\u001b[A\n","Epoch 2:  98%|█████████▊| 144/147 [00:30<00:00,  4.89it/s, training_loss=0.326]\u001b[A\n","Epoch 2:  98%|█████████▊| 144/147 [00:30<00:00,  4.89it/s, training_loss=0.403]\u001b[A\n","Epoch 2:  99%|█████████▊| 145/147 [00:30<00:00,  4.84it/s, training_loss=0.403]\u001b[A\n","Epoch 2:  99%|█████████▊| 145/147 [00:30<00:00,  4.84it/s, training_loss=0.321]\u001b[A\n","Epoch 2:  99%|█████████▉| 146/147 [00:30<00:00,  4.84it/s, training_loss=0.321]\u001b[A\n","Epoch 2:  99%|█████████▉| 146/147 [00:30<00:00,  4.84it/s, training_loss=0.432]\u001b[A\n","Epoch 2: 100%|██████████| 147/147 [00:30<00:00,  5.15it/s, training_loss=0.432]\u001b[A\n","Epoch Progress:  40%|████      | 2/5 [01:07<01:40, 33.52s/it]\n","Epoch 3:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\n","Epoch 3:   0%|          | 0/147 [00:00<?, ?it/s, training_loss=0.289]\u001b[A\n","Epoch 3:   1%|          | 1/147 [00:00<00:29,  4.94it/s, training_loss=0.289]\u001b[A\n","Epoch 3:   1%|          | 1/147 [00:00<00:29,  4.94it/s, training_loss=0.278]\u001b[A\n","Epoch 3:   1%|▏         | 2/147 [00:00<00:29,  4.88it/s, training_loss=0.278]\u001b[A\n","Epoch 3:   1%|▏         | 2/147 [00:00<00:29,  4.88it/s, training_loss=0.345]\u001b[A\n","Epoch 3:   2%|▏         | 3/147 [00:00<00:29,  4.90it/s, training_loss=0.345]\u001b[A\n","Epoch 3:   2%|▏         | 3/147 [00:00<00:29,  4.90it/s, training_loss=0.284]\u001b[A\n","Epoch 3:   3%|▎         | 4/147 [00:00<00:29,  4.83it/s, training_loss=0.284]\u001b[A\n","Epoch 3:   3%|▎         | 4/147 [00:01<00:29,  4.83it/s, training_loss=0.280]\u001b[A\n","Epoch 3:   3%|▎         | 5/147 [00:01<00:29,  4.78it/s, training_loss=0.280]\u001b[A\n","Epoch 3:   3%|▎         | 5/147 [00:01<00:29,  4.78it/s, training_loss=0.202]\u001b[A\n","Epoch 3:   4%|▍         | 6/147 [00:01<00:29,  4.76it/s, training_loss=0.202]\u001b[A\n","Epoch 3:   4%|▍         | 6/147 [00:01<00:29,  4.76it/s, training_loss=0.239]\u001b[A\n","Epoch 3:   5%|▍         | 7/147 [00:01<00:29,  4.78it/s, training_loss=0.239]\u001b[A\n","Epoch 3:   5%|▍         | 7/147 [00:01<00:29,  4.78it/s, training_loss=0.256]\u001b[A\n","Epoch 3:   5%|▌         | 8/147 [00:01<00:29,  4.77it/s, training_loss=0.256]\u001b[A\n","Epoch 3:   5%|▌         | 8/147 [00:01<00:29,  4.77it/s, training_loss=0.370]\u001b[A\n","Epoch 3:   6%|▌         | 9/147 [00:01<00:28,  4.77it/s, training_loss=0.370]\u001b[A\n","Epoch 3:   6%|▌         | 9/147 [00:02<00:28,  4.77it/s, training_loss=0.397]\u001b[A\n","Epoch 3:   7%|▋         | 10/147 [00:02<00:28,  4.80it/s, training_loss=0.397]\u001b[A\n","Epoch 3:   7%|▋         | 10/147 [00:02<00:28,  4.80it/s, training_loss=0.271]\u001b[A\n","Epoch 3:   7%|▋         | 11/147 [00:02<00:28,  4.82it/s, training_loss=0.271]\u001b[A\n","Epoch 3:   7%|▋         | 11/147 [00:02<00:28,  4.82it/s, training_loss=0.364]\u001b[A\n","Epoch 3:   8%|▊         | 12/147 [00:02<00:27,  4.84it/s, training_loss=0.364]\u001b[A\n","Epoch 3:   8%|▊         | 12/147 [00:02<00:27,  4.84it/s, training_loss=0.168]\u001b[A\n","Epoch 3:   9%|▉         | 13/147 [00:02<00:27,  4.85it/s, training_loss=0.168]\u001b[A\n","Epoch 3:   9%|▉         | 13/147 [00:02<00:27,  4.85it/s, training_loss=0.357]\u001b[A\n","Epoch 3:  10%|▉         | 14/147 [00:02<00:27,  4.86it/s, training_loss=0.357]\u001b[A\n","Epoch 3:  10%|▉         | 14/147 [00:03<00:27,  4.86it/s, training_loss=0.288]\u001b[A\n","Epoch 3:  10%|█         | 15/147 [00:03<00:27,  4.87it/s, training_loss=0.288]\u001b[A\n","Epoch 3:  10%|█         | 15/147 [00:03<00:27,  4.87it/s, training_loss=0.214]\u001b[A\n","Epoch 3:  11%|█         | 16/147 [00:03<00:26,  4.89it/s, training_loss=0.214]\u001b[A\n","Epoch 3:  11%|█         | 16/147 [00:03<00:26,  4.89it/s, training_loss=0.215]\u001b[A\n","Epoch 3:  12%|█▏        | 17/147 [00:03<00:26,  4.90it/s, training_loss=0.215]\u001b[A\n","Epoch 3:  12%|█▏        | 17/147 [00:03<00:26,  4.90it/s, training_loss=0.189]\u001b[A\n","Epoch 3:  12%|█▏        | 18/147 [00:03<00:26,  4.88it/s, training_loss=0.189]\u001b[A\n","Epoch 3:  12%|█▏        | 18/147 [00:03<00:26,  4.88it/s, training_loss=0.222]\u001b[A\n","Epoch 3:  13%|█▎        | 19/147 [00:03<00:26,  4.87it/s, training_loss=0.222]\u001b[A\n","Epoch 3:  13%|█▎        | 19/147 [00:04<00:26,  4.87it/s, training_loss=0.347]\u001b[A\n","Epoch 3:  14%|█▎        | 20/147 [00:04<00:26,  4.86it/s, training_loss=0.347]\u001b[A\n","Epoch 3:  14%|█▎        | 20/147 [00:04<00:26,  4.86it/s, training_loss=0.318]\u001b[A\n","Epoch 3:  14%|█▍        | 21/147 [00:04<00:25,  4.85it/s, training_loss=0.318]\u001b[A\n","Epoch 3:  14%|█▍        | 21/147 [00:04<00:25,  4.85it/s, training_loss=0.343]\u001b[A\n","Epoch 3:  15%|█▍        | 22/147 [00:04<00:25,  4.88it/s, training_loss=0.343]\u001b[A\n","Epoch 3:  15%|█▍        | 22/147 [00:04<00:25,  4.88it/s, training_loss=0.244]\u001b[A\n","Epoch 3:  16%|█▌        | 23/147 [00:04<00:25,  4.90it/s, training_loss=0.244]\u001b[A\n","Epoch 3:  16%|█▌        | 23/147 [00:04<00:25,  4.90it/s, training_loss=0.392]\u001b[A\n","Epoch 3:  16%|█▋        | 24/147 [00:04<00:25,  4.91it/s, training_loss=0.392]\u001b[A\n","Epoch 3:  16%|█▋        | 24/147 [00:05<00:25,  4.91it/s, training_loss=0.291]\u001b[A\n","Epoch 3:  17%|█▋        | 25/147 [00:05<00:25,  4.86it/s, training_loss=0.291]\u001b[A\n","Epoch 3:  17%|█▋        | 25/147 [00:05<00:25,  4.86it/s, training_loss=0.297]\u001b[A\n","Epoch 3:  18%|█▊        | 26/147 [00:05<00:25,  4.82it/s, training_loss=0.297]\u001b[A\n","Epoch 3:  18%|█▊        | 26/147 [00:05<00:25,  4.82it/s, training_loss=0.246]\u001b[A\n","Epoch 3:  18%|█▊        | 27/147 [00:05<00:24,  4.82it/s, training_loss=0.246]\u001b[A\n","Epoch 3:  18%|█▊        | 27/147 [00:05<00:24,  4.82it/s, training_loss=0.373]\u001b[A\n","Epoch 3:  19%|█▉        | 28/147 [00:05<00:24,  4.82it/s, training_loss=0.373]\u001b[A\n","Epoch 3:  19%|█▉        | 28/147 [00:05<00:24,  4.82it/s, training_loss=0.320]\u001b[A\n","Epoch 3:  20%|█▉        | 29/147 [00:05<00:24,  4.85it/s, training_loss=0.320]\u001b[A\n","Epoch 3:  20%|█▉        | 29/147 [00:06<00:24,  4.85it/s, training_loss=0.236]\u001b[A\n","Epoch 3:  20%|██        | 30/147 [00:06<00:24,  4.83it/s, training_loss=0.236]\u001b[A\n","Epoch 3:  20%|██        | 30/147 [00:06<00:24,  4.83it/s, training_loss=0.189]\u001b[A\n","Epoch 3:  21%|██        | 31/147 [00:06<00:24,  4.78it/s, training_loss=0.189]\u001b[A\n","Epoch 3:  21%|██        | 31/147 [00:06<00:24,  4.78it/s, training_loss=0.283]\u001b[A\n","Epoch 3:  22%|██▏       | 32/147 [00:06<00:23,  4.82it/s, training_loss=0.283]\u001b[A\n","Epoch 3:  22%|██▏       | 32/147 [00:06<00:23,  4.82it/s, training_loss=0.247]\u001b[A\n","Epoch 3:  22%|██▏       | 33/147 [00:06<00:23,  4.84it/s, training_loss=0.247]\u001b[A\n","Epoch 3:  22%|██▏       | 33/147 [00:07<00:23,  4.84it/s, training_loss=0.250]\u001b[A\n","Epoch 3:  23%|██▎       | 34/147 [00:07<00:23,  4.86it/s, training_loss=0.250]\u001b[A\n","Epoch 3:  23%|██▎       | 34/147 [00:07<00:23,  4.86it/s, training_loss=0.256]\u001b[A\n","Epoch 3:  24%|██▍       | 35/147 [00:07<00:23,  4.82it/s, training_loss=0.256]\u001b[A\n","Epoch 3:  24%|██▍       | 35/147 [00:07<00:23,  4.82it/s, training_loss=0.249]\u001b[A\n","Epoch 3:  24%|██▍       | 36/147 [00:07<00:23,  4.79it/s, training_loss=0.249]\u001b[A\n","Epoch 3:  24%|██▍       | 36/147 [00:07<00:23,  4.79it/s, training_loss=0.301]\u001b[A\n","Epoch 3:  25%|██▌       | 37/147 [00:07<00:22,  4.81it/s, training_loss=0.301]\u001b[A\n","Epoch 3:  25%|██▌       | 37/147 [00:07<00:22,  4.81it/s, training_loss=0.276]\u001b[A\n","Epoch 3:  26%|██▌       | 38/147 [00:07<00:22,  4.84it/s, training_loss=0.276]\u001b[A\n","Epoch 3:  26%|██▌       | 38/147 [00:08<00:22,  4.84it/s, training_loss=0.284]\u001b[A\n","Epoch 3:  27%|██▋       | 39/147 [00:08<00:22,  4.84it/s, training_loss=0.284]\u001b[A\n","Epoch 3:  27%|██▋       | 39/147 [00:08<00:22,  4.84it/s, training_loss=0.230]\u001b[A\n","Epoch 3:  27%|██▋       | 40/147 [00:08<00:22,  4.86it/s, training_loss=0.230]\u001b[A\n","Epoch 3:  27%|██▋       | 40/147 [00:08<00:22,  4.86it/s, training_loss=0.210]\u001b[A\n","Epoch 3:  28%|██▊       | 41/147 [00:08<00:21,  4.85it/s, training_loss=0.210]\u001b[A\n","Epoch 3:  28%|██▊       | 41/147 [00:08<00:21,  4.85it/s, training_loss=0.218]\u001b[A\n","Epoch 3:  29%|██▊       | 42/147 [00:08<00:21,  4.88it/s, training_loss=0.218]\u001b[A\n","Epoch 3:  29%|██▊       | 42/147 [00:08<00:21,  4.88it/s, training_loss=0.295]\u001b[A\n","Epoch 3:  29%|██▉       | 43/147 [00:08<00:21,  4.88it/s, training_loss=0.295]\u001b[A\n","Epoch 3:  29%|██▉       | 43/147 [00:09<00:21,  4.88it/s, training_loss=0.194]\u001b[A\n","Epoch 3:  30%|██▉       | 44/147 [00:09<00:21,  4.85it/s, training_loss=0.194]\u001b[A\n","Epoch 3:  30%|██▉       | 44/147 [00:09<00:21,  4.85it/s, training_loss=0.241]\u001b[A\n","Epoch 3:  31%|███       | 45/147 [00:09<00:20,  4.87it/s, training_loss=0.241]\u001b[A\n","Epoch 3:  31%|███       | 45/147 [00:09<00:20,  4.87it/s, training_loss=0.238]\u001b[A\n","Epoch 3:  31%|███▏      | 46/147 [00:09<00:20,  4.89it/s, training_loss=0.238]\u001b[A\n","Epoch 3:  31%|███▏      | 46/147 [00:09<00:20,  4.89it/s, training_loss=0.271]\u001b[A\n","Epoch 3:  32%|███▏      | 47/147 [00:09<00:20,  4.87it/s, training_loss=0.271]\u001b[A\n","Epoch 3:  32%|███▏      | 47/147 [00:09<00:20,  4.87it/s, training_loss=0.300]\u001b[A\n","Epoch 3:  33%|███▎      | 48/147 [00:09<00:20,  4.83it/s, training_loss=0.300]\u001b[A\n","Epoch 3:  33%|███▎      | 48/147 [00:10<00:20,  4.83it/s, training_loss=0.244]\u001b[A\n","Epoch 3:  33%|███▎      | 49/147 [00:10<00:20,  4.81it/s, training_loss=0.244]\u001b[A\n","Epoch 3:  33%|███▎      | 49/147 [00:10<00:20,  4.81it/s, training_loss=0.346]\u001b[A\n","Epoch 3:  34%|███▍      | 50/147 [00:10<00:20,  4.81it/s, training_loss=0.346]\u001b[A\n","Epoch 3:  34%|███▍      | 50/147 [00:10<00:20,  4.81it/s, training_loss=0.252]\u001b[A\n","Epoch 3:  35%|███▍      | 51/147 [00:10<00:19,  4.83it/s, training_loss=0.252]\u001b[A\n","Epoch 3:  35%|███▍      | 51/147 [00:10<00:19,  4.83it/s, training_loss=0.169]\u001b[A\n","Epoch 3:  35%|███▌      | 52/147 [00:10<00:19,  4.83it/s, training_loss=0.169]\u001b[A\n","Epoch 3:  35%|███▌      | 52/147 [00:10<00:19,  4.83it/s, training_loss=0.236]\u001b[A\n","Epoch 3:  36%|███▌      | 53/147 [00:10<00:19,  4.79it/s, training_loss=0.236]\u001b[A\n","Epoch 3:  36%|███▌      | 53/147 [00:11<00:19,  4.79it/s, training_loss=0.207]\u001b[A\n","Epoch 3:  37%|███▋      | 54/147 [00:11<00:19,  4.78it/s, training_loss=0.207]\u001b[A\n","Epoch 3:  37%|███▋      | 54/147 [00:11<00:19,  4.78it/s, training_loss=0.298]\u001b[A\n","Epoch 3:  37%|███▋      | 55/147 [00:11<00:19,  4.78it/s, training_loss=0.298]\u001b[A\n","Epoch 3:  37%|███▋      | 55/147 [00:11<00:19,  4.78it/s, training_loss=0.241]\u001b[A\n","Epoch 3:  38%|███▊      | 56/147 [00:11<00:19,  4.78it/s, training_loss=0.241]\u001b[A\n","Epoch 3:  38%|███▊      | 56/147 [00:11<00:19,  4.78it/s, training_loss=0.287]\u001b[A\n","Epoch 3:  39%|███▉      | 57/147 [00:11<00:18,  4.79it/s, training_loss=0.287]\u001b[A\n","Epoch 3:  39%|███▉      | 57/147 [00:11<00:18,  4.79it/s, training_loss=0.397]\u001b[A\n","Epoch 3:  39%|███▉      | 58/147 [00:12<00:18,  4.76it/s, training_loss=0.397]\u001b[A\n","Epoch 3:  39%|███▉      | 58/147 [00:12<00:18,  4.76it/s, training_loss=0.357]\u001b[A\n","Epoch 3:  40%|████      | 59/147 [00:12<00:18,  4.77it/s, training_loss=0.357]\u001b[A\n","Epoch 3:  40%|████      | 59/147 [00:12<00:18,  4.77it/s, training_loss=0.457]\u001b[A\n","Epoch 3:  41%|████      | 60/147 [00:12<00:18,  4.81it/s, training_loss=0.457]\u001b[A\n","Epoch 3:  41%|████      | 60/147 [00:12<00:18,  4.81it/s, training_loss=0.369]\u001b[A\n","Epoch 3:  41%|████▏     | 61/147 [00:12<00:17,  4.83it/s, training_loss=0.369]\u001b[A\n","Epoch 3:  41%|████▏     | 61/147 [00:12<00:17,  4.83it/s, training_loss=0.224]\u001b[A\n","Epoch 3:  42%|████▏     | 62/147 [00:12<00:17,  4.80it/s, training_loss=0.224]\u001b[A\n","Epoch 3:  42%|████▏     | 62/147 [00:13<00:17,  4.80it/s, training_loss=0.303]\u001b[A\n","Epoch 3:  43%|████▎     | 63/147 [00:13<00:17,  4.78it/s, training_loss=0.303]\u001b[A\n","Epoch 3:  43%|████▎     | 63/147 [00:13<00:17,  4.78it/s, training_loss=0.201]\u001b[A\n","Epoch 3:  44%|████▎     | 64/147 [00:13<00:17,  4.72it/s, training_loss=0.201]\u001b[A\n","Epoch 3:  44%|████▎     | 64/147 [00:13<00:17,  4.72it/s, training_loss=0.245]\u001b[A\n","Epoch 3:  44%|████▍     | 65/147 [00:13<00:17,  4.77it/s, training_loss=0.245]\u001b[A\n","Epoch 3:  44%|████▍     | 65/147 [00:13<00:17,  4.77it/s, training_loss=0.317]\u001b[A\n","Epoch 3:  45%|████▍     | 66/147 [00:13<00:16,  4.78it/s, training_loss=0.317]\u001b[A\n","Epoch 3:  45%|████▍     | 66/147 [00:13<00:16,  4.78it/s, training_loss=0.328]\u001b[A\n","Epoch 3:  46%|████▌     | 67/147 [00:13<00:16,  4.78it/s, training_loss=0.328]\u001b[A\n","Epoch 3:  46%|████▌     | 67/147 [00:14<00:16,  4.78it/s, training_loss=0.410]\u001b[A\n","Epoch 3:  46%|████▋     | 68/147 [00:14<00:16,  4.75it/s, training_loss=0.410]\u001b[A\n","Epoch 3:  46%|████▋     | 68/147 [00:14<00:16,  4.75it/s, training_loss=0.233]\u001b[A\n","Epoch 3:  47%|████▋     | 69/147 [00:14<00:16,  4.76it/s, training_loss=0.233]\u001b[A\n","Epoch 3:  47%|████▋     | 69/147 [00:14<00:16,  4.76it/s, training_loss=0.270]\u001b[A\n","Epoch 3:  48%|████▊     | 70/147 [00:14<00:16,  4.77it/s, training_loss=0.270]\u001b[A\n","Epoch 3:  48%|████▊     | 70/147 [00:14<00:16,  4.77it/s, training_loss=0.297]\u001b[A\n","Epoch 3:  48%|████▊     | 71/147 [00:14<00:15,  4.78it/s, training_loss=0.297]\u001b[A\n","Epoch 3:  48%|████▊     | 71/147 [00:14<00:15,  4.78it/s, training_loss=0.238]\u001b[A\n","Epoch 3:  49%|████▉     | 72/147 [00:14<00:15,  4.82it/s, training_loss=0.238]\u001b[A\n","Epoch 3:  49%|████▉     | 72/147 [00:15<00:15,  4.82it/s, training_loss=0.275]\u001b[A\n","Epoch 3:  50%|████▉     | 73/147 [00:15<00:15,  4.82it/s, training_loss=0.275]\u001b[A\n","Epoch 3:  50%|████▉     | 73/147 [00:15<00:15,  4.82it/s, training_loss=0.225]\u001b[A\n","Epoch 3:  50%|█████     | 74/147 [00:15<00:15,  4.83it/s, training_loss=0.225]\u001b[A\n","Epoch 3:  50%|█████     | 74/147 [00:15<00:15,  4.83it/s, training_loss=0.154]\u001b[A\n","Epoch 3:  51%|█████     | 75/147 [00:15<00:14,  4.82it/s, training_loss=0.154]\u001b[A\n","Epoch 3:  51%|█████     | 75/147 [00:15<00:14,  4.82it/s, training_loss=0.261]\u001b[A\n","Epoch 3:  52%|█████▏    | 76/147 [00:15<00:14,  4.82it/s, training_loss=0.261]\u001b[A\n","Epoch 3:  52%|█████▏    | 76/147 [00:15<00:14,  4.82it/s, training_loss=0.301]\u001b[A\n","Epoch 3:  52%|█████▏    | 77/147 [00:15<00:14,  4.82it/s, training_loss=0.301]\u001b[A\n","Epoch 3:  52%|█████▏    | 77/147 [00:16<00:14,  4.82it/s, training_loss=0.215]\u001b[A\n","Epoch 3:  53%|█████▎    | 78/147 [00:16<00:14,  4.83it/s, training_loss=0.215]\u001b[A\n","Epoch 3:  53%|█████▎    | 78/147 [00:16<00:14,  4.83it/s, training_loss=0.279]\u001b[A\n","Epoch 3:  54%|█████▎    | 79/147 [00:16<00:14,  4.83it/s, training_loss=0.279]\u001b[A\n","Epoch 3:  54%|█████▎    | 79/147 [00:16<00:14,  4.83it/s, training_loss=0.258]\u001b[A\n","Epoch 3:  54%|█████▍    | 80/147 [00:16<00:13,  4.80it/s, training_loss=0.258]\u001b[A\n","Epoch 3:  54%|█████▍    | 80/147 [00:16<00:13,  4.80it/s, training_loss=0.151]\u001b[A\n","Epoch 3:  55%|█████▌    | 81/147 [00:16<00:13,  4.78it/s, training_loss=0.151]\u001b[A\n","Epoch 3:  55%|█████▌    | 81/147 [00:16<00:13,  4.78it/s, training_loss=0.257]\u001b[A\n","Epoch 3:  56%|█████▌    | 82/147 [00:17<00:13,  4.81it/s, training_loss=0.257]\u001b[A\n","Epoch 3:  56%|█████▌    | 82/147 [00:17<00:13,  4.81it/s, training_loss=0.203]\u001b[A\n","Epoch 3:  56%|█████▋    | 83/147 [00:17<00:13,  4.82it/s, training_loss=0.203]\u001b[A\n","Epoch 3:  56%|█████▋    | 83/147 [00:17<00:13,  4.82it/s, training_loss=0.363]\u001b[A\n","Epoch 3:  57%|█████▋    | 84/147 [00:17<00:13,  4.79it/s, training_loss=0.363]\u001b[A\n","Epoch 3:  57%|█████▋    | 84/147 [00:17<00:13,  4.79it/s, training_loss=0.231]\u001b[A\n","Epoch 3:  58%|█████▊    | 85/147 [00:17<00:12,  4.82it/s, training_loss=0.231]\u001b[A\n","Epoch 3:  58%|█████▊    | 85/147 [00:17<00:12,  4.82it/s, training_loss=0.288]\u001b[A\n","Epoch 3:  59%|█████▊    | 86/147 [00:17<00:12,  4.81it/s, training_loss=0.288]\u001b[A"]}]}]}