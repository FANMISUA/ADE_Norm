{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12800,
     "status": "ok",
     "timestamp": 1722754418563,
     "user": {
      "displayName": "F D",
      "userId": "09431385063825136827"
     },
     "user_tz": 300
    },
    "id": "u4Ymm56BAtwR",
    "outputId": "0f42ffa7-f478-4264-9807-5b1bf5b38aaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "(5962, 4)\n",
      "(3348, 4)\n",
      "SOC count in CADEC:  soc_code\n",
      "10028395    962\n",
      "10018065    654\n",
      "10037175    401\n",
      "10017947    300\n",
      "10029205    286\n",
      "10040785    184\n",
      "10007541     92\n",
      "10038738     91\n",
      "10022891     82\n",
      "10015919     67\n",
      "10038604     59\n",
      "10038359     50\n",
      "10022117     35\n",
      "10047065     25\n",
      "10013993     16\n",
      "10019805     15\n",
      "10041244      7\n",
      "10027433      6\n",
      "10021881      5\n",
      "10021428      4\n",
      "10014698      3\n",
      "10005329      3\n",
      "10029104      1\n",
      "Name: count, dtype: int64\n",
      "CADEC top3 in SMM4H:                             ade  soc_code\n",
      "926            voracious hunger  10018065\n",
      "927            loss of appetite  10018065\n",
      "929            lack of appetite  10018065\n",
      "931                    anorexia  10018065\n",
      "932                    anorexic  10018065\n",
      "...                         ...       ...\n",
      "5326  short term memory lacking  10037175\n",
      "5328      couldn't eat or drink  10037175\n",
      "5329              Could not eat  10037175\n",
      "5331           can't eat normal  10037175\n",
      "5332   Disturbed sleep patterns  10037175\n",
      "\n",
      "[1341 rows x 2 columns]\n",
      "                            ade  soc_code  label\n",
      "926            voracious hunger  10018065      1\n",
      "927            loss of appetite  10018065      1\n",
      "929            lack of appetite  10018065      1\n",
      "931                    anorexia  10018065      1\n",
      "932                    anorexic  10018065      1\n",
      "...                         ...       ...    ...\n",
      "5326  short term memory lacking  10037175      0\n",
      "5328      couldn't eat or drink  10037175      0\n",
      "5329              Could not eat  10037175      0\n",
      "5331           can't eat normal  10037175      0\n",
      "5332   Disturbed sleep patterns  10037175      0\n",
      "\n",
      "[1341 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(csv_url,names=column_names, header=None)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(cadec_all.shape)\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "print(\"SOC count in CADEC: \",soc_code_counts)\n",
    "\n",
    "# #get top 3 of the SMM4H list\n",
    "# #['10018065','10037175','10029205','10022891','10028395','10017947']\n",
    "# top3SMM4H = [10018065,10037175,10029205]\n",
    "# top3label_dict = {\n",
    "#     'Label': [0, 1, 2],\n",
    "#     'soc_code': [10018065, 10037175, 10029205]\n",
    "# }\n",
    "# top3label_dict = {\n",
    "#     10018065: 0,\n",
    "#     10037175: 1,\n",
    "#     10029205: 2\n",
    "# }\n",
    "\n",
    "#get top 3 of the SMM4H list\n",
    "#['10037175','10018065','10029205','10017947''10028395','10022891']\n",
    "# top6SMM4H = [10018065,10037175,10029205,10022891,10028395,10017947]\n",
    "top3SMM4H = [10037175, 10018065,10029205]\n",
    "# top3SMM4H = ['10018065', '10037175', '10029205']\n",
    "\n",
    "top3label_dict = {\n",
    "    'Label': [0, 1, 2],\n",
    "    'soc_code': [10037175,10018065,10029205]\n",
    "}\n",
    "top3label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2\n",
    "}\n",
    "\n",
    "\n",
    "# top6SMM4H = [10018065,10037175,10029205,10022891,10028395,10017947]\n",
    "\n",
    "# Filter DataFrame\n",
    "filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(top3SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECtop3inSMM4H = filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "print(\"CADEC top3 in SMM4H:\",CADECtop3inSMM4H)\n",
    "data = CADECtop3inSMM4H\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(CADECtop3inSMM4H)\n",
    "\n",
    "# Get unique labels and their counts\n",
    "# label_counts = df['soc_code'].value_counts()\n",
    "\n",
    "# print(label_counts)\n",
    "# Sort labels by counts\n",
    "# sorted_labels = label_counts.index.tolist()\n",
    "\n",
    "# Create label dictionary with labels assigned from 1 to the number of unique labels\n",
    "# label_dict = {label: i for i, label in enumerate(sorted_labels, 0)}\n",
    "\n",
    "# Print label dictionary\n",
    "# print(\"label_dict\",label_dict)\n",
    "\n",
    "# label_df = pd.DataFrame.from_dict(label_dict, orient='index', columns=['Label'])\n",
    "\n",
    "# Reset index to make the index a regular column\n",
    "# label_df.reset_index(inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "# label_df.columns = ['soc_code', 'Label']\n",
    "\n",
    "# Sort DataFrame by label\n",
    "# label_df = label_df.sort_values(by='Label')\n",
    "\n",
    "# Print the DataFrame\n",
    "# print(\"label_df\",label_df)\n",
    "\n",
    "#mapping dictionary for soc to label\n",
    "# soc_code_to_label = dict(zip(top3label_dict['soc_code'], top3label_dict['Label']))\n",
    "df['label'] = df['soc_code'].map(top3label_dict)\n",
    "\n",
    "# Replace `soc_code` values with corresponding labels\n",
    "# df['label'] = df['soc_code'].replace(soc_code_to_label)\n",
    "# print(top3label_dict)\n",
    "print(df)\n",
    "\n",
    "\n",
    "def custom_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    min_class_count = min(counts)\n",
    "\n",
    "    # Find classes with only one or two instances\n",
    "    single_or_double_instance_classes = classes[np.logical_or(counts == 1, counts == 2)]\n",
    "\n",
    "    # Remove instances of single-instance or two-instance classes\n",
    "    X_filtered = X[~np.isin(y, single_or_double_instance_classes)]\n",
    "    y_filtered = y[~np.isin(y, single_or_double_instance_classes)]\n",
    "\n",
    "    if len(y_filtered) < 2:\n",
    "        raise ValueError(\"No classes have more than two instances after filtering.\")\n",
    "\n",
    "    # Perform stratified split on the filtered dataset\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_filtered, y_filtered, test_size=test_size, random_state=random_state, stratify=y_filtered)\n",
    "\n",
    "    # Randomly assign instances of single-instance classes to training or testing sets\n",
    "    for class_label in single_or_double_instance_classes:\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        np.random.shuffle(class_indices)\n",
    "\n",
    "        if len(class_indices) <= 2:\n",
    "            # Only one instance, randomly assign to training or testing set\n",
    "            if np.random.rand() < test_size:\n",
    "                X_val = np.concatenate((X_val, X[class_indices]))\n",
    "                y_val = np.concatenate((y_val, y[class_indices]))\n",
    "            else:\n",
    "                X_train = np.concatenate((X_train, X[class_indices]))\n",
    "                y_train = np.concatenate((y_train, y[class_indices]))\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "    \n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "    \n",
    "    return accuracy_dict, count_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vRUOcg2DXcZ",
    "outputId": "58270a1e-29e5-4d9a-dd5a-86a94807c9e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                           | 0/10 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                 | 0/134 [00:00<?, ?it/s]\u001b[AC:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "Epoch 1:   0%|                                                            | 0/134 [00:00<?, ?it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:   1%|▍                                                   | 1/134 [00:00<01:35,  1.40it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:   1%|▍                                                   | 1/134 [00:01<01:35,  1.40it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:   1%|▊                                                   | 2/134 [00:01<01:02,  2.11it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:   1%|▊                                                   | 2/134 [00:01<01:02,  2.11it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                  | 3/134 [00:01<00:50,  2.60it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                  | 3/134 [00:01<00:50,  2.60it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:   3%|█▌                                                  | 4/134 [00:01<00:44,  2.90it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:   3%|█▌                                                  | 4/134 [00:01<00:44,  2.90it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:   4%|█▉                                                  | 5/134 [00:01<00:42,  3.04it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:   4%|█▉                                                  | 5/134 [00:02<00:42,  3.04it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:   4%|██▎                                                 | 6/134 [00:02<00:40,  3.18it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:   4%|██▎                                                 | 6/134 [00:02<00:40,  3.18it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:   5%|██▋                                                 | 7/134 [00:02<00:38,  3.30it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:   5%|██▋                                                 | 7/134 [00:02<00:38,  3.30it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:   6%|███                                                 | 8/134 [00:02<00:37,  3.38it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:   6%|███                                                 | 8/134 [00:03<00:37,  3.38it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:   7%|███▍                                                | 9/134 [00:03<00:36,  3.42it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:   7%|███▍                                                | 9/134 [00:03<00:36,  3.42it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   7%|███▊                                               | 10/134 [00:03<00:36,  3.44it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   7%|███▊                                               | 10/134 [00:03<00:36,  3.44it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:   8%|████▏                                              | 11/134 [00:03<00:34,  3.53it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:   8%|████▏                                              | 11/134 [00:03<00:34,  3.53it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:   9%|████▌                                              | 12/134 [00:03<00:34,  3.50it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:   9%|████▌                                              | 12/134 [00:04<00:34,  3.50it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  10%|████▉                                              | 13/134 [00:04<00:34,  3.54it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  10%|████▉                                              | 13/134 [00:04<00:34,  3.54it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  10%|█████▎                                             | 14/134 [00:04<00:34,  3.48it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  10%|█████▎                                             | 14/134 [00:04<00:34,  3.48it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                             | 15/134 [00:04<00:34,  3.41it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                             | 15/134 [00:05<00:34,  3.41it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  12%|██████                                             | 16/134 [00:05<00:34,  3.43it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  12%|██████                                             | 16/134 [00:05<00:34,  3.43it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  13%|██████▍                                            | 17/134 [00:05<00:34,  3.43it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  13%|██████▍                                            | 17/134 [00:05<00:34,  3.43it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  13%|██████▊                                            | 18/134 [00:05<00:33,  3.52it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  13%|██████▊                                            | 18/134 [00:05<00:33,  3.52it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                           | 19/134 [00:05<00:32,  3.58it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                           | 19/134 [00:06<00:32,  3.58it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  15%|███████▌                                           | 20/134 [00:06<00:31,  3.57it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  15%|███████▌                                           | 20/134 [00:06<00:31,  3.57it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  16%|███████▉                                           | 21/134 [00:06<00:32,  3.52it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  16%|███████▉                                           | 21/134 [00:06<00:32,  3.52it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  16%|████████▎                                          | 22/134 [00:06<00:30,  3.68it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  16%|████████▎                                          | 22/134 [00:06<00:30,  3.68it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 1:  17%|████████▊                                          | 23/134 [00:06<00:31,  3.53it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 1:  17%|████████▊                                          | 23/134 [00:07<00:31,  3.53it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  18%|█████████▏                                         | 24/134 [00:07<00:31,  3.51it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  18%|█████████▏                                         | 24/134 [00:07<00:31,  3.51it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  19%|█████████▌                                         | 25/134 [00:07<00:30,  3.55it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  19%|█████████▌                                         | 25/134 [00:07<00:30,  3.55it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  19%|█████████▉                                         | 26/134 [00:07<00:31,  3.47it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  19%|█████████▉                                         | 26/134 [00:08<00:31,  3.47it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  20%|██████████▎                                        | 27/134 [00:08<00:30,  3.50it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  20%|██████████▎                                        | 27/134 [00:08<00:30,  3.50it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  21%|██████████▋                                        | 28/134 [00:08<00:30,  3.44it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  21%|██████████▋                                        | 28/134 [00:08<00:30,  3.44it/s, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  22%|███████████                                        | 29/134 [00:08<00:30,  3.50it/s, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  22%|███████████                                        | 29/134 [00:08<00:30,  3.50it/s, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                       | 30/134 [00:08<00:29,  3.56it/s, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                       | 30/134 [00:09<00:29,  3.56it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  23%|███████████▊                                       | 31/134 [00:09<00:29,  3.55it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  23%|███████████▊                                       | 31/134 [00:09<00:29,  3.55it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  24%|████████████▏                                      | 32/134 [00:09<00:29,  3.51it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  24%|████████████▏                                      | 32/134 [00:09<00:29,  3.51it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 1:  25%|████████████▌                                      | 33/134 [00:09<00:28,  3.58it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 1:  25%|████████████▌                                      | 33/134 [00:10<00:28,  3.58it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  25%|████████████▉                                      | 34/134 [00:10<00:28,  3.55it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  25%|████████████▉                                      | 34/134 [00:10<00:28,  3.55it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  26%|█████████████▎                                     | 35/134 [00:10<00:28,  3.50it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  26%|█████████████▎                                     | 35/134 [00:10<00:28,  3.50it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  27%|█████████████▋                                     | 36/134 [00:10<00:28,  3.47it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  27%|█████████████▋                                     | 36/134 [00:10<00:28,  3.47it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  28%|██████████████                                     | 37/134 [00:10<00:27,  3.50it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  28%|██████████████                                     | 37/134 [00:11<00:27,  3.50it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  28%|██████████████▍                                    | 38/134 [00:11<00:27,  3.48it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  28%|██████████████▍                                    | 38/134 [00:11<00:27,  3.48it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  29%|██████████████▊                                    | 39/134 [00:11<00:26,  3.52it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  29%|██████████████▊                                    | 39/134 [00:11<00:26,  3.52it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  30%|███████████████▏                                   | 40/134 [00:11<00:26,  3.59it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  30%|███████████████▏                                   | 40/134 [00:12<00:26,  3.59it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  31%|███████████████▌                                   | 41/134 [00:12<00:25,  3.61it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  31%|███████████████▌                                   | 41/134 [00:12<00:25,  3.61it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  31%|███████████████▉                                   | 42/134 [00:12<00:25,  3.65it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  31%|███████████████▉                                   | 42/134 [00:12<00:25,  3.65it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 1:  32%|████████████████▎                                  | 43/134 [00:12<00:25,  3.55it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 1:  32%|████████████████▎                                  | 43/134 [00:12<00:25,  3.55it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  33%|████████████████▋                                  | 44/134 [00:12<00:24,  3.66it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  33%|████████████████▋                                  | 44/134 [00:13<00:24,  3.66it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▏                                 | 45/134 [00:13<00:24,  3.64it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▏                                 | 45/134 [00:13<00:24,  3.64it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▌                                 | 46/134 [00:13<00:24,  3.64it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▌                                 | 46/134 [00:13<00:24,  3.64it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▉                                 | 47/134 [00:13<00:23,  3.69it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▉                                 | 47/134 [00:13<00:23,  3.69it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▎                                | 48/134 [00:13<00:23,  3.67it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▎                                | 48/134 [00:14<00:23,  3.67it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 1:  37%|██████████████████▋                                | 49/134 [00:14<00:22,  3.70it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 1:  37%|██████████████████▋                                | 49/134 [00:14<00:22,  3.70it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  37%|███████████████████                                | 50/134 [00:14<00:22,  3.68it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  37%|███████████████████                                | 50/134 [00:14<00:22,  3.68it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▍                               | 51/134 [00:14<00:23,  3.57it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▍                               | 51/134 [00:15<00:23,  3.57it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  39%|███████████████████▊                               | 52/134 [00:15<00:23,  3.55it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  39%|███████████████████▊                               | 52/134 [00:15<00:23,  3.55it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▏                              | 53/134 [00:15<00:22,  3.55it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▏                              | 53/134 [00:15<00:22,  3.55it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▌                              | 54/134 [00:15<00:22,  3.60it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▌                              | 54/134 [00:15<00:22,  3.60it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  41%|████████████████████▉                              | 55/134 [00:15<00:22,  3.54it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  41%|████████████████████▉                              | 55/134 [00:16<00:22,  3.54it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▎                             | 56/134 [00:16<00:21,  3.57it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▎                             | 56/134 [00:16<00:21,  3.57it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  43%|█████████████████████▋                             | 57/134 [00:16<00:21,  3.55it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  43%|█████████████████████▋                             | 57/134 [00:16<00:21,  3.55it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████                             | 58/134 [00:16<00:21,  3.46it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████                             | 58/134 [00:17<00:21,  3.46it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▍                            | 59/134 [00:17<00:20,  3.57it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▍                            | 59/134 [00:17<00:20,  3.57it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  45%|██████████████████████▊                            | 60/134 [00:17<00:21,  3.46it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  45%|██████████████████████▊                            | 60/134 [00:17<00:21,  3.46it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▏                           | 61/134 [00:17<00:21,  3.43it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▏                           | 61/134 [00:17<00:21,  3.43it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▌                           | 62/134 [00:17<00:20,  3.55it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▌                           | 62/134 [00:18<00:20,  3.55it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  47%|███████████████████████▉                           | 63/134 [00:18<00:20,  3.53it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  47%|███████████████████████▉                           | 63/134 [00:18<00:20,  3.53it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████▎                          | 64/134 [00:18<00:19,  3.54it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████▎                          | 64/134 [00:18<00:19,  3.54it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████▋                          | 65/134 [00:18<00:19,  3.54it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████▋                          | 65/134 [00:19<00:19,  3.54it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████                          | 66/134 [00:19<00:18,  3.59it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████                          | 66/134 [00:19<00:18,  3.59it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████▌                         | 67/134 [00:19<00:18,  3.71it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████▌                         | 67/134 [00:19<00:18,  3.71it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████▉                         | 68/134 [00:19<00:18,  3.54it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████▉                         | 68/134 [00:19<00:18,  3.54it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▎                        | 69/134 [00:19<00:18,  3.46it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▎                        | 69/134 [00:20<00:18,  3.46it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████▋                        | 70/134 [00:20<00:18,  3.49it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████▋                        | 70/134 [00:20<00:18,  3.49it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 1:  53%|███████████████████████████                        | 71/134 [00:20<00:18,  3.38it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 1:  53%|███████████████████████████                        | 71/134 [00:20<00:18,  3.38it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▍                       | 72/134 [00:20<00:18,  3.37it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▍                       | 72/134 [00:21<00:18,  3.37it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▊                       | 73/134 [00:21<00:17,  3.46it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▊                       | 73/134 [00:21<00:17,  3.46it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████▏                      | 74/134 [00:21<00:17,  3.37it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████▏                      | 74/134 [00:21<00:17,  3.37it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████▌                      | 75/134 [00:21<00:17,  3.34it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████▌                      | 75/134 [00:22<00:17,  3.34it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████▉                      | 76/134 [00:22<00:17,  3.39it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████▉                      | 76/134 [00:22<00:17,  3.39it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▎                     | 77/134 [00:22<00:16,  3.44it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▎                     | 77/134 [00:22<00:16,  3.44it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▋                     | 78/134 [00:22<00:16,  3.41it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▋                     | 78/134 [00:22<00:16,  3.41it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████                     | 79/134 [00:22<00:16,  3.36it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████                     | 79/134 [00:23<00:16,  3.36it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████▍                    | 80/134 [00:23<00:15,  3.45it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████▍                    | 80/134 [00:23<00:15,  3.45it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████▊                    | 81/134 [00:23<00:15,  3.41it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████▊                    | 81/134 [00:23<00:15,  3.41it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████▏                   | 82/134 [00:23<00:15,  3.39it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████▏                   | 82/134 [00:24<00:15,  3.39it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████▌                   | 83/134 [00:24<00:15,  3.36it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████▌                   | 83/134 [00:24<00:15,  3.36it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  63%|███████████████████████████████▉                   | 84/134 [00:24<00:14,  3.46it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  63%|███████████████████████████████▉                   | 84/134 [00:24<00:14,  3.46it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  63%|████████████████████████████████▎                  | 85/134 [00:24<00:14,  3.46it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  63%|████████████████████████████████▎                  | 85/134 [00:24<00:14,  3.46it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████▋                  | 86/134 [00:24<00:13,  3.50it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████▋                  | 86/134 [00:25<00:13,  3.50it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████                  | 87/134 [00:25<00:13,  3.51it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████                  | 87/134 [00:25<00:13,  3.51it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████▍                 | 88/134 [00:25<00:13,  3.46it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████▍                 | 88/134 [00:25<00:13,  3.46it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████▊                 | 89/134 [00:25<00:12,  3.50it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████▊                 | 89/134 [00:26<00:12,  3.50it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  67%|██████████████████████████████████▎                | 90/134 [00:26<00:12,  3.47it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  67%|██████████████████████████████████▎                | 90/134 [00:26<00:12,  3.47it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████████████████▋                | 91/134 [00:26<00:12,  3.47it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████████████████▋                | 91/134 [00:26<00:12,  3.47it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████                | 92/134 [00:26<00:12,  3.46it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████                | 92/134 [00:26<00:12,  3.46it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████▍               | 93/134 [00:26<00:11,  3.47it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████▍               | 93/134 [00:27<00:11,  3.47it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████████████████▊               | 94/134 [00:27<00:11,  3.45it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████████████████▊               | 94/134 [00:27<00:11,  3.45it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  71%|████████████████████████████████████▏              | 95/134 [00:27<00:11,  3.51it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  71%|████████████████████████████████████▏              | 95/134 [00:27<00:11,  3.51it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████▌              | 96/134 [00:27<00:10,  3.54it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████▌              | 96/134 [00:28<00:10,  3.54it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████▉              | 97/134 [00:28<00:10,  3.63it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████▉              | 97/134 [00:28<00:10,  3.63it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▎             | 98/134 [00:28<00:10,  3.57it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▎             | 98/134 [00:28<00:10,  3.57it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████▋             | 99/134 [00:28<00:10,  3.47it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████▋             | 99/134 [00:28<00:10,  3.47it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████▎            | 100/134 [00:28<00:09,  3.48it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████▎            | 100/134 [00:29<00:09,  3.48it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████▋            | 101/134 [00:29<00:09,  3.44it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████▋            | 101/134 [00:29<00:09,  3.44it/s, training_loss=0.282]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████            | 102/134 [00:29<00:09,  3.47it/s, training_loss=0.282]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████            | 102/134 [00:29<00:09,  3.47it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████▍           | 103/134 [00:29<00:08,  3.50it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████▍           | 103/134 [00:30<00:08,  3.50it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  78%|██████████████████████████████████████▊           | 104/134 [00:30<00:08,  3.45it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  78%|██████████████████████████████████████▊           | 104/134 [00:30<00:08,  3.45it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  78%|███████████████████████████████████████▏          | 105/134 [00:30<00:08,  3.50it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  78%|███████████████████████████████████████▏          | 105/134 [00:30<00:08,  3.50it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  79%|███████████████████████████████████████▌          | 106/134 [00:30<00:08,  3.46it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  79%|███████████████████████████████████████▌          | 106/134 [00:30<00:08,  3.46it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  80%|███████████████████████████████████████▉          | 107/134 [00:30<00:07,  3.43it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  80%|███████████████████████████████████████▉          | 107/134 [00:31<00:07,  3.43it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████▎         | 108/134 [00:31<00:07,  3.44it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████▎         | 108/134 [00:31<00:07,  3.44it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████▋         | 109/134 [00:31<00:07,  3.46it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████▋         | 109/134 [00:31<00:07,  3.46it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  82%|█████████████████████████████████████████         | 110/134 [00:31<00:06,  3.57it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  82%|█████████████████████████████████████████         | 110/134 [00:32<00:06,  3.57it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████▍        | 111/134 [00:32<00:06,  3.49it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████▍        | 111/134 [00:32<00:06,  3.49it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  84%|█████████████████████████████████████████▊        | 112/134 [00:32<00:06,  3.50it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  84%|█████████████████████████████████████████▊        | 112/134 [00:32<00:06,  3.50it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  84%|██████████████████████████████████████████▏       | 113/134 [00:32<00:05,  3.53it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  84%|██████████████████████████████████████████▏       | 113/134 [00:32<00:05,  3.53it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████▌       | 114/134 [00:32<00:05,  3.55it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████▌       | 114/134 [00:33<00:05,  3.55it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████▉       | 115/134 [00:33<00:05,  3.52it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████▉       | 115/134 [00:33<00:05,  3.52it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▎      | 116/134 [00:33<00:05,  3.53it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▎      | 116/134 [00:33<00:05,  3.53it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▋      | 117/134 [00:33<00:04,  3.52it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▋      | 117/134 [00:34<00:04,  3.52it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████      | 118/134 [00:34<00:04,  3.49it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████      | 118/134 [00:34<00:04,  3.49it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████▍     | 119/134 [00:34<00:04,  3.49it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████▍     | 119/134 [00:34<00:04,  3.49it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████▊     | 120/134 [00:34<00:03,  3.51it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████▊     | 120/134 [00:34<00:03,  3.51it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████████████████████▏    | 121/134 [00:34<00:03,  3.51it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████████████████████▏    | 121/134 [00:35<00:03,  3.51it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████▌    | 122/134 [00:35<00:03,  3.43it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████▌    | 122/134 [00:35<00:03,  3.43it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  92%|█████████████████████████████████████████████▉    | 123/134 [00:35<00:03,  3.43it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  92%|█████████████████████████████████████████████▉    | 123/134 [00:35<00:03,  3.43it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▎   | 124/134 [00:35<00:02,  3.40it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▎   | 124/134 [00:36<00:02,  3.40it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▋   | 125/134 [00:36<00:02,  3.36it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▋   | 125/134 [00:36<00:02,  3.36it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████   | 126/134 [00:36<00:02,  3.48it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████   | 126/134 [00:36<00:02,  3.48it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████▍  | 127/134 [00:36<00:02,  3.45it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████▍  | 127/134 [00:36<00:02,  3.45it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  96%|███████████████████████████████████████████████▊  | 128/134 [00:36<00:01,  3.46it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  96%|███████████████████████████████████████████████▊  | 128/134 [00:37<00:01,  3.46it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████▏ | 129/134 [00:37<00:01,  3.44it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████▏ | 129/134 [00:37<00:01,  3.44it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▌ | 130/134 [00:37<00:01,  3.45it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▌ | 130/134 [00:37<00:01,  3.45it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  98%|████████████████████████████████████████████████▉ | 131/134 [00:37<00:00,  3.51it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  98%|████████████████████████████████████████████████▉ | 131/134 [00:38<00:00,  3.51it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▎| 132/134 [00:38<00:00,  3.53it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▎| 132/134 [00:38<00:00,  3.53it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▋| 133/134 [00:38<00:00,  3.53it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▋| 133/134 [00:38<00:00,  3.53it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████| 134/134 [00:38<00:00,  3.59it/s, training_loss=0.376]\u001b[A\n",
      "Epoch Progress:  10%|██████▋                                                            | 1/10 [00:41<06:12, 41.35s/it]\u001b[A\n",
      "Epoch 2:   0%|                                                                                 | 0/134 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:   0%|                                                            | 0/134 [00:00<?, ?it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 2:   1%|▍                                                   | 1/134 [00:00<00:37,  3.59it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 2:   1%|▍                                                   | 1/134 [00:00<00:37,  3.59it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 2:   1%|▊                                                   | 2/134 [00:00<00:37,  3.55it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 2:   1%|▊                                                   | 2/134 [00:00<00:37,  3.55it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 2:   2%|█▏                                                  | 3/134 [00:00<00:37,  3.49it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 2:   2%|█▏                                                  | 3/134 [00:01<00:37,  3.49it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 2:   3%|█▌                                                  | 4/134 [00:01<00:37,  3.51it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 2:   3%|█▌                                                  | 4/134 [00:01<00:37,  3.51it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 2:   4%|█▉                                                  | 5/134 [00:01<00:36,  3.51it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 2:   4%|█▉                                                  | 5/134 [00:01<00:36,  3.51it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 2:   4%|██▎                                                 | 6/134 [00:01<00:37,  3.45it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 2:   4%|██▎                                                 | 6/134 [00:01<00:37,  3.45it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 2:   5%|██▋                                                 | 7/134 [00:02<00:36,  3.46it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 2:   5%|██▋                                                 | 7/134 [00:02<00:36,  3.46it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 2:   6%|███                                                 | 8/134 [00:02<00:36,  3.50it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 2:   6%|███                                                 | 8/134 [00:02<00:36,  3.50it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 2:   7%|███▍                                                | 9/134 [00:02<00:35,  3.53it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 2:   7%|███▍                                                | 9/134 [00:02<00:35,  3.53it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 2:   7%|███▊                                               | 10/134 [00:02<00:35,  3.52it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 2:   7%|███▊                                               | 10/134 [00:03<00:35,  3.52it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 2:   8%|████▏                                              | 11/134 [00:03<00:35,  3.46it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 2:   8%|████▏                                              | 11/134 [00:03<00:35,  3.46it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:   9%|████▌                                              | 12/134 [00:03<00:34,  3.52it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:   9%|████▌                                              | 12/134 [00:03<00:34,  3.52it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 2:  10%|████▉                                              | 13/134 [00:03<00:34,  3.49it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 2:  10%|████▉                                              | 13/134 [00:03<00:34,  3.49it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 2:  10%|█████▎                                             | 14/134 [00:04<00:34,  3.51it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 2:  10%|█████▎                                             | 14/134 [00:04<00:34,  3.51it/s, training_loss=0.259]\u001b[A\n",
      "Epoch 2:  11%|█████▋                                             | 15/134 [00:04<00:33,  3.56it/s, training_loss=0.259]\u001b[A\n",
      "Epoch 2:  11%|█████▋                                             | 15/134 [00:04<00:33,  3.56it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 2:  12%|██████                                             | 16/134 [00:04<00:33,  3.56it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 2:  12%|██████                                             | 16/134 [00:04<00:33,  3.56it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  13%|██████▍                                            | 17/134 [00:04<00:32,  3.55it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  13%|██████▍                                            | 17/134 [00:05<00:32,  3.55it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 2:  13%|██████▊                                            | 18/134 [00:05<00:32,  3.53it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 2:  13%|██████▊                                            | 18/134 [00:05<00:32,  3.53it/s, training_loss=0.252]\u001b[A\n",
      "Epoch 2:  14%|███████▏                                           | 19/134 [00:05<00:33,  3.48it/s, training_loss=0.252]\u001b[A\n",
      "Epoch 2:  14%|███████▏                                           | 19/134 [00:05<00:33,  3.48it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 2:  15%|███████▌                                           | 20/134 [00:05<00:32,  3.55it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 2:  15%|███████▌                                           | 20/134 [00:05<00:32,  3.55it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 2:  16%|███████▉                                           | 21/134 [00:05<00:31,  3.54it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 2:  16%|███████▉                                           | 21/134 [00:06<00:31,  3.54it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 2:  16%|████████▎                                          | 22/134 [00:06<00:32,  3.48it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 2:  16%|████████▎                                          | 22/134 [00:06<00:32,  3.48it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 2:  17%|████████▊                                          | 23/134 [00:06<00:31,  3.53it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 2:  17%|████████▊                                          | 23/134 [00:06<00:31,  3.53it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 2:  18%|█████████▏                                         | 24/134 [00:06<00:30,  3.60it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 2:  18%|█████████▏                                         | 24/134 [00:07<00:30,  3.60it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 2:  19%|█████████▌                                         | 25/134 [00:07<00:30,  3.54it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 2:  19%|█████████▌                                         | 25/134 [00:07<00:30,  3.54it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 2:  19%|█████████▉                                         | 26/134 [00:07<00:30,  3.58it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 2:  19%|█████████▉                                         | 26/134 [00:07<00:30,  3.58it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 2:  20%|██████████▎                                        | 27/134 [00:07<00:30,  3.54it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 2:  20%|██████████▎                                        | 27/134 [00:07<00:30,  3.54it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 2:  21%|██████████▋                                        | 28/134 [00:07<00:29,  3.54it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 2:  21%|██████████▋                                        | 28/134 [00:08<00:29,  3.54it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 2:  22%|███████████                                        | 29/134 [00:08<00:29,  3.54it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 2:  22%|███████████                                        | 29/134 [00:08<00:29,  3.54it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 2:  22%|███████████▍                                       | 30/134 [00:08<00:29,  3.54it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 2:  22%|███████████▍                                       | 30/134 [00:08<00:29,  3.54it/s, training_loss=0.421]\u001b[A\n",
      "Epoch 2:  23%|███████████▊                                       | 31/134 [00:08<00:29,  3.54it/s, training_loss=0.421]\u001b[A\n",
      "Epoch 2:  23%|███████████▊                                       | 31/134 [00:09<00:29,  3.54it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 2:  24%|████████████▏                                      | 32/134 [00:09<00:28,  3.64it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 2:  24%|████████████▏                                      | 32/134 [00:09<00:28,  3.64it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:  25%|████████████▌                                      | 33/134 [00:09<00:28,  3.51it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:  25%|████████████▌                                      | 33/134 [00:09<00:28,  3.51it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  25%|████████████▉                                      | 34/134 [00:09<00:28,  3.57it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  25%|████████████▉                                      | 34/134 [00:09<00:28,  3.57it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 2:  26%|█████████████▎                                     | 35/134 [00:09<00:27,  3.56it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 2:  26%|█████████████▎                                     | 35/134 [00:10<00:27,  3.56it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 2:  27%|█████████████▋                                     | 36/134 [00:10<00:27,  3.61it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 2:  27%|█████████████▋                                     | 36/134 [00:10<00:27,  3.61it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  28%|██████████████                                     | 37/134 [00:10<00:27,  3.47it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  28%|██████████████                                     | 37/134 [00:10<00:27,  3.47it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  28%|██████████████▍                                    | 38/134 [00:10<00:27,  3.49it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  28%|██████████████▍                                    | 38/134 [00:11<00:27,  3.49it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 2:  29%|██████████████▊                                    | 39/134 [00:11<00:27,  3.41it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 2:  29%|██████████████▊                                    | 39/134 [00:11<00:27,  3.41it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 2:  30%|███████████████▏                                   | 40/134 [00:11<00:27,  3.47it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 2:  30%|███████████████▏                                   | 40/134 [00:11<00:27,  3.47it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  31%|███████████████▌                                   | 41/134 [00:11<00:26,  3.54it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 2:  31%|███████████████▌                                   | 41/134 [00:11<00:26,  3.54it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 2:  31%|███████████████▉                                   | 42/134 [00:11<00:25,  3.58it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 2:  31%|███████████████▉                                   | 42/134 [00:12<00:25,  3.58it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 2:  32%|████████████████▎                                  | 43/134 [00:12<00:25,  3.52it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 2:  32%|████████████████▎                                  | 43/134 [00:12<00:25,  3.52it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 2:  33%|████████████████▋                                  | 44/134 [00:12<00:25,  3.56it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 2:  33%|████████████████▋                                  | 44/134 [00:12<00:25,  3.56it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▏                                 | 45/134 [00:12<00:25,  3.52it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▏                                 | 45/134 [00:13<00:25,  3.52it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▌                                 | 46/134 [00:13<00:25,  3.52it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▌                                 | 46/134 [00:13<00:25,  3.52it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▉                                 | 47/134 [00:13<00:25,  3.46it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▉                                 | 47/134 [00:13<00:25,  3.46it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▎                                | 48/134 [00:13<00:25,  3.42it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▎                                | 48/134 [00:13<00:25,  3.42it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 2:  37%|██████████████████▋                                | 49/134 [00:13<00:24,  3.44it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 2:  37%|██████████████████▋                                | 49/134 [00:14<00:24,  3.44it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 2:  37%|███████████████████                                | 50/134 [00:14<00:24,  3.47it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 2:  37%|███████████████████                                | 50/134 [00:14<00:24,  3.47it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▍                               | 51/134 [00:14<00:24,  3.44it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▍                               | 51/134 [00:14<00:24,  3.44it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 2:  39%|███████████████████▊                               | 52/134 [00:14<00:23,  3.51it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 2:  39%|███████████████████▊                               | 52/134 [00:15<00:23,  3.51it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▏                              | 53/134 [00:15<00:23,  3.39it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▏                              | 53/134 [00:15<00:23,  3.39it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▌                              | 54/134 [00:15<00:23,  3.38it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▌                              | 54/134 [00:15<00:23,  3.38it/s, training_loss=0.279]\u001b[A\n",
      "Epoch 2:  41%|████████████████████▉                              | 55/134 [00:15<00:23,  3.39it/s, training_loss=0.279]\u001b[A\n",
      "Epoch 2:  41%|████████████████████▉                              | 55/134 [00:15<00:23,  3.39it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▎                             | 56/134 [00:15<00:22,  3.42it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▎                             | 56/134 [00:16<00:22,  3.42it/s, training_loss=0.433]\u001b[A\n",
      "Epoch 2:  43%|█████████████████████▋                             | 57/134 [00:16<00:22,  3.45it/s, training_loss=0.433]\u001b[A\n",
      "Epoch 2:  43%|█████████████████████▋                             | 57/134 [00:16<00:22,  3.45it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 2:  43%|██████████████████████                             | 58/134 [00:16<00:21,  3.51it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 2:  43%|██████████████████████                             | 58/134 [00:16<00:21,  3.51it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 2:  44%|██████████████████████▍                            | 59/134 [00:16<00:21,  3.52it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 2:  44%|██████████████████████▍                            | 59/134 [00:17<00:21,  3.52it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 2:  45%|██████████████████████▊                            | 60/134 [00:17<00:21,  3.52it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 2:  45%|██████████████████████▊                            | 60/134 [00:17<00:21,  3.52it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▏                           | 61/134 [00:17<00:20,  3.53it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▏                           | 61/134 [00:17<00:20,  3.53it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▌                           | 62/134 [00:17<00:20,  3.54it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▌                           | 62/134 [00:17<00:20,  3.54it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 2:  47%|███████████████████████▉                           | 63/134 [00:17<00:20,  3.54it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 2:  47%|███████████████████████▉                           | 63/134 [00:18<00:20,  3.54it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████▎                          | 64/134 [00:18<00:19,  3.53it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████▎                          | 64/134 [00:18<00:19,  3.53it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 2:  49%|████████████████████████▋                          | 65/134 [00:18<00:19,  3.54it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 2:  49%|████████████████████████▋                          | 65/134 [00:18<00:19,  3.54it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 2:  49%|█████████████████████████                          | 66/134 [00:18<00:19,  3.57it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 2:  49%|█████████████████████████                          | 66/134 [00:19<00:19,  3.57it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  50%|█████████████████████████▌                         | 67/134 [00:19<00:18,  3.56it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  50%|█████████████████████████▌                         | 67/134 [00:19<00:18,  3.56it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 2:  51%|█████████████████████████▉                         | 68/134 [00:19<00:18,  3.54it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 2:  51%|█████████████████████████▉                         | 68/134 [00:19<00:18,  3.54it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  51%|██████████████████████████▎                        | 69/134 [00:19<00:18,  3.53it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  51%|██████████████████████████▎                        | 69/134 [00:19<00:18,  3.53it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 2:  52%|██████████████████████████▋                        | 70/134 [00:19<00:17,  3.60it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 2:  52%|██████████████████████████▋                        | 70/134 [00:20<00:17,  3.60it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 2:  53%|███████████████████████████                        | 71/134 [00:20<00:18,  3.50it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 2:  53%|███████████████████████████                        | 71/134 [00:20<00:18,  3.50it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▍                       | 72/134 [00:20<00:17,  3.51it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▍                       | 72/134 [00:20<00:17,  3.51it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▊                       | 73/134 [00:20<00:16,  3.59it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▊                       | 73/134 [00:21<00:16,  3.59it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  55%|████████████████████████████▏                      | 74/134 [00:21<00:16,  3.59it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  55%|████████████████████████████▏                      | 74/134 [00:21<00:16,  3.59it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 2:  56%|████████████████████████████▌                      | 75/134 [00:21<00:16,  3.60it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 2:  56%|████████████████████████████▌                      | 75/134 [00:21<00:16,  3.60it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 2:  57%|████████████████████████████▉                      | 76/134 [00:21<00:15,  3.66it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 2:  57%|████████████████████████████▉                      | 76/134 [00:21<00:15,  3.66it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  57%|█████████████████████████████▎                     | 77/134 [00:21<00:15,  3.62it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  57%|█████████████████████████████▎                     | 77/134 [00:22<00:15,  3.62it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▋                     | 78/134 [00:22<00:15,  3.64it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▋                     | 78/134 [00:22<00:15,  3.64it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 2:  59%|██████████████████████████████                     | 79/134 [00:22<00:15,  3.61it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 2:  59%|██████████████████████████████                     | 79/134 [00:22<00:15,  3.61it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  60%|██████████████████████████████▍                    | 80/134 [00:22<00:14,  3.65it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  60%|██████████████████████████████▍                    | 80/134 [00:22<00:14,  3.65it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 2:  60%|██████████████████████████████▊                    | 81/134 [00:22<00:14,  3.62it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 2:  60%|██████████████████████████████▊                    | 81/134 [00:23<00:14,  3.62it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 2:  61%|███████████████████████████████▏                   | 82/134 [00:23<00:14,  3.59it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 2:  61%|███████████████████████████████▏                   | 82/134 [00:23<00:14,  3.59it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▌                   | 83/134 [00:23<00:14,  3.63it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▌                   | 83/134 [00:23<00:14,  3.63it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 2:  63%|███████████████████████████████▉                   | 84/134 [00:23<00:13,  3.60it/s, training_loss=0.333]\u001b[A"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score as f1_score_func\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='cadec_top3_finetuning_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 4, 2))\n",
    "# batch_size = 8\n",
    "# epochs = 10\n",
    "# Define parameter grid\n",
    "learning_rates = [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "epochs_list = [ 10, 20, 30, 40]\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "# Placeholder for accuracies\n",
    "all_accuracies = {label: [] for label in range(len(top3label_dict))}\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "# Main loop over seed values\n",
    "for seed_val in seed_values:\n",
    "  # Set seeds\n",
    "  random.seed(seed_val)\n",
    "  np.random.seed(seed_val)\n",
    "  torch.manual_seed(seed_val)\n",
    "  torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "  # Data preparation\n",
    "  X_train, X_val, y_train, y_val = custom_train_test_split(df.index.values, df.label.values, test_size=0.2, random_state=seed_val)\n",
    "  df['data_type'] = ['not_set'] * df.shape[0]\n",
    "  df.loc[X_train, 'data_type'] = 'train'\n",
    "  df.loc[X_val, 'data_type'] = 'val'\n",
    "  # logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "\n",
    "  # Training loop for grid search\n",
    "  for lr in learning_rates:\n",
    "      for batch_size in batch_sizes:\n",
    "          for epochs in epochs_list:\n",
    "            logger.info(f\"Seed: {seed_val}, Learning Rate: {lr}, Batch Size: {batch_size}, Epochs: {epochs}\")\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "            encoded_data_train = tokenizer.batch_encode_plus(\n",
    "                df[df.data_type == 'train'].ade.values,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=True,\n",
    "                pad_to_max_length=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            encoded_data_val = tokenizer.batch_encode_plus(\n",
    "                df[df.data_type == 'val'].ade.values,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=True,\n",
    "                pad_to_max_length=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids_train = encoded_data_train['input_ids']\n",
    "            attention_masks_train = encoded_data_train['attention_mask']\n",
    "            labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "            input_ids_val = encoded_data_val['input_ids']\n",
    "            attention_masks_val = encoded_data_val['attention_mask']\n",
    "            labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n",
    "\n",
    "            dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "            dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "            model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top3label_dict), output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "            dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "            dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "            optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            logger.info(f\"Device used: {device}\")\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "                model.train()\n",
    "                loss_train_total = 0\n",
    "\n",
    "                progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "                for batch in progress_bar:\n",
    "                    model.zero_grad()\n",
    "                    batch = tuple(b.to(device) for b in batch)\n",
    "                    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "                    outputs = model(**inputs)\n",
    "                    loss = outputs[0]\n",
    "                    loss_train_total += loss.item()\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "\n",
    "                # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n",
    "\n",
    "                # logger.info(f'\\nEpoch {epoch}')\n",
    "                loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "                # logger.info(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "                val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "                val_f1 = f1_score_func(true_vals, np.argmax(predictions, axis=1), average='weighted')\n",
    "                # logger.info(f'Validation loss: {val_loss}')\n",
    "                # logger.info(f'F1 Score (Weighted): {val_f1}')\n",
    "\n",
    "            _, predictions, true_vals = evaluate(dataloader_validation)\n",
    "            accuracy_dict, count_dict = accuracy_per_class(predictions, true_vals)\n",
    "\n",
    "            for label, accuracy in accuracy_dict.items():\n",
    "                all_accuracies[label].append(accuracy)\n",
    "\n",
    "\n",
    "            # Calculate the average accuracy for each label\n",
    "            avg_accuracy = {label: np.mean(accs) for label, accs in all_accuracies.items()}\n",
    "\n",
    "            # Calculate the overall average accuracy across all labels\n",
    "            overall_avg_accuracy = np.mean(list(avg_accuracy.values()))\n",
    "\n",
    "            logger.info(f'Seed {seed_val} - Accuracy: {overall_avg_accuracy} - Count: {count_dict} - lr: {lr} -batchsize:{batch_size} -epochs:{epochs}')\n",
    "            #store results\n",
    "            results.append((lr, batch_size, epochs, overall_avg_accuracy))\n",
    "\n",
    "\n",
    "# Extract each parameter and accuracy for plotting\n",
    "learning_rates = [result[0] for result in results]\n",
    "batch_sizes = [result[1] for result in results]\n",
    "epochs = [result[2] for result in results]\n",
    "accuracies = [result[3] for result in results]\n",
    "\n",
    "# Find the best result based on accuracy\n",
    "best_result = max(results, key=lambda x: x[3])\n",
    "print(f\"Best result: LR={best_result[0]}, Batch={best_result[1]}, Epoch={best_result[2]}, Accuracy={best_result[3]:.4f}\")\n",
    "\n",
    "with open('cadec_finetuning_results.txt', 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(f'Learning Rate: {result[0]}, Batch Size: {result[1]}, Epochs: {result[2]}, Accuracy: {result[3]}\\n')\n",
    "     # Write the best result at the end\n",
    "    file.write('\\nBest result:\\n')\n",
    "    file.write(f'Learning Rate: {best_result[0]}, Batch Size: {best_result[1]}, Epochs: {best_result[2]}, Accuracy: {best_result[3]:.4f}\\n')\n",
    "    \n",
    "# Convert data to a DataFrame for easier manipulation\n",
    "data = pd.DataFrame({\n",
    "    'Learning Rate': learning_rates,\n",
    "    'Batch Size': batch_sizes,\n",
    "    'Epochs': epochs,\n",
    "    'Accuracy': accuracies\n",
    "})\n",
    "\n",
    "# Convert 'Learning Rate', 'Batch Size', and 'Epochs' to categorical types\n",
    "data['Learning Rate'] = pd.Categorical(data['Learning Rate'])\n",
    "data['Batch Size'] = pd.Categorical(data['Batch Size'])\n",
    "data['Epochs'] = pd.Categorical(data['Epochs'])\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Map categorical data to numeric values for plotting\n",
    "learning_rate_codes = data['Learning Rate'].cat.codes\n",
    "batch_size_codes = data['Batch Size'].cat.codes\n",
    "epoch_codes = data['Epochs'].cat.codes\n",
    "\n",
    "# Plot the data points\n",
    "sc = ax.scatter(learning_rate_codes, batch_size_codes, epoch_codes, c=accuracies, cmap='viridis', s=100, edgecolors='k')\n",
    "\n",
    "# Add color bar\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Overall Average Accuracy')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('Batch Size')\n",
    "ax.set_zlabel('Epochs')\n",
    "\n",
    "# Set tick labels to categories\n",
    "ax.set_xticks(np.arange(len(data['Learning Rate'].cat.categories)))\n",
    "ax.set_xticklabels(data['Learning Rate'].cat.categories)\n",
    "ax.set_yticks(np.arange(len(data['Batch Size'].cat.categories)))\n",
    "ax.set_yticklabels(data['Batch Size'].cat.categories)\n",
    "ax.set_zticks(np.arange(len(data['Epochs'].cat.categories)))\n",
    "ax.set_zticklabels(data['Epochs'].cat.categories)\n",
    "\n",
    "# Title\n",
    "ax.set_title('Hyperparameter Tuning Results for cadec top3')\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig(\"cadec_top3_hyperparameter_tuning_3d_plot_cadec_top3.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOMq7buPEWSRlRCyRvoA9j+",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
