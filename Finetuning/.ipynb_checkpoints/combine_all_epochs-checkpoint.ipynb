{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf957e-1944-4e7a-8526-9ffaa9268863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def custom_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    if test_size == 0:\n",
    "        return X, [], y, []\n",
    "    # Find classes with only one or two instances\n",
    "    small_classes = classes[counts < 5]\n",
    "\n",
    "    # Separate out the instances of small classes\n",
    "    large_class_mask = ~np.isin(y, small_classes)\n",
    "    X_large = X[large_class_mask]\n",
    "    y_large = y[large_class_mask]\n",
    "    X_small = X[~large_class_mask]\n",
    "    y_small = y[~large_class_mask]\n",
    "\n",
    "    # Perform stratified split on the larger classes dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_large, y_large, test_size=test_size, random_state=random_state, stratify=y_large\n",
    "    )\n",
    "\n",
    "    # Randomly assign instances of small classes to training or testing sets\n",
    "    for i in range(len(X_small)):\n",
    "        if np.random.rand() < test_size:\n",
    "            X_test = np.vstack([X_test, X_small[i]])\n",
    "            y_test = np.hstack([y_test, y_small[i]])\n",
    "        else:\n",
    "            X_train = np.vstack([X_train, X_small[i]])\n",
    "            y_train = np.hstack([y_train, y_small[i]])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#evaluation\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "\n",
    "    return accuracy_dict, count_dict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='combine_all_finetuning_training_epochs_100.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "cadec_csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "# read data from smm4h\n",
    "smm4h_csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n",
    "\n",
    "# top6SMM4H = [10037175, 10018065,10029205, 10017947, 10028395, 10022891]\n",
    "# top6label_dict = {\n",
    "#     10037175: 0,\n",
    "#     10018065: 1,\n",
    "#     10029205: 2,\n",
    "#     10017947: 3,\n",
    "#     10028395: 4,\n",
    "#     10022891: 5\n",
    "# }\n",
    "\n",
    "allSMM4H = [10037175, 10018065,10029205, 10017947, 10028395, 10022891, 10027433, 10040785, 10038738, 10022117, 10015919, 10038604, 10047065, \n",
    "            10021428,10041244, 10007541, 10038359, 10021881, 10013993, 10019805, 10042613, 10029104, 10077536, 10010331, 10014698]\n",
    "\n",
    "label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2,\n",
    "    10017947: 3,\n",
    "    10028395: 4,\n",
    "    10022891: 5,\n",
    "    10027433: 6,\n",
    "    10040785: 7,\n",
    "    10038738: 8,\n",
    "    10022117: 9,\n",
    "    10015919: 10,\n",
    "    10038604: 11,\n",
    "    10047065: 12,\n",
    "    10021428: 13,\n",
    "    10041244: 14,\n",
    "    10007541: 15,\n",
    "    10038359: 16,\n",
    "    10021881: 17,\n",
    "    10013993: 18,\n",
    "    10019805: 19,\n",
    "    10042613: 20,\n",
    "    10029104: 21,\n",
    "    10077536: 22,\n",
    "    10010331: 23,\n",
    "    10014698: 24\n",
    "}\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"ade\", \"soc_code\"]\n",
    "smm4h_all = pd.read_csv(smm4h_csv_url,names=column_names, sep = '\\t', header=None)\n",
    "print(\"smm4h data:\",smm4h_all.shape)\n",
    "\n",
    "smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n",
    "smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n",
    "\n",
    "smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n",
    "\n",
    "print(\"smm4h data after filtering:\",smm4h_all.shape)\n",
    "smm4h_soc_code_counts = smm4h_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "print(\"SOC count in SMM4H: \",smm4h_soc_code_counts)\n",
    "# Filter DataFrame\n",
    "smm4h_filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(allSMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "allinSMM4H = smm4h_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(cadec_csv_url,names=column_names, header=None)\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(\"clean cadec data:\",cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "cadec_soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "print(\"SOC count in CADEC: \",cadec_soc_code_counts)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "cadec_filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(allSMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECallinSMM4H = cadec_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "\n",
    "# For SMM4H data\n",
    "df1 = allinSMM4H.copy()\n",
    "df1.loc[:, 'label'] = df1['soc_code'].map(label_dict)\n",
    "\n",
    "# For CADEC data\n",
    "df2 = CADECallinSMM4H.copy()\n",
    "df2.loc[:, 'label'] = df2['soc_code'].map(label_dict)\n",
    "\n",
    "print(\"SMM4H :\",df1)\n",
    "print(\"CADEC :\",df2)\n",
    "\n",
    "#combine all data\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 4, 2))\n",
    "\n",
    "# fix batch_size 16, learning rate 1e-5; Finetuning epochs\n",
    "learning_rates = [1e-5]\n",
    "batch_sizes = [16]\n",
    "epochs_list = [100]\n",
    "# Results storage\n",
    "results = []\n",
    "# Store the training losses across all splits\n",
    "all_training_losses = []\n",
    "# Main loop over seed values\n",
    "# Training loop\n",
    "for seed_val in seed_values:\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    \n",
    "    training_losses = []\n",
    "    \n",
    "\n",
    "\n",
    "    # Training loop for grid search\n",
    "    for lr in learning_rates:\n",
    "      for batch_size in batch_sizes:\n",
    "          for epochs in epochs_list:\n",
    "              # Data preparation\n",
    "                tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "                encoded_data_train = tokenizer.batch_encode_plus(\n",
    "                    df.ade.values,\n",
    "                    add_special_tokens=True,\n",
    "                    return_attention_mask=True,\n",
    "                    pad_to_max_length=True,\n",
    "                    max_length=256,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                input_ids_train = encoded_data_train['input_ids']\n",
    "                attention_masks_train = encoded_data_train['attention_mask']\n",
    "                labels_train = torch.tensor(df.label.values)\n",
    "                \n",
    "                dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "                dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "                \n",
    "                model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict), output_attentions=False, output_hidden_states=False)\n",
    "                optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "                scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "                \n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model.to(device)\n",
    "                \n",
    "                # Training loop\n",
    "                for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "                    model.train()\n",
    "                    loss_train_total = 0\n",
    "                \n",
    "                    progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "                    for batch in progress_bar:\n",
    "                        model.zero_grad()\n",
    "                        batch = tuple(b.to(device) for b in batch)\n",
    "                        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "                \n",
    "                        outputs = model(**inputs)\n",
    "                        loss = outputs[0]\n",
    "                        loss_train_total += loss.item()\n",
    "                        loss.backward()\n",
    "                \n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                \n",
    "                        progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "                \n",
    "                    loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "                    training_losses.append(loss_train_avg)\n",
    "                \n",
    "                    logger.info(f'Epoch {epoch}: Training loss = {loss_train_avg:.4f}')\n",
    "                \n",
    "                all_training_losses.append(training_losses)\n",
    "\n",
    "# Calculate the average training loss and standard deviation across all splits\n",
    "average_training_losses = np.mean(all_training_losses, axis=0)\n",
    "std_training_losses = np.std(all_training_losses, axis=0)\n",
    "\n",
    "# Save the training losses to a file\n",
    "with open('combine_all_training_losses.txt', 'w') as f:\n",
    "    for epoch, loss in enumerate(average_training_losses, 1):\n",
    "        f.write(f'Epoch {epoch}: {loss:.4f}\\n')\n",
    "\n",
    "# Plotting training loss\n",
    "epochs_range = range(1, epochs + 1)\n",
    "mean_losses = np.mean(all_training_losses, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs_range, mean_losses, '-o', color='b', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs. Epochs')\n",
    "plt.grid(True)\n",
    "plt.savefig('combine_all_trainingloss_vs_epochs.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
