{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2367b9-1427-4f0f-980a-2c8905c36e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#data split function\n",
    "def custom_train_val_test_split(X, y, test_size=0.1, val_size=0.1, random_state=None):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    # Find classes with only one or two instances\n",
    "    small_classes = classes[counts < 5]\n",
    "\n",
    "    # Separate out the instances of small classes\n",
    "    large_class_mask = ~np.isin(y, small_classes)\n",
    "    X_large = X[large_class_mask]\n",
    "    y_large = y[large_class_mask]\n",
    "    X_small = X[~large_class_mask]\n",
    "    y_small = y[~large_class_mask]\n",
    "\n",
    "    # Perform stratified split on the larger classes dataset\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_large, y_large, test_size=(val_size + test_size),\n",
    "        random_state=random_state, stratify=y_large\n",
    "    )\n",
    "\n",
    "    # Split the remaining data into validation and testing sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=(test_size / (test_size + val_size)),\n",
    "        random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # Randomly assign instances of small classes to training, validation, or testing sets\n",
    "    for i in range(len(X_small)):\n",
    "        rand_choice = np.random.rand()\n",
    "        if rand_choice < test_size:\n",
    "            X_test = np.vstack([X_test, X_small[i]])\n",
    "            y_test = np.hstack([y_test, y_small[i]])\n",
    "        elif rand_choice < (test_size + val_size):\n",
    "            X_val = np.vstack([X_val, X_small[i]])\n",
    "            y_val = np.hstack([y_val, y_small[i]])\n",
    "        else:\n",
    "            X_train = np.vstack([X_train, X_small[i]])\n",
    "            y_train = np.hstack([y_train, y_small[i]])\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "#evaluation\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "\n",
    "    return accuracy_dict, count_dict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='combine_top3_finetuning_batchsize_test.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "cadec_csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "# read data from smm4h\n",
    "smm4h_csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n",
    "\n",
    "top3SMM4H = [10037175, 10018065,10029205]\n",
    "top3label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2\n",
    "}\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"ade\", \"soc_code\"]\n",
    "smm4h_all = pd.read_csv(smm4h_csv_url,names=column_names, sep = '\\t', header=None)\n",
    "\n",
    "smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n",
    "smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n",
    "smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# print(\"smm4h data:\",smm4h_all.shape)\n",
    "smm4h_soc_code_counts = smm4h_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",smm4h_soc_code_counts)\n",
    "# Filter DataFrame\n",
    "smm4h_filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(top3SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "top3inSMM4H = smm4h_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "top3inSMM4H.loc[:, 'label'] = top3inSMM4H['soc_code'].map(top3label_dict)\n",
    "\n",
    "# print(\"top3 in SMM4H:\",top3inSMM4H)\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(cadec_csv_url,names=column_names, header=None)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "# print(\"cadec raw data:\",cadec_all.shape)\n",
    "\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(\"clean cadec data:\",cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "cadec_soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",cadec_soc_code_counts)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "cadec_filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(top3SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECtop3inSMM4H = cadec_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "\n",
    "# print(\"CADEC top3 in SMM4H:\",CADECtop3inSMM4H)\n",
    "\n",
    "df1 = top3inSMM4H\n",
    "df2 = CADECtop3inSMM4H\n",
    "df1.loc[:, 'label'] = df1['soc_code'].map(top3label_dict)\n",
    "df2.loc[:, 'label'] = df2['soc_code'].map(top3label_dict)\n",
    "\n",
    "print(\"SMM4H top 3\",df1)\n",
    "print(\"CADEC top 3\",df2)\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 4, 2))\n",
    "\n",
    "# fix batch_size 16, learning rate 1e-5; Finetuning epochs\n",
    "# learning_rates = [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "# batch_sizes = [8, 16, 32, 64]\n",
    "# epochs_list = [10, 20, 30, 40]\n",
    "learning_rates = [1e-5]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "epochs_list = [2]\n",
    "# Results storage\n",
    "results = []\n",
    "# Store the training losses across all splits\n",
    "all_training_losses = []\n",
    "all_val_losses = []\n",
    "\n",
    "# Store results for plotting\n",
    "macro_precisions = {bs: [] for bs in batch_sizes}\n",
    "macro_recalls = {bs: [] for bs in batch_sizes}\n",
    "macro_f1s = {bs: [] for bs in batch_sizes}\n",
    "\n",
    "# Main loop over seed values\n",
    "for seed_val in seed_values:\n",
    "    # Set seeds\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Placeholder for accuracies\n",
    "    all_accuracies = {label: [] for label in range(len(top3label_dict))}\n",
    "    # Store the training losses for this split\n",
    "    training_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "\n",
    "    # Data preparation\n",
    "    # X_train, X_val, y_train, y_val = custom_train_test_split(df.index.values, df.label.values, test_size=0.2, random_state=seed_val)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = custom_train_val_test_split(df.index.values, df.label.values, test_size=0.1, val_size=0.1, random_state=seed_val)\n",
    "\n",
    "    df['data_type'] = ['not_set'] * df.shape[0]\n",
    "    df.loc[X_train, 'data_type'] = 'train'\n",
    "    df.loc[X_val, 'data_type'] = 'val'\n",
    "    df.loc[X_test, 'data_type'] = 'test'\n",
    "    logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "\n",
    "    #add \n",
    "    X_train_idx1, X_val_idx1, X_test_idx1, y_train1, y_val1, y_test1 = custom_train_val_test_split(df1.index.values, df1.label.values, test_size=0.1, val_size=0.1, random_state=seed_val)\n",
    "    \n",
    "    # Perform train-test split on df2\n",
    "    X_train_idx2, X_val_idx2, X_test_idx2, y_train2, y_val2, y_test2 = custom_train_val_test_split(df2.index.values, df2.label.values, test_size=0.1, val_size=0.1, random_state=seed_val)\n",
    "\n",
    "    \n",
    "    # Combine the training indices and labels from df1 and df2\n",
    "    X_train_combined = np.concatenate((X_train_idx1, X_train_idx2))\n",
    "    y_train_combined = np.concatenate((y_train1, y_train2))\n",
    "    \n",
    "    # Combine the validation indices and labels from df1 and df2\n",
    "    X_val_combined = np.concatenate((X_val_idx1, X_val_idx2))\n",
    "    y_val_combined = np.concatenate((y_val1, y_val2))\n",
    "\n",
    "    # Combine the validation indices and labels from df1 and df2\n",
    "    X_test_combined = np.concatenate((X_test_idx1, X_test_idx2))\n",
    "    y_test_combined = np.concatenate((y_test1, y_test2))\n",
    "    \n",
    "    # Optionally, you can set the 'data_type' column for df1 and df2\n",
    "    df1['data_type'] = 'not_set'\n",
    "    df2['data_type'] = 'not_set'\n",
    "    \n",
    "    df1.loc[X_train_idx1, 'data_type'] = 'train'\n",
    "    df1.loc[X_val_idx1, 'data_type'] = 'val'\n",
    "    df1.loc[X_test_idx1, 'data_type'] = 'test'\n",
    "    \n",
    "    df2.loc[X_train_idx2, 'data_type'] = 'train'\n",
    "    df2.loc[X_val_idx2, 'data_type'] = 'val'\n",
    "    df2.loc[X_test_idx2, 'data_type'] = 'test'\n",
    "    \n",
    "    # If you want to combine df1 and df2 into a single dataframe:\n",
    "    df = pd.concat([df1, df2])\n",
    "\n",
    "\n",
    "    \n",
    "    # Training loop for grid search\n",
    "    for lr in learning_rates:\n",
    "      for batch_size in batch_sizes:\n",
    "          for epochs in epochs_list:\n",
    "            logger.info(f\"Seed: {seed_val}, Learning Rate: {lr}, Batch Size: {batch_size}, Epochs: {epochs}\")\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "            encoded_data_train = tokenizer.batch_encode_plus(\n",
    "                df[df.data_type == 'train'].ade.values,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=True,\n",
    "                pad_to_max_length=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            encoded_data_val = tokenizer.batch_encode_plus(\n",
    "                df[df.data_type == 'val'].ade.values,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=True,\n",
    "                pad_to_max_length=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids_train = encoded_data_train['input_ids']\n",
    "            attention_masks_train = encoded_data_train['attention_mask']\n",
    "            labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "            input_ids_val = encoded_data_val['input_ids']\n",
    "            attention_masks_val = encoded_data_val['attention_mask']\n",
    "            labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n",
    "\n",
    "            dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "            dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "            model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top3label_dict), output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "            dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "            dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "            optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            logger.info(f\"Device used: {device}\")\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "                model.train()\n",
    "                loss_train_total = 0\n",
    "\n",
    "                progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "                for batch in progress_bar:\n",
    "                    model.zero_grad()\n",
    "                    batch = tuple(b.to(device) for b in batch)\n",
    "                    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "                    outputs = model(**inputs)\n",
    "                    loss = outputs[0]\n",
    "                    loss_train_total += loss.item()\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "\n",
    "                # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n",
    "\n",
    "                logger.info(f'\\nEpoch {epoch}')\n",
    "                loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "                logger.info(f'Training loss: {loss_train_avg}')\n",
    "                training_losses.append(loss_train_avg)\n",
    "\n",
    "            val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(true_vals, np.argmax(predictions, axis=1), average=None)\n",
    "            macro_precision = np.mean(precision)\n",
    "            macro_recall = np.mean(recall)\n",
    "            macro_f1 = np.mean(f1)\n",
    "\n",
    "            # Store metrics\n",
    "            macro_precisions[batch_size].append(macro_precision)\n",
    "            macro_recalls[batch_size].append(macro_recall)\n",
    "            macro_f1s[batch_size].append(macro_f1)\n",
    "\n",
    "\n",
    "            #store the training loss\n",
    "            all_training_losses.append(training_losses)\n",
    "            #store the val loss\n",
    "            all_val_losses.append(val_losses)\n",
    "\n",
    "            #store the prediction\n",
    "            accuracy_dict, count_dict = accuracy_per_class(predictions, true_vals)\n",
    "\n",
    "            for label, accuracy in accuracy_dict.items():\n",
    "                all_accuracies[label].append(accuracy)\n",
    "\n",
    "\n",
    "            # Calculate the average accuracy for each label\n",
    "            avg_accuracy = {label: np.mean(accs) for label, accs in all_accuracies.items()}\n",
    "\n",
    "            # Calculate the overall average accuracy across all labels\n",
    "            overall_avg_accuracy = np.mean(list(avg_accuracy.values()))\n",
    "\n",
    "            logger.info(f'Seed {seed_val} - Accuracy: {overall_avg_accuracy} - Count: {count_dict} - lr: {lr} -batchsize:{batch_size} -epochs:{epochs}')\n",
    "            #store results\n",
    "            results.append((lr, batch_size, epochs, overall_avg_accuracy))\n",
    "\n",
    "\n",
    "# Compute means and standard deviations\n",
    "mean_precisions = {bs: np.mean(macro_precisions[bs]) for bs in batch_sizes}\n",
    "std_precisions = {bs: np.std(macro_precisions[bs]) for bs in batch_sizes}\n",
    "mean_recalls = {bs: np.mean(macro_recalls[bs]) for bs in batch_sizes}\n",
    "std_recalls = {bs: np.std(macro_recalls[bs]) for bs in batch_sizes}\n",
    "mean_f1s = {bs: np.mean(macro_f1s[bs]) for bs in batch_sizes}\n",
    "std_f1s = {bs: np.std(macro_f1s[bs]) for bs in batch_sizes}\n",
    "\n",
    "# Open a file to write the results\n",
    "with open('combine_finetuning_batchsize_test.txt', 'w') as file:\n",
    "    file.write(\"Batch Size\\tMacro Precision\\tMacro Recall\\tMacro F1 Score\\n\")\n",
    "    for bs in batch_sizes:\n",
    "        precisions = ', '.join(map(str, macro_precisions[bs]))\n",
    "        recalls = ', '.join(map(str, macro_recalls[bs]))\n",
    "        f1s = ', '.join(map(str, macro_f1s[bs]))\n",
    "        file.write(f\"{bs}\\t{precisions}\\t{recalls}\\t{f1s}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert batch sizes to strings for categorical x-axis\n",
    "batch_size_labels = list(map(str, batch_sizes))\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Precision with a specific color\n",
    "plt.errorbar(batch_size_labels, [mean_precisions[bs] for bs in batch_sizes], \n",
    "             yerr=[std_precisions[bs] for bs in batch_sizes], \n",
    "             label='Macro Precision', fmt='-o', color='blue')\n",
    "\n",
    "# Recall with a different color\n",
    "plt.errorbar(batch_size_labels, [mean_recalls[bs] for bs in batch_sizes], \n",
    "             yerr=[std_recalls[bs] for bs in batch_sizes], \n",
    "             label='Macro Recall', fmt='-o', color='green')\n",
    "\n",
    "# F1 Score with another color\n",
    "plt.errorbar(batch_size_labels, [mean_f1s[bs] for bs in batch_sizes], \n",
    "             yerr=[std_f1s[bs] for bs in batch_sizes], \n",
    "             label='Macro F1 Score', fmt='-o', color='red')\n",
    "\n",
    "# Add labels and title with larger font size\n",
    "plt.xlabel('Batch Size', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.title('Macro Average Precision, Recall, and F1 Score vs. Batch Size', fontsize=16)\n",
    "\n",
    "# Add legend with larger font size\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Grid with default settings\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot to a file with higher resolution\n",
    "plt.savefig('combine_top3_f1_vs_batchsize_test.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
