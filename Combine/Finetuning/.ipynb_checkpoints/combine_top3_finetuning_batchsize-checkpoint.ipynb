{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2367b9-1427-4f0f-980a-2c8905c36e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\AppData\\Local\\Temp\\ipykernel_11484\\536078145.py:213: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2.loc[:, 'label'] = df2['soc_code'].map(top3label_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMM4H top 3                             ade  soc_code  label\n",
      "3                            AD  10037175      0\n",
      "4                         focus  10029205      2\n",
      "5                          died  10018065      1\n",
      "8                        dreams  10037175      0\n",
      "10                   withdrawal  10018065      1\n",
      "...                         ...       ...    ...\n",
      "1695       talk a mile a minute  10037175      0\n",
      "1698     can't go back to sleep  10037175      0\n",
      "1703                 chest hurt  10018065      1\n",
      "1704   got ten minutes of sleep  10037175      0\n",
      "1708  never have another orgasm  10037175      0\n",
      "\n",
      "[734 rows x 3 columns]\n",
      "CADEC top 3                             ade  soc_code  label\n",
      "926            voracious hunger  10018065      1\n",
      "927            loss of appetite  10018065      1\n",
      "929            lack of appetite  10018065      1\n",
      "931                    anorexia  10018065      1\n",
      "932                    anorexic  10018065      1\n",
      "...                         ...       ...    ...\n",
      "5326  short term memory lacking  10037175      0\n",
      "5328      couldn't eat or drink  10037175      0\n",
      "5329              Could not eat  10037175      0\n",
      "5331           can't eat normal  10037175      0\n",
      "5332   Disturbed sleep patterns  10037175      0\n",
      "\n",
      "[1341 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                 | 0/208 [00:00<?, ?it/s]\u001b[AC:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "Epoch 1:   0%|                                                            | 0/208 [00:02<?, ?it/s, training_loss=0.414]\u001b[A\n",
      "Epoch 1:   0%|▎                                                   | 1/208 [00:02<09:02,  2.62s/it, training_loss=0.414]\u001b[A\n",
      "Epoch 1:   0%|▎                                                   | 1/208 [00:02<09:02,  2.62s/it, training_loss=0.344]\u001b[A\n",
      "Epoch 1:   1%|▌                                                   | 2/208 [00:02<03:54,  1.14s/it, training_loss=0.344]\u001b[A\n",
      "Epoch 1:   1%|▌                                                   | 2/208 [00:02<03:54,  1.14s/it, training_loss=0.386]\u001b[A\n",
      "Epoch 1:   1%|▊                                                   | 3/208 [00:02<02:18,  1.48it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:   1%|▊                                                   | 3/208 [00:02<02:18,  1.48it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   2%|█                                                   | 4/208 [00:02<01:32,  2.21it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   2%|█                                                   | 4/208 [00:03<01:32,  2.21it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:   2%|█                                                   | 4/208 [00:03<01:32,  2.21it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:   3%|█▌                                                  | 6/208 [00:03<00:53,  3.79it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:   3%|█▌                                                  | 6/208 [00:03<00:53,  3.79it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:   3%|█▌                                                  | 6/208 [00:03<00:53,  3.79it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:   4%|██                                                  | 8/208 [00:03<00:38,  5.20it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:   4%|██                                                  | 8/208 [00:03<00:38,  5.20it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:   4%|██                                                  | 8/208 [00:03<00:38,  5.20it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:   5%|██▍                                                | 10/208 [00:03<00:30,  6.49it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:   5%|██▍                                                | 10/208 [00:03<00:30,  6.49it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:   5%|██▍                                                | 10/208 [00:03<00:30,  6.49it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:   6%|██▉                                                | 12/208 [00:03<00:26,  7.42it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:   6%|██▉                                                | 12/208 [00:03<00:26,  7.42it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   6%|███▏                                               | 13/208 [00:03<00:25,  7.63it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   6%|███▏                                               | 13/208 [00:03<00:25,  7.63it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   7%|███▍                                               | 14/208 [00:03<00:24,  8.04it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   7%|███▍                                               | 14/208 [00:04<00:24,  8.04it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:   7%|███▋                                               | 15/208 [00:04<00:22,  8.44it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:   7%|███▋                                               | 15/208 [00:04<00:22,  8.44it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:   8%|███▉                                               | 16/208 [00:04<00:22,  8.71it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:   8%|███▉                                               | 16/208 [00:04<00:22,  8.71it/s, training_loss=0.409]\u001b[A\n",
      "Epoch 1:   8%|███▉                                               | 16/208 [00:04<00:22,  8.71it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:   9%|████▍                                              | 18/208 [00:04<00:20,  9.32it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:   9%|████▍                                              | 18/208 [00:04<00:20,  9.32it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:   9%|████▋                                              | 19/208 [00:04<00:20,  9.45it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:   9%|████▋                                              | 19/208 [00:04<00:20,  9.45it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  10%|████▉                                              | 20/208 [00:04<00:19,  9.55it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  10%|████▉                                              | 20/208 [00:04<00:19,  9.55it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  10%|█████▏                                             | 21/208 [00:04<00:19,  9.66it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  10%|█████▏                                             | 21/208 [00:04<00:19,  9.66it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  10%|█████▏                                             | 21/208 [00:04<00:19,  9.66it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                             | 23/208 [00:04<00:18,  9.81it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                             | 23/208 [00:04<00:18,  9.81it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  12%|█████▉                                             | 24/208 [00:04<00:18,  9.84it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  12%|█████▉                                             | 24/208 [00:05<00:18,  9.84it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  12%|██████▏                                            | 25/208 [00:05<00:18,  9.87it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  12%|██████▏                                            | 25/208 [00:05<00:18,  9.87it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  12%|██████▏                                            | 25/208 [00:05<00:18,  9.87it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  13%|██████▌                                            | 27/208 [00:05<00:18,  9.92it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  13%|██████▌                                            | 27/208 [00:05<00:18,  9.92it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  13%|██████▌                                            | 27/208 [00:05<00:18,  9.92it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  14%|███████                                            | 29/208 [00:05<00:17,  9.95it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  14%|███████                                            | 29/208 [00:05<00:17,  9.95it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  14%|███████▎                                           | 30/208 [00:05<00:18,  9.63it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  14%|███████▎                                           | 30/208 [00:05<00:18,  9.63it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  14%|███████▎                                           | 30/208 [00:05<00:18,  9.63it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  15%|███████▊                                           | 32/208 [00:05<00:17,  9.90it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  15%|███████▊                                           | 32/208 [00:05<00:17,  9.90it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  15%|███████▊                                           | 32/208 [00:05<00:17,  9.90it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  16%|████████▎                                          | 34/208 [00:05<00:17, 10.05it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  16%|████████▎                                          | 34/208 [00:06<00:17, 10.05it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  16%|████████▎                                          | 34/208 [00:06<00:17, 10.05it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  17%|████████▊                                          | 36/208 [00:06<00:16, 10.14it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  17%|████████▊                                          | 36/208 [00:06<00:16, 10.14it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  17%|████████▊                                          | 36/208 [00:06<00:16, 10.14it/s, training_loss=0.395]\u001b[A\n",
      "Epoch 1:  18%|█████████▎                                         | 38/208 [00:06<00:16, 10.26it/s, training_loss=0.395]\u001b[A\n",
      "Epoch 1:  18%|█████████▎                                         | 38/208 [00:06<00:16, 10.26it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  18%|█████████▎                                         | 38/208 [00:06<00:16, 10.26it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  19%|█████████▊                                         | 40/208 [00:06<00:16, 10.30it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  19%|█████████▊                                         | 40/208 [00:06<00:16, 10.30it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  19%|█████████▊                                         | 40/208 [00:06<00:16, 10.30it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  20%|██████████▎                                        | 42/208 [00:06<00:16, 10.26it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  20%|██████████▎                                        | 42/208 [00:06<00:16, 10.26it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  20%|██████████▎                                        | 42/208 [00:06<00:16, 10.26it/s, training_loss=0.432]\u001b[A\n",
      "Epoch 1:  21%|██████████▊                                        | 44/208 [00:06<00:15, 10.57it/s, training_loss=0.432]\u001b[A\n",
      "Epoch 1:  21%|██████████▊                                        | 44/208 [00:07<00:15, 10.57it/s, training_loss=0.408]\u001b[A\n",
      "Epoch 1:  21%|██████████▊                                        | 44/208 [00:07<00:15, 10.57it/s, training_loss=0.449]\u001b[A\n",
      "Epoch 1:  22%|███████████▎                                       | 46/208 [00:07<00:15, 10.40it/s, training_loss=0.449]\u001b[A\n",
      "Epoch 1:  22%|███████████▎                                       | 46/208 [00:07<00:15, 10.40it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  22%|███████████▎                                       | 46/208 [00:07<00:15, 10.40it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  23%|███████████▊                                       | 48/208 [00:07<00:15, 10.26it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  23%|███████████▊                                       | 48/208 [00:07<00:15, 10.26it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  23%|███████████▊                                       | 48/208 [00:07<00:15, 10.26it/s, training_loss=0.413]\u001b[A\n",
      "Epoch 1:  24%|████████████▎                                      | 50/208 [00:07<00:15, 10.46it/s, training_loss=0.413]\u001b[A\n",
      "Epoch 1:  24%|████████████▎                                      | 50/208 [00:07<00:15, 10.46it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  24%|████████████▎                                      | 50/208 [00:07<00:15, 10.46it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  25%|████████████▊                                      | 52/208 [00:07<00:15, 10.27it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  25%|████████████▊                                      | 52/208 [00:07<00:15, 10.27it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  25%|████████████▊                                      | 52/208 [00:07<00:15, 10.27it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 1:  26%|█████████████▏                                     | 54/208 [00:07<00:14, 10.50it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 1:  26%|█████████████▏                                     | 54/208 [00:07<00:14, 10.50it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  26%|█████████████▏                                     | 54/208 [00:08<00:14, 10.50it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  27%|█████████████▋                                     | 56/208 [00:08<00:14, 10.36it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  27%|█████████████▋                                     | 56/208 [00:08<00:14, 10.36it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  27%|█████████████▋                                     | 56/208 [00:08<00:14, 10.36it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  28%|██████████████▏                                    | 58/208 [00:08<00:14, 10.50it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  28%|██████████████▏                                    | 58/208 [00:08<00:14, 10.50it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  28%|██████████████▏                                    | 58/208 [00:08<00:14, 10.50it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  29%|██████████████▋                                    | 60/208 [00:08<00:14, 10.31it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  29%|██████████████▋                                    | 60/208 [00:08<00:14, 10.31it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  29%|██████████████▋                                    | 60/208 [00:08<00:14, 10.31it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  30%|███████████████▏                                   | 62/208 [00:08<00:14, 10.21it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  30%|███████████████▏                                   | 62/208 [00:08<00:14, 10.21it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  30%|███████████████▏                                   | 62/208 [00:08<00:14, 10.21it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  31%|███████████████▋                                   | 64/208 [00:08<00:13, 10.41it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  31%|███████████████▋                                   | 64/208 [00:08<00:13, 10.41it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  31%|███████████████▋                                   | 64/208 [00:09<00:13, 10.41it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  32%|████████████████▏                                  | 66/208 [00:09<00:13, 10.28it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  32%|████████████████▏                                  | 66/208 [00:09<00:13, 10.28it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  32%|████████████████▏                                  | 66/208 [00:09<00:13, 10.28it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  33%|████████████████▋                                  | 68/208 [00:09<00:13, 10.23it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  33%|████████████████▋                                  | 68/208 [00:09<00:13, 10.23it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 1:  33%|████████████████▋                                  | 68/208 [00:09<00:13, 10.23it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▏                                 | 70/208 [00:09<00:13, 10.00it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▏                                 | 70/208 [00:09<00:13, 10.00it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▏                                 | 70/208 [00:09<00:13, 10.00it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▋                                 | 72/208 [00:09<00:13, 10.01it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▋                                 | 72/208 [00:09<00:13, 10.01it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▋                                 | 72/208 [00:09<00:13, 10.01it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▏                                | 74/208 [00:09<00:13, 10.13it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▏                                | 74/208 [00:09<00:13, 10.13it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▏                                | 74/208 [00:10<00:13, 10.13it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  37%|██████████████████▋                                | 76/208 [00:10<00:12, 10.34it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  37%|██████████████████▋                                | 76/208 [00:10<00:12, 10.34it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  37%|██████████████████▋                                | 76/208 [00:10<00:12, 10.34it/s, training_loss=0.300]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▏                               | 78/208 [00:10<00:12, 10.20it/s, training_loss=0.300]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▏                               | 78/208 [00:10<00:12, 10.20it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▏                               | 78/208 [00:10<00:12, 10.20it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▌                               | 80/208 [00:10<00:12, 10.42it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▌                               | 80/208 [00:10<00:12, 10.42it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▌                               | 80/208 [00:10<00:12, 10.42it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 1:  39%|████████████████████                               | 82/208 [00:10<00:12, 10.32it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 1:  39%|████████████████████                               | 82/208 [00:10<00:12, 10.32it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 1:  39%|████████████████████                               | 82/208 [00:10<00:12, 10.32it/s, training_loss=0.287]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▌                              | 84/208 [00:10<00:11, 10.45it/s, training_loss=0.287]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▌                              | 84/208 [00:10<00:11, 10.45it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▌                              | 84/208 [00:10<00:11, 10.45it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                              | 86/208 [00:10<00:11, 10.32it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                              | 86/208 [00:11<00:11, 10.32it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                              | 86/208 [00:11<00:11, 10.32it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▌                             | 88/208 [00:11<00:11, 10.47it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▌                             | 88/208 [00:11<00:11, 10.47it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▌                             | 88/208 [00:11<00:11, 10.47it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████                             | 90/208 [00:11<00:11, 10.36it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████                             | 90/208 [00:11<00:11, 10.36it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████                             | 90/208 [00:11<00:11, 10.36it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▌                            | 92/208 [00:11<00:11, 10.25it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▌                            | 92/208 [00:11<00:11, 10.25it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▌                            | 92/208 [00:11<00:11, 10.25it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 1:  45%|███████████████████████                            | 94/208 [00:11<00:10, 10.39it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 1:  45%|███████████████████████                            | 94/208 [00:11<00:10, 10.39it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  45%|███████████████████████                            | 94/208 [00:11<00:10, 10.39it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▌                           | 96/208 [00:11<00:11, 10.10it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▌                           | 96/208 [00:12<00:11, 10.10it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▌                           | 96/208 [00:12<00:11, 10.10it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  47%|████████████████████████                           | 98/208 [00:12<00:10, 10.25it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  47%|████████████████████████                           | 98/208 [00:12<00:10, 10.25it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  47%|████████████████████████                           | 98/208 [00:12<00:10, 10.25it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████                          | 100/208 [00:12<00:10, 10.46it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████                          | 100/208 [00:12<00:10, 10.46it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████                          | 100/208 [00:12<00:10, 10.46it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████▌                         | 102/208 [00:12<00:10, 10.32it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████▌                         | 102/208 [00:12<00:10, 10.32it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████▌                         | 102/208 [00:12<00:10, 10.32it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████                         | 104/208 [00:12<00:10, 10.21it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████                         | 104/208 [00:12<00:10, 10.21it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████                         | 104/208 [00:12<00:10, 10.21it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████▍                        | 106/208 [00:12<00:09, 10.39it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████▍                        | 106/208 [00:13<00:09, 10.39it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████▍                        | 106/208 [00:13<00:09, 10.39it/s, training_loss=0.259]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████▉                        | 108/208 [00:13<00:09, 10.22it/s, training_loss=0.259]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████▉                        | 108/208 [00:13<00:09, 10.22it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████▉                        | 108/208 [00:13<00:09, 10.22it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████▍                       | 110/208 [00:13<00:09, 10.14it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████▍                       | 110/208 [00:13<00:09, 10.14it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████▍                       | 110/208 [00:13<00:09, 10.14it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  54%|██████████████████████████▉                       | 112/208 [00:13<00:09, 10.10it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  54%|██████████████████████████▉                       | 112/208 [00:13<00:09, 10.10it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  54%|██████████████████████████▉                       | 112/208 [00:13<00:09, 10.10it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████▍                      | 114/208 [00:13<00:09, 10.10it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████▍                      | 114/208 [00:13<00:09, 10.10it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████▍                      | 114/208 [00:13<00:09, 10.10it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  56%|███████████████████████████▉                      | 116/208 [00:13<00:09, 10.03it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  56%|███████████████████████████▉                      | 116/208 [00:14<00:09, 10.03it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  56%|███████████████████████████▉                      | 116/208 [00:14<00:09, 10.03it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████▎                     | 118/208 [00:14<00:08, 10.05it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████▎                     | 118/208 [00:14<00:08, 10.05it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████▎                     | 118/208 [00:14<00:08, 10.05it/s, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  58%|████████████████████████████▊                     | 120/208 [00:14<00:08, 10.02it/s, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  58%|████████████████████████████▊                     | 120/208 [00:14<00:08, 10.02it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  58%|████████████████████████████▊                     | 120/208 [00:14<00:08, 10.02it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  59%|█████████████████████████████▎                    | 122/208 [00:14<00:08, 10.05it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  59%|█████████████████████████████▎                    | 122/208 [00:14<00:08, 10.05it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  59%|█████████████████████████████▎                    | 122/208 [00:14<00:08, 10.05it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  60%|█████████████████████████████▊                    | 124/208 [00:14<00:08, 10.00it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  60%|█████████████████████████████▊                    | 124/208 [00:14<00:08, 10.00it/s, training_loss=0.287]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████                    | 125/208 [00:14<00:08,  9.97it/s, training_loss=0.287]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████                    | 125/208 [00:14<00:08,  9.97it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████                    | 125/208 [00:15<00:08,  9.97it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  61%|██████████████████████████████▌                   | 127/208 [00:15<00:08, 10.01it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  61%|██████████████████████████████▌                   | 127/208 [00:15<00:08, 10.01it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 1:  62%|██████████████████████████████▊                   | 128/208 [00:15<00:08,  9.98it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 1:  62%|██████████████████████████████▊                   | 128/208 [00:15<00:08,  9.98it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████                   | 129/208 [00:15<00:07,  9.96it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████                   | 129/208 [00:15<00:07,  9.96it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████                   | 129/208 [00:15<00:07,  9.96it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 1:  63%|███████████████████████████████▍                  | 131/208 [00:15<00:07, 10.00it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 1:  63%|███████████████████████████████▍                  | 131/208 [00:15<00:07, 10.00it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  63%|███████████████████████████████▋                  | 132/208 [00:15<00:07, 10.00it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  63%|███████████████████████████████▋                  | 132/208 [00:15<00:07, 10.00it/s, training_loss=0.282]\u001b[A\n",
      "Epoch 1:  64%|███████████████████████████████▉                  | 133/208 [00:15<00:07,  9.97it/s, training_loss=0.282]\u001b[A\n",
      "Epoch 1:  64%|███████████████████████████████▉                  | 133/208 [00:15<00:07,  9.97it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████▏                 | 134/208 [00:15<00:07,  9.97it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████▏                 | 134/208 [00:15<00:07,  9.97it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  65%|████████████████████████████████▍                 | 135/208 [00:15<00:07,  9.93it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  65%|████████████████████████████████▍                 | 135/208 [00:15<00:07,  9.93it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 1:  65%|████████████████████████████████▍                 | 135/208 [00:16<00:07,  9.93it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  66%|████████████████████████████████▉                 | 137/208 [00:16<00:07, 10.04it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  66%|████████████████████████████████▉                 | 137/208 [00:16<00:07, 10.04it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████▏                | 138/208 [00:16<00:06, 10.00it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████▏                | 138/208 [00:16<00:06, 10.00it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████▏                | 138/208 [00:16<00:06, 10.00it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  67%|█████████████████████████████████▋                | 140/208 [00:16<00:06, 10.06it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  67%|█████████████████████████████████▋                | 140/208 [00:16<00:06, 10.06it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  67%|█████████████████████████████████▋                | 140/208 [00:16<00:06, 10.06it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████████████████▏               | 142/208 [00:16<00:06, 10.32it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████████████████▏               | 142/208 [00:16<00:06, 10.32it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████████████████▏               | 142/208 [00:16<00:06, 10.32it/s, training_loss=0.189]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████▌               | 144/208 [00:16<00:06, 10.19it/s, training_loss=0.189]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████▌               | 144/208 [00:16<00:06, 10.19it/s, training_loss=0.252]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████▌               | 144/208 [00:16<00:06, 10.19it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████████████████               | 146/208 [00:16<00:06, 10.12it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████████████████               | 146/208 [00:17<00:06, 10.12it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████████████████               | 146/208 [00:17<00:06, 10.12it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████▌              | 148/208 [00:17<00:06,  9.86it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████▌              | 148/208 [00:17<00:06,  9.86it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████▊              | 149/208 [00:17<00:05,  9.86it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████▊              | 149/208 [00:17<00:05,  9.86it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████              | 150/208 [00:17<00:05,  9.87it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████              | 150/208 [00:17<00:05,  9.87it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████▎             | 151/208 [00:17<00:05,  9.88it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████▎             | 151/208 [00:17<00:05,  9.88it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████▌             | 152/208 [00:17<00:05,  9.88it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████▌             | 152/208 [00:17<00:05,  9.88it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 1:  74%|████████████████████████████████████▊             | 153/208 [00:17<00:05,  9.90it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 1:  74%|████████████████████████████████████▊             | 153/208 [00:17<00:05,  9.90it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████             | 154/208 [00:17<00:05,  9.90it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████             | 154/208 [00:17<00:05,  9.90it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████             | 154/208 [00:17<00:05,  9.90it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████▌            | 156/208 [00:17<00:05, 10.07it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████▌            | 156/208 [00:18<00:05, 10.07it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████▌            | 156/208 [00:18<00:05, 10.07it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  76%|█████████████████████████████████████▉            | 158/208 [00:18<00:04, 10.03it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  76%|█████████████████████████████████████▉            | 158/208 [00:18<00:04, 10.03it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  76%|█████████████████████████████████████▉            | 158/208 [00:18<00:04, 10.03it/s, training_loss=0.287]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████▍           | 160/208 [00:18<00:04, 10.29it/s, training_loss=0.287]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████▍           | 160/208 [00:18<00:04, 10.29it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████▍           | 160/208 [00:18<00:04, 10.29it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  78%|██████████████████████████████████████▉           | 162/208 [00:18<00:04, 10.16it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  78%|██████████████████████████████████████▉           | 162/208 [00:18<00:04, 10.16it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  78%|██████████████████████████████████████▉           | 162/208 [00:18<00:04, 10.16it/s, training_loss=0.223]\u001b[A\n",
      "Epoch 1:  79%|███████████████████████████████████████▍          | 164/208 [00:18<00:04, 10.15it/s, training_loss=0.223]\u001b[A\n",
      "Epoch 1:  79%|███████████████████████████████████████▍          | 164/208 [00:18<00:04, 10.15it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  79%|███████████████████████████████████████▍          | 164/208 [00:18<00:04, 10.15it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  80%|███████████████████████████████████████▉          | 166/208 [00:18<00:04, 10.10it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  80%|███████████████████████████████████████▉          | 166/208 [00:19<00:04, 10.10it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 1:  80%|███████████████████████████████████████▉          | 166/208 [00:19<00:04, 10.10it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████▍         | 168/208 [00:19<00:03, 10.06it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████▍         | 168/208 [00:19<00:03, 10.06it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████▍         | 168/208 [00:19<00:03, 10.06it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 1:  82%|████████████████████████████████████████▊         | 170/208 [00:19<00:03, 10.06it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 1:  82%|████████████████████████████████████████▊         | 170/208 [00:19<00:03, 10.06it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  82%|████████████████████████████████████████▊         | 170/208 [00:19<00:03, 10.06it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████▎        | 172/208 [00:19<00:03, 10.03it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████▎        | 172/208 [00:19<00:03, 10.03it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████▎        | 172/208 [00:19<00:03, 10.03it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  84%|█████████████████████████████████████████▊        | 174/208 [00:19<00:03, 10.03it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  84%|█████████████████████████████████████████▊        | 174/208 [00:19<00:03, 10.03it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 1:  84%|█████████████████████████████████████████▊        | 174/208 [00:19<00:03, 10.03it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████▎       | 176/208 [00:19<00:03, 10.17it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████▎       | 176/208 [00:20<00:03, 10.17it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████▎       | 176/208 [00:20<00:03, 10.17it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████▊       | 178/208 [00:20<00:03,  9.96it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████▊       | 178/208 [00:20<00:03,  9.96it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  86%|███████████████████████████████████████████       | 179/208 [00:20<00:02,  9.96it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  86%|███████████████████████████████████████████       | 179/208 [00:20<00:02,  9.96it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▎      | 180/208 [00:20<00:02,  9.93it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▎      | 180/208 [00:20<00:02,  9.93it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▌      | 181/208 [00:20<00:02,  9.93it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▌      | 181/208 [00:20<00:02,  9.93it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████▊      | 182/208 [00:20<00:02,  9.94it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████▊      | 182/208 [00:20<00:02,  9.94it/s, training_loss=0.261]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████▊      | 182/208 [00:20<00:02,  9.94it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████▏     | 184/208 [00:20<00:02, 10.00it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████▏     | 184/208 [00:20<00:02, 10.00it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████▏     | 184/208 [00:20<00:02, 10.00it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████▋     | 186/208 [00:20<00:02, 10.04it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████▋     | 186/208 [00:21<00:02, 10.04it/s, training_loss=0.207]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████▋     | 186/208 [00:21<00:02, 10.04it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████████████████████▏    | 188/208 [00:21<00:01, 10.18it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████████████████████▏    | 188/208 [00:21<00:01, 10.18it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████████████████████▏    | 188/208 [00:21<00:01, 10.18it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████▋    | 190/208 [00:21<00:01,  9.97it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████▋    | 190/208 [00:21<00:01,  9.97it/s, training_loss=0.243]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████▋    | 190/208 [00:21<00:01,  9.97it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████▏   | 192/208 [00:21<00:01, 10.14it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████▏   | 192/208 [00:21<00:01, 10.14it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████▏   | 192/208 [00:21<00:01, 10.14it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▋   | 194/208 [00:21<00:01, 10.07it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▋   | 194/208 [00:21<00:01, 10.07it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▋   | 194/208 [00:21<00:01, 10.07it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████   | 196/208 [00:21<00:01,  9.94it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████   | 196/208 [00:22<00:01,  9.94it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████   | 196/208 [00:22<00:01,  9.94it/s, training_loss=0.243]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████▌  | 198/208 [00:22<00:01,  9.97it/s, training_loss=0.243]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████▌  | 198/208 [00:22<00:01,  9.97it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 1:  96%|███████████████████████████████████████████████▊  | 199/208 [00:22<00:00,  9.93it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 1:  96%|███████████████████████████████████████████████▊  | 199/208 [00:22<00:00,  9.93it/s, training_loss=0.233]\u001b[A\n",
      "Epoch 1:  96%|███████████████████████████████████████████████▊  | 199/208 [00:22<00:00,  9.93it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▎ | 201/208 [00:22<00:00,  9.98it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▎ | 201/208 [00:22<00:00,  9.98it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▌ | 202/208 [00:22<00:00,  9.96it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▌ | 202/208 [00:22<00:00,  9.96it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  98%|████████████████████████████████████████████████▊ | 203/208 [00:22<00:00,  9.93it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  98%|████████████████████████████████████████████████▊ | 203/208 [00:22<00:00,  9.93it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 1:  98%|█████████████████████████████████████████████████ | 204/208 [00:22<00:00,  9.92it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 1:  98%|█████████████████████████████████████████████████ | 204/208 [00:22<00:00,  9.92it/s, training_loss=0.287]\u001b[A\n",
      "Epoch 1:  98%|█████████████████████████████████████████████████ | 204/208 [00:22<00:00,  9.92it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▌| 206/208 [00:22<00:00, 10.00it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▌| 206/208 [00:23<00:00, 10.00it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████▊| 207/208 [00:23<00:00,  9.98it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████▊| 207/208 [00:23<00:00,  9.98it/s, training_loss=0.522]\u001b[A\n",
      "Epoch Progress:  50%|██████████████████████████████████                                  | 1/2 [00:23<00:23, 23.11s/it]\u001b[A\n",
      "Epoch 2:   0%|                                                                                 | 0/208 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:   0%|                                                            | 0/208 [00:00<?, ?it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 2:   0%|                                                            | 0/208 [00:00<?, ?it/s, training_loss=0.161]\u001b[A\n",
      "Epoch 2:   1%|▌                                                   | 2/208 [00:00<00:20, 10.00it/s, training_loss=0.161]\u001b[A\n",
      "Epoch 2:   1%|▌                                                   | 2/208 [00:00<00:20, 10.00it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 2:   1%|▌                                                   | 2/208 [00:00<00:20, 10.00it/s, training_loss=0.214]\u001b[A\n",
      "Epoch 2:   2%|█                                                   | 4/208 [00:00<00:20,  9.97it/s, training_loss=0.214]\u001b[A\n",
      "Epoch 2:   2%|█                                                   | 4/208 [00:00<00:20,  9.97it/s, training_loss=0.179]\u001b[A\n",
      "Epoch 2:   2%|█▎                                                  | 5/208 [00:00<00:20,  9.96it/s, training_loss=0.179]\u001b[A\n",
      "Epoch 2:   2%|█▎                                                  | 5/208 [00:00<00:20,  9.96it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 2:   2%|█▎                                                  | 5/208 [00:00<00:20,  9.96it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 2:   3%|█▊                                                  | 7/208 [00:00<00:19, 10.07it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 2:   3%|█▊                                                  | 7/208 [00:00<00:19, 10.07it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:   3%|█▊                                                  | 7/208 [00:00<00:19, 10.07it/s, training_loss=0.184]\u001b[A\n",
      "Epoch 2:   4%|██▎                                                 | 9/208 [00:00<00:19,  9.98it/s, training_loss=0.184]\u001b[A\n",
      "Epoch 2:   4%|██▎                                                 | 9/208 [00:01<00:19,  9.98it/s, training_loss=0.221]\u001b[A\n",
      "Epoch 2:   5%|██▍                                                | 10/208 [00:01<00:20,  9.58it/s, training_loss=0.221]\u001b[A\n",
      "Epoch 2:   5%|██▍                                                | 10/208 [00:01<00:20,  9.58it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:   5%|██▋                                                | 11/208 [00:01<00:20,  9.65it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:   5%|██▋                                                | 11/208 [00:01<00:20,  9.65it/s, training_loss=0.179]\u001b[A\n",
      "Epoch 2:   6%|██▉                                                | 12/208 [00:01<00:20,  9.68it/s, training_loss=0.179]\u001b[A\n",
      "Epoch 2:   6%|██▉                                                | 12/208 [00:01<00:20,  9.68it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 2:   6%|███▏                                               | 13/208 [00:01<00:19,  9.76it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 2:   6%|███▏                                               | 13/208 [00:01<00:19,  9.76it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 2:   6%|███▏                                               | 13/208 [00:01<00:19,  9.76it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 2:   7%|███▋                                               | 15/208 [00:01<00:19,  9.90it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 2:   7%|███▋                                               | 15/208 [00:01<00:19,  9.90it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 2:   8%|███▉                                               | 16/208 [00:01<00:19,  9.89it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 2:   8%|███▉                                               | 16/208 [00:01<00:19,  9.89it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 2:   8%|████▏                                              | 17/208 [00:01<00:19,  9.89it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 2:   8%|████▏                                              | 17/208 [00:01<00:19,  9.89it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 2:   8%|████▏                                              | 17/208 [00:01<00:19,  9.89it/s, training_loss=0.194]\u001b[A\n",
      "Epoch 2:   9%|████▋                                              | 19/208 [00:01<00:18, 10.12it/s, training_loss=0.194]\u001b[A\n",
      "Epoch 2:   9%|████▋                                              | 19/208 [00:02<00:18, 10.12it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 2:   9%|████▋                                              | 19/208 [00:02<00:18, 10.12it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 2:  10%|█████▏                                             | 21/208 [00:02<00:18, 10.04it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 2:  10%|█████▏                                             | 21/208 [00:02<00:18, 10.04it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 2:  10%|█████▏                                             | 21/208 [00:02<00:18, 10.04it/s, training_loss=0.233]\u001b[A\n",
      "Epoch 2:  11%|█████▋                                             | 23/208 [00:02<00:18,  9.97it/s, training_loss=0.233]\u001b[A\n",
      "Epoch 2:  11%|█████▋                                             | 23/208 [00:02<00:18,  9.97it/s, training_loss=0.273]\u001b[A\n",
      "Epoch 2:  12%|█████▉                                             | 24/208 [00:02<00:18,  9.97it/s, training_loss=0.273]\u001b[A\n",
      "Epoch 2:  12%|█████▉                                             | 24/208 [00:02<00:18,  9.97it/s, training_loss=0.193]\u001b[A\n",
      "Epoch 2:  12%|██████▏                                            | 25/208 [00:02<00:18,  9.94it/s, training_loss=0.193]\u001b[A\n",
      "Epoch 2:  12%|██████▏                                            | 25/208 [00:02<00:18,  9.94it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  12%|██████▍                                            | 26/208 [00:02<00:18,  9.92it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  12%|██████▍                                            | 26/208 [00:02<00:18,  9.92it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  13%|██████▌                                            | 27/208 [00:02<00:18,  9.93it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  13%|██████▌                                            | 27/208 [00:02<00:18,  9.93it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  13%|██████▊                                            | 28/208 [00:02<00:18,  9.94it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  13%|██████▊                                            | 28/208 [00:02<00:18,  9.94it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 2:  13%|██████▊                                            | 28/208 [00:03<00:18,  9.94it/s, training_loss=0.215]\u001b[A\n",
      "Epoch 2:  14%|███████▎                                           | 30/208 [00:03<00:17, 10.08it/s, training_loss=0.215]\u001b[A\n",
      "Epoch 2:  14%|███████▎                                           | 30/208 [00:03<00:17, 10.08it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 2:  14%|███████▎                                           | 30/208 [00:03<00:17, 10.08it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  15%|███████▊                                           | 32/208 [00:03<00:17, 10.07it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  15%|███████▊                                           | 32/208 [00:03<00:17, 10.07it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 2:  15%|███████▊                                           | 32/208 [00:03<00:17, 10.07it/s, training_loss=0.464]\u001b[A\n",
      "Epoch 2:  16%|████████▎                                          | 34/208 [00:03<00:16, 10.35it/s, training_loss=0.464]\u001b[A\n",
      "Epoch 2:  16%|████████▎                                          | 34/208 [00:03<00:16, 10.35it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  16%|████████▎                                          | 34/208 [00:03<00:16, 10.35it/s, training_loss=0.171]\u001b[A\n",
      "Epoch 2:  17%|████████▊                                          | 36/208 [00:03<00:17, 10.11it/s, training_loss=0.171]\u001b[A\n",
      "Epoch 2:  17%|████████▊                                          | 36/208 [00:03<00:17, 10.11it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 2:  17%|████████▊                                          | 36/208 [00:03<00:17, 10.11it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 2:  18%|█████████▎                                         | 38/208 [00:03<00:16, 10.20it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 2:  18%|█████████▎                                         | 38/208 [00:03<00:16, 10.20it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 2:  18%|█████████▎                                         | 38/208 [00:03<00:16, 10.20it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 2:  19%|█████████▊                                         | 40/208 [00:03<00:16, 10.40it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 2:  19%|█████████▊                                         | 40/208 [00:04<00:16, 10.40it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  19%|█████████▊                                         | 40/208 [00:04<00:16, 10.40it/s, training_loss=0.204]\u001b[A\n",
      "Epoch 2:  20%|██████████▎                                        | 42/208 [00:04<00:16, 10.12it/s, training_loss=0.204]\u001b[A\n",
      "Epoch 2:  20%|██████████▎                                        | 42/208 [00:04<00:16, 10.12it/s, training_loss=0.223]\u001b[A\n",
      "Epoch 2:  20%|██████████▎                                        | 42/208 [00:04<00:16, 10.12it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 2:  21%|██████████▊                                        | 44/208 [00:04<00:16, 10.15it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 2:  21%|██████████▊                                        | 44/208 [00:04<00:16, 10.15it/s, training_loss=0.141]\u001b[A\n",
      "Epoch 2:  21%|██████████▊                                        | 44/208 [00:04<00:16, 10.15it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 2:  22%|███████████▎                                       | 46/208 [00:04<00:15, 10.46it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 2:  22%|███████████▎                                       | 46/208 [00:04<00:15, 10.46it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 2:  22%|███████████▎                                       | 46/208 [00:04<00:15, 10.46it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 2:  23%|███████████▊                                       | 48/208 [00:04<00:15, 10.17it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 2:  23%|███████████▊                                       | 48/208 [00:04<00:15, 10.17it/s, training_loss=0.223]\u001b[A\n",
      "Epoch 2:  23%|███████████▊                                       | 48/208 [00:04<00:15, 10.17it/s, training_loss=0.131]\u001b[A\n",
      "Epoch 2:  24%|████████████▎                                      | 50/208 [00:04<00:15, 10.23it/s, training_loss=0.131]\u001b[A\n",
      "Epoch 2:  24%|████████████▎                                      | 50/208 [00:05<00:15, 10.23it/s, training_loss=0.223]\u001b[A\n",
      "Epoch 2:  24%|████████████▎                                      | 50/208 [00:05<00:15, 10.23it/s, training_loss=0.197]\u001b[A\n",
      "Epoch 2:  25%|████████████▊                                      | 52/208 [00:05<00:15, 10.16it/s, training_loss=0.197]\u001b[A\n",
      "Epoch 2:  25%|████████████▊                                      | 52/208 [00:05<00:15, 10.16it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 2:  25%|████████████▊                                      | 52/208 [00:05<00:15, 10.16it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  26%|█████████████▏                                     | 54/208 [00:05<00:14, 10.39it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  26%|█████████████▏                                     | 54/208 [00:05<00:14, 10.39it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 2:  26%|█████████████▏                                     | 54/208 [00:05<00:14, 10.39it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 2:  27%|█████████████▋                                     | 56/208 [00:05<00:14, 10.19it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 2:  27%|█████████████▋                                     | 56/208 [00:05<00:14, 10.19it/s, training_loss=0.215]\u001b[A\n",
      "Epoch 2:  27%|█████████████▋                                     | 56/208 [00:05<00:14, 10.19it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 2:  28%|██████████████▏                                    | 58/208 [00:05<00:14, 10.18it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 2:  28%|██████████████▏                                    | 58/208 [00:05<00:14, 10.18it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 2:  28%|██████████████▏                                    | 58/208 [00:05<00:14, 10.18it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  29%|██████████████▋                                    | 60/208 [00:05<00:14, 10.19it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  29%|██████████████▋                                    | 60/208 [00:06<00:14, 10.19it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 2:  29%|██████████████▋                                    | 60/208 [00:06<00:14, 10.19it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 2:  30%|███████████████▏                                   | 62/208 [00:06<00:14, 10.32it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 2:  30%|███████████████▏                                   | 62/208 [00:06<00:14, 10.32it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  30%|███████████████▏                                   | 62/208 [00:06<00:14, 10.32it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 2:  31%|███████████████▋                                   | 64/208 [00:06<00:13, 10.49it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 2:  31%|███████████████▋                                   | 64/208 [00:06<00:13, 10.49it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 2:  31%|███████████████▋                                   | 64/208 [00:06<00:13, 10.49it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:  32%|████████████████▏                                  | 66/208 [00:06<00:14, 10.14it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:  32%|████████████████▏                                  | 66/208 [00:06<00:14, 10.14it/s, training_loss=0.273]\u001b[A\n",
      "Epoch 2:  32%|████████████████▏                                  | 66/208 [00:06<00:14, 10.14it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  33%|████████████████▋                                  | 68/208 [00:06<00:13, 10.13it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  33%|████████████████▋                                  | 68/208 [00:06<00:13, 10.13it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 2:  33%|████████████████▋                                  | 68/208 [00:06<00:13, 10.13it/s, training_loss=0.186]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▏                                 | 70/208 [00:06<00:13, 10.28it/s, training_loss=0.186]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▏                                 | 70/208 [00:07<00:13, 10.28it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▏                                 | 70/208 [00:07<00:13, 10.28it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▋                                 | 72/208 [00:07<00:13, 10.40it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▋                                 | 72/208 [00:07<00:13, 10.40it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▋                                 | 72/208 [00:07<00:13, 10.40it/s, training_loss=0.211]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▏                                | 74/208 [00:07<00:13, 10.13it/s, training_loss=0.211]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▏                                | 74/208 [00:07<00:13, 10.13it/s, training_loss=0.139]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▏                                | 74/208 [00:07<00:13, 10.13it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  37%|██████████████████▋                                | 76/208 [00:07<00:12, 10.26it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  37%|██████████████████▋                                | 76/208 [00:07<00:12, 10.26it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 2:  37%|██████████████████▋                                | 76/208 [00:07<00:12, 10.26it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▏                               | 78/208 [00:07<00:12, 10.13it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▏                               | 78/208 [00:07<00:12, 10.13it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▏                               | 78/208 [00:07<00:12, 10.13it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▌                               | 80/208 [00:07<00:12, 10.18it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▌                               | 80/208 [00:07<00:12, 10.18it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▌                               | 80/208 [00:08<00:12, 10.18it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 2:  39%|████████████████████                               | 82/208 [00:08<00:12, 10.20it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 2:  39%|████████████████████                               | 82/208 [00:08<00:12, 10.20it/s, training_loss=0.155]\u001b[A\n",
      "Epoch 2:  39%|████████████████████                               | 82/208 [00:08<00:12, 10.20it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▌                              | 84/208 [00:08<00:12, 10.18it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▌                              | 84/208 [00:08<00:12, 10.18it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▌                              | 84/208 [00:08<00:12, 10.18it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 2:  41%|█████████████████████                              | 86/208 [00:08<00:12, 10.16it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 2:  41%|█████████████████████                              | 86/208 [00:08<00:12, 10.16it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 2:  41%|█████████████████████                              | 86/208 [00:08<00:12, 10.16it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▌                             | 88/208 [00:08<00:11, 10.14it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▌                             | 88/208 [00:08<00:11, 10.14it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▌                             | 88/208 [00:08<00:11, 10.14it/s, training_loss=0.115]\u001b[A\n",
      "Epoch 2:  43%|██████████████████████                             | 90/208 [00:08<00:11, 10.34it/s, training_loss=0.115]\u001b[A\n",
      "Epoch 2:  43%|██████████████████████                             | 90/208 [00:08<00:11, 10.34it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 2:  43%|██████████████████████                             | 90/208 [00:09<00:11, 10.34it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 2:  44%|██████████████████████▌                            | 92/208 [00:09<00:11, 10.24it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 2:  44%|██████████████████████▌                            | 92/208 [00:09<00:11, 10.24it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 2:  44%|██████████████████████▌                            | 92/208 [00:09<00:11, 10.24it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 2:  45%|███████████████████████                            | 94/208 [00:09<00:11, 10.19it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 2:  45%|███████████████████████                            | 94/208 [00:09<00:11, 10.19it/s, training_loss=0.186]\u001b[A\n",
      "Epoch 2:  45%|███████████████████████                            | 94/208 [00:09<00:11, 10.19it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▌                           | 96/208 [00:09<00:11, 10.11it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▌                           | 96/208 [00:09<00:11, 10.11it/s, training_loss=0.149]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▌                           | 96/208 [00:09<00:11, 10.11it/s, training_loss=0.145]\u001b[A\n",
      "Epoch 2:  47%|████████████████████████                           | 98/208 [00:09<00:10, 10.09it/s, training_loss=0.145]\u001b[A\n",
      "Epoch 2:  47%|████████████████████████                           | 98/208 [00:09<00:10, 10.09it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 2:  47%|████████████████████████                           | 98/208 [00:09<00:10, 10.09it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████                          | 100/208 [00:09<00:10, 10.29it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████                          | 100/208 [00:09<00:10, 10.29it/s, training_loss=0.207]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████                          | 100/208 [00:10<00:10, 10.29it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 2:  49%|████████████████████████▌                         | 102/208 [00:10<00:10, 10.20it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 2:  49%|████████████████████████▌                         | 102/208 [00:10<00:10, 10.20it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  49%|████████████████████████▌                         | 102/208 [00:10<00:10, 10.20it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:  50%|█████████████████████████                         | 104/208 [00:10<00:10, 10.12it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:  50%|█████████████████████████                         | 104/208 [00:10<00:10, 10.12it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 2:  50%|█████████████████████████                         | 104/208 [00:10<00:10, 10.12it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 2:  51%|█████████████████████████▍                        | 106/208 [00:10<00:09, 10.36it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 2:  51%|█████████████████████████▍                        | 106/208 [00:10<00:09, 10.36it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  51%|█████████████████████████▍                        | 106/208 [00:10<00:09, 10.36it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 2:  52%|█████████████████████████▉                        | 108/208 [00:10<00:09, 10.28it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 2:  52%|█████████████████████████▉                        | 108/208 [00:10<00:09, 10.28it/s, training_loss=0.238]\u001b[A\n",
      "Epoch 2:  52%|█████████████████████████▉                        | 108/208 [00:10<00:09, 10.28it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 2:  53%|██████████████████████████▍                       | 110/208 [00:10<00:09, 10.16it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 2:  53%|██████████████████████████▍                       | 110/208 [00:10<00:09, 10.16it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 2:  53%|██████████████████████████▍                       | 110/208 [00:11<00:09, 10.16it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 2:  54%|██████████████████████████▉                       | 112/208 [00:11<00:09, 10.16it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 2:  54%|██████████████████████████▉                       | 112/208 [00:11<00:09, 10.16it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 2:  54%|██████████████████████████▉                       | 112/208 [00:11<00:09, 10.16it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 2:  55%|███████████████████████████▍                      | 114/208 [00:11<00:09, 10.34it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 2:  55%|███████████████████████████▍                      | 114/208 [00:11<00:09, 10.34it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:  55%|███████████████████████████▍                      | 114/208 [00:11<00:09, 10.34it/s, training_loss=0.147]\u001b[A\n",
      "Epoch 2:  56%|███████████████████████████▉                      | 116/208 [00:11<00:09, 10.21it/s, training_loss=0.147]\u001b[A\n",
      "Epoch 2:  56%|███████████████████████████▉                      | 116/208 [00:11<00:09, 10.21it/s, training_loss=0.206]\u001b[A\n",
      "Epoch 2:  56%|███████████████████████████▉                      | 116/208 [00:11<00:09, 10.21it/s, training_loss=0.208]\u001b[A\n",
      "Epoch 2:  57%|████████████████████████████▎                     | 118/208 [00:11<00:08, 10.14it/s, training_loss=0.208]\u001b[A\n",
      "Epoch 2:  57%|████████████████████████████▎                     | 118/208 [00:11<00:08, 10.14it/s, training_loss=0.157]\u001b[A\n",
      "Epoch 2:  57%|████████████████████████████▎                     | 118/208 [00:11<00:08, 10.14it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 2:  58%|████████████████████████████▊                     | 120/208 [00:11<00:08, 10.11it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 2:  58%|████████████████████████████▊                     | 120/208 [00:11<00:08, 10.11it/s, training_loss=0.178]\u001b[A\n",
      "Epoch 2:  58%|████████████████████████████▊                     | 120/208 [00:12<00:08, 10.11it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 2:  59%|█████████████████████████████▎                    | 122/208 [00:12<00:08, 10.32it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 2:  59%|█████████████████████████████▎                    | 122/208 [00:12<00:08, 10.32it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 2:  59%|█████████████████████████████▎                    | 122/208 [00:12<00:08, 10.32it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 2:  60%|█████████████████████████████▊                    | 124/208 [00:12<00:08, 10.24it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 2:  60%|█████████████████████████████▊                    | 124/208 [00:12<00:08, 10.24it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 2:  60%|█████████████████████████████▊                    | 124/208 [00:12<00:08, 10.24it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████▎                   | 126/208 [00:12<00:08, 10.16it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████▎                   | 126/208 [00:12<00:08, 10.16it/s, training_loss=0.149]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████▎                   | 126/208 [00:12<00:08, 10.16it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  62%|██████████████████████████████▊                   | 128/208 [00:12<00:07, 10.07it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  62%|██████████████████████████████▊                   | 128/208 [00:12<00:07, 10.07it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 2:  62%|██████████████████████████████▊                   | 128/208 [00:12<00:07, 10.07it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▎                  | 130/208 [00:12<00:07, 10.06it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▎                  | 130/208 [00:12<00:07, 10.06it/s, training_loss=0.170]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▎                  | 130/208 [00:12<00:07, 10.06it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 2:  63%|███████████████████████████████▋                  | 132/208 [00:13<00:07, 10.08it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 2:  63%|███████████████████████████████▋                  | 132/208 [00:13<00:07, 10.08it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  63%|███████████████████████████████▋                  | 132/208 [00:13<00:07, 10.08it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 2:  64%|████████████████████████████████▏                 | 134/208 [00:13<00:07, 10.29it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 2:  64%|████████████████████████████████▏                 | 134/208 [00:13<00:07, 10.29it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 2:  64%|████████████████████████████████▏                 | 134/208 [00:13<00:07, 10.29it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  65%|████████████████████████████████▋                 | 136/208 [00:13<00:07, 10.19it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  65%|████████████████████████████████▋                 | 136/208 [00:13<00:07, 10.19it/s, training_loss=0.227]\u001b[A\n",
      "Epoch 2:  65%|████████████████████████████████▋                 | 136/208 [00:13<00:07, 10.19it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 2:  66%|█████████████████████████████████▏                | 138/208 [00:13<00:06, 10.13it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 2:  66%|█████████████████████████████████▏                | 138/208 [00:13<00:06, 10.13it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 2:  66%|█████████████████████████████████▏                | 138/208 [00:13<00:06, 10.13it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:  67%|█████████████████████████████████▋                | 140/208 [00:13<00:06, 10.12it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:  67%|█████████████████████████████████▋                | 140/208 [00:13<00:06, 10.12it/s, training_loss=0.096]\u001b[A\n",
      "Epoch 2:  67%|█████████████████████████████████▋                | 140/208 [00:13<00:06, 10.12it/s, training_loss=0.192]\u001b[A\n",
      "Epoch 2:  68%|██████████████████████████████████▏               | 142/208 [00:13<00:06, 10.27it/s, training_loss=0.192]\u001b[A\n",
      "Epoch 2:  68%|██████████████████████████████████▏               | 142/208 [00:14<00:06, 10.27it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 2:  68%|██████████████████████████████████▏               | 142/208 [00:14<00:06, 10.27it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  69%|██████████████████████████████████▌               | 144/208 [00:14<00:06, 10.23it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  69%|██████████████████████████████████▌               | 144/208 [00:14<00:06, 10.23it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  69%|██████████████████████████████████▌               | 144/208 [00:14<00:06, 10.23it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  70%|███████████████████████████████████               | 146/208 [00:14<00:06, 10.17it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  70%|███████████████████████████████████               | 146/208 [00:14<00:06, 10.17it/s, training_loss=0.466]\u001b[A\n",
      "Epoch 2:  70%|███████████████████████████████████               | 146/208 [00:14<00:06, 10.17it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 2:  71%|███████████████████████████████████▌              | 148/208 [00:14<00:05, 10.12it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 2:  71%|███████████████████████████████████▌              | 148/208 [00:14<00:05, 10.12it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 2:  71%|███████████████████████████████████▌              | 148/208 [00:14<00:05, 10.12it/s, training_loss=0.196]\u001b[A\n",
      "Epoch 2:  72%|████████████████████████████████████              | 150/208 [00:14<00:05, 10.32it/s, training_loss=0.196]\u001b[A\n",
      "Epoch 2:  72%|████████████████████████████████████              | 150/208 [00:14<00:05, 10.32it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 2:  72%|████████████████████████████████████              | 150/208 [00:14<00:05, 10.32it/s, training_loss=0.166]\u001b[A\n",
      "Epoch 2:  73%|████████████████████████████████████▌             | 152/208 [00:14<00:05, 10.20it/s, training_loss=0.166]\u001b[A\n",
      "Epoch 2:  73%|████████████████████████████████████▌             | 152/208 [00:15<00:05, 10.20it/s, training_loss=0.160]\u001b[A\n",
      "Epoch 2:  73%|████████████████████████████████████▌             | 152/208 [00:15<00:05, 10.20it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  74%|█████████████████████████████████████             | 154/208 [00:15<00:05, 10.02it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  74%|█████████████████████████████████████             | 154/208 [00:15<00:05, 10.02it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 2:  74%|█████████████████████████████████████             | 154/208 [00:15<00:05, 10.02it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 2:  75%|█████████████████████████████████████▌            | 156/208 [00:15<00:05, 10.12it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 2:  75%|█████████████████████████████████████▌            | 156/208 [00:15<00:05, 10.12it/s, training_loss=0.158]\u001b[A\n",
      "Epoch 2:  75%|█████████████████████████████████████▌            | 156/208 [00:15<00:05, 10.12it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 2:  76%|█████████████████████████████████████▉            | 158/208 [00:15<00:04, 10.10it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 2:  76%|█████████████████████████████████████▉            | 158/208 [00:15<00:04, 10.10it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 2:  76%|█████████████████████████████████████▉            | 158/208 [00:15<00:04, 10.10it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  77%|██████████████████████████████████████▍           | 160/208 [00:15<00:04, 10.31it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  77%|██████████████████████████████████████▍           | 160/208 [00:15<00:04, 10.31it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  77%|██████████████████████████████████████▍           | 160/208 [00:15<00:04, 10.31it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  78%|██████████████████████████████████████▉           | 162/208 [00:15<00:04, 10.23it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  78%|██████████████████████████████████████▉           | 162/208 [00:16<00:04, 10.23it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 2:  78%|██████████████████████████████████████▉           | 162/208 [00:16<00:04, 10.23it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 2:  79%|███████████████████████████████████████▍          | 164/208 [00:16<00:04, 10.14it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 2:  79%|███████████████████████████████████████▍          | 164/208 [00:16<00:04, 10.14it/s, training_loss=0.241]\u001b[A\n",
      "Epoch 2:  79%|███████████████████████████████████████▍          | 164/208 [00:16<00:04, 10.14it/s, training_loss=0.227]\u001b[A\n",
      "Epoch 2:  80%|███████████████████████████████████████▉          | 166/208 [00:16<00:04, 10.13it/s, training_loss=0.227]\u001b[A\n",
      "Epoch 2:  80%|███████████████████████████████████████▉          | 166/208 [00:16<00:04, 10.13it/s, training_loss=0.214]\u001b[A\n",
      "Epoch 2:  80%|███████████████████████████████████████▉          | 166/208 [00:16<00:04, 10.13it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 2:  81%|████████████████████████████████████████▍         | 168/208 [00:16<00:03, 10.07it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 2:  81%|████████████████████████████████████████▍         | 168/208 [00:16<00:03, 10.07it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 2:  81%|████████████████████████████████████████▍         | 168/208 [00:16<00:03, 10.07it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  82%|████████████████████████████████████████▊         | 170/208 [00:16<00:03, 10.12it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  82%|████████████████████████████████████████▊         | 170/208 [00:16<00:03, 10.12it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 2:  82%|████████████████████████████████████████▊         | 170/208 [00:16<00:03, 10.12it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 2:  83%|█████████████████████████████████████████▎        | 172/208 [00:16<00:03, 10.26it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 2:  83%|█████████████████████████████████████████▎        | 172/208 [00:17<00:03, 10.26it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 2:  83%|█████████████████████████████████████████▎        | 172/208 [00:17<00:03, 10.26it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  84%|█████████████████████████████████████████▊        | 174/208 [00:17<00:03, 10.20it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  84%|█████████████████████████████████████████▊        | 174/208 [00:17<00:03, 10.20it/s, training_loss=0.203]\u001b[A\n",
      "Epoch 2:  84%|█████████████████████████████████████████▊        | 174/208 [00:17<00:03, 10.20it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 2:  85%|██████████████████████████████████████████▎       | 176/208 [00:17<00:03, 10.38it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 2:  85%|██████████████████████████████████████████▎       | 176/208 [00:17<00:03, 10.38it/s, training_loss=0.174]\u001b[A\n",
      "Epoch 2:  85%|██████████████████████████████████████████▎       | 176/208 [00:17<00:03, 10.38it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:  86%|██████████████████████████████████████████▊       | 178/208 [00:17<00:02, 10.24it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:  86%|██████████████████████████████████████████▊       | 178/208 [00:17<00:02, 10.24it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  86%|██████████████████████████████████████████▊       | 178/208 [00:17<00:02, 10.24it/s, training_loss=0.152]\u001b[A\n",
      "Epoch 2:  87%|███████████████████████████████████████████▎      | 180/208 [00:17<00:02, 10.18it/s, training_loss=0.152]\u001b[A\n",
      "Epoch 2:  87%|███████████████████████████████████████████▎      | 180/208 [00:17<00:02, 10.18it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 2:  87%|███████████████████████████████████████████▎      | 180/208 [00:17<00:02, 10.18it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  88%|███████████████████████████████████████████▊      | 182/208 [00:17<00:02, 10.12it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  88%|███████████████████████████████████████████▊      | 182/208 [00:18<00:02, 10.12it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 2:  88%|███████████████████████████████████████████▊      | 182/208 [00:18<00:02, 10.12it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████████████████████▏     | 184/208 [00:18<00:02, 10.10it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████████████████████▏     | 184/208 [00:18<00:02, 10.10it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████████████████████▏     | 184/208 [00:18<00:02, 10.10it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 2:  89%|████████████████████████████████████████████▋     | 186/208 [00:18<00:02, 10.09it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 2:  89%|████████████████████████████████████████████▋     | 186/208 [00:18<00:02, 10.09it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 2:  89%|████████████████████████████████████████████▋     | 186/208 [00:18<00:02, 10.09it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 2:  90%|█████████████████████████████████████████████▏    | 188/208 [00:18<00:01, 10.28it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 2:  90%|█████████████████████████████████████████████▏    | 188/208 [00:18<00:01, 10.28it/s, training_loss=0.221]\u001b[A\n",
      "Epoch 2:  90%|█████████████████████████████████████████████▏    | 188/208 [00:18<00:01, 10.28it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 2:  91%|█████████████████████████████████████████████▋    | 190/208 [00:18<00:01, 10.18it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 2:  91%|█████████████████████████████████████████████▋    | 190/208 [00:18<00:01, 10.18it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 2:  91%|█████████████████████████████████████████████▋    | 190/208 [00:18<00:01, 10.18it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  92%|██████████████████████████████████████████████▏   | 192/208 [00:18<00:01, 10.13it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  92%|██████████████████████████████████████████████▏   | 192/208 [00:19<00:01, 10.13it/s, training_loss=0.188]\u001b[A\n",
      "Epoch 2:  92%|██████████████████████████████████████████████▏   | 192/208 [00:19<00:01, 10.13it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 2:  93%|██████████████████████████████████████████████▋   | 194/208 [00:19<00:01, 10.08it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 2:  93%|██████████████████████████████████████████████▋   | 194/208 [00:19<00:01, 10.08it/s, training_loss=0.144]\u001b[A\n",
      "Epoch 2:  93%|██████████████████████████████████████████████▋   | 194/208 [00:19<00:01, 10.08it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 2:  94%|███████████████████████████████████████████████   | 196/208 [00:19<00:01, 10.30it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 2:  94%|███████████████████████████████████████████████   | 196/208 [00:19<00:01, 10.30it/s, training_loss=0.224]\u001b[A\n",
      "Epoch 2:  94%|███████████████████████████████████████████████   | 196/208 [00:19<00:01, 10.30it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████████████████████▌  | 198/208 [00:19<00:00, 10.23it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████████████████████▌  | 198/208 [00:19<00:00, 10.23it/s, training_loss=0.204]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████████████████████▌  | 198/208 [00:19<00:00, 10.23it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 2:  96%|████████████████████████████████████████████████  | 200/208 [00:19<00:00, 10.05it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 2:  96%|████████████████████████████████████████████████  | 200/208 [00:19<00:00, 10.05it/s, training_loss=0.146]\u001b[A\n",
      "Epoch 2:  96%|████████████████████████████████████████████████  | 200/208 [00:19<00:00, 10.05it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  97%|████████████████████████████████████████████████▌ | 202/208 [00:19<00:00, 10.13it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  97%|████████████████████████████████████████████████▌ | 202/208 [00:19<00:00, 10.13it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  97%|████████████████████████████████████████████████▌ | 202/208 [00:20<00:00, 10.13it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 2:  98%|█████████████████████████████████████████████████ | 204/208 [00:20<00:00, 10.07it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 2:  98%|█████████████████████████████████████████████████ | 204/208 [00:20<00:00, 10.07it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 2:  98%|█████████████████████████████████████████████████ | 204/208 [00:20<00:00, 10.07it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  99%|█████████████████████████████████████████████████▌| 206/208 [00:20<00:00, 10.07it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  99%|█████████████████████████████████████████████████▌| 206/208 [00:20<00:00, 10.07it/s, training_loss=0.145]\u001b[A\n",
      "Epoch 2:  99%|█████████████████████████████████████████████████▌| 206/208 [00:20<00:00, 10.07it/s, training_loss=0.145]\u001b[A\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████| 208/208 [00:20<00:00, 10.65it/s, training_loss=0.145]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:43<00:00, 21.79s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                 | 0/104 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                            | 0/104 [00:00<?, ?it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:   1%|▌                                                   | 1/104 [00:00<00:44,  2.30it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:   1%|▌                                                   | 1/104 [00:00<00:44,  2.30it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   2%|█                                                   | 2/104 [00:00<00:28,  3.61it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   2%|█                                                   | 2/104 [00:00<00:28,  3.61it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:   3%|█▌                                                  | 3/104 [00:00<00:23,  4.38it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:   3%|█▌                                                  | 3/104 [00:00<00:23,  4.38it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:   4%|██                                                  | 4/104 [00:00<00:20,  4.91it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:   4%|██                                                  | 4/104 [00:01<00:20,  4.91it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:   5%|██▌                                                 | 5/104 [00:01<00:18,  5.24it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:   5%|██▌                                                 | 5/104 [00:01<00:18,  5.24it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:   6%|███                                                 | 6/104 [00:01<00:17,  5.48it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:   6%|███                                                 | 6/104 [00:01<00:17,  5.48it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:   7%|███▌                                                | 7/104 [00:01<00:17,  5.63it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:   7%|███▌                                                | 7/104 [00:01<00:17,  5.63it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:   8%|████                                                | 8/104 [00:01<00:16,  5.75it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:   8%|████                                                | 8/104 [00:01<00:16,  5.75it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   9%|████▌                                               | 9/104 [00:01<00:16,  5.82it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   9%|████▌                                               | 9/104 [00:01<00:16,  5.82it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  10%|████▉                                              | 10/104 [00:01<00:16,  5.69it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  10%|████▉                                              | 10/104 [00:02<00:16,  5.69it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  11%|█████▍                                             | 11/104 [00:02<00:16,  5.74it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  11%|█████▍                                             | 11/104 [00:02<00:16,  5.74it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  12%|█████▉                                             | 12/104 [00:02<00:15,  5.87it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  12%|█████▉                                             | 12/104 [00:02<00:15,  5.87it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  12%|██████▍                                            | 13/104 [00:02<00:15,  5.91it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  12%|██████▍                                            | 13/104 [00:02<00:15,  5.91it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  13%|██████▊                                            | 14/104 [00:02<00:15,  5.92it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  13%|██████▊                                            | 14/104 [00:02<00:15,  5.92it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  14%|███████▎                                           | 15/104 [00:02<00:15,  5.84it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  14%|███████▎                                           | 15/104 [00:02<00:15,  5.84it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  15%|███████▊                                           | 16/104 [00:02<00:14,  5.99it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  15%|███████▊                                           | 16/104 [00:03<00:14,  5.99it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  16%|████████▎                                          | 17/104 [00:03<00:14,  5.82it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  16%|████████▎                                          | 17/104 [00:03<00:14,  5.82it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  17%|████████▊                                          | 18/104 [00:03<00:14,  5.88it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  17%|████████▊                                          | 18/104 [00:03<00:14,  5.88it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  18%|█████████▎                                         | 19/104 [00:03<00:14,  5.90it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  18%|█████████▎                                         | 19/104 [00:03<00:14,  5.90it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  19%|█████████▊                                         | 20/104 [00:03<00:14,  5.94it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  19%|█████████▊                                         | 20/104 [00:03<00:14,  5.94it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  20%|██████████▎                                        | 21/104 [00:03<00:14,  5.87it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  20%|██████████▎                                        | 21/104 [00:03<00:14,  5.87it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  21%|██████████▊                                        | 22/104 [00:03<00:13,  5.91it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  21%|██████████▊                                        | 22/104 [00:04<00:13,  5.91it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  22%|███████████▎                                       | 23/104 [00:04<00:13,  6.02it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  22%|███████████▎                                       | 23/104 [00:04<00:13,  6.02it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  23%|███████████▊                                       | 24/104 [00:04<00:13,  6.01it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  23%|███████████▊                                       | 24/104 [00:04<00:13,  6.01it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  24%|████████████▎                                      | 25/104 [00:04<00:13,  5.99it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  24%|████████████▎                                      | 25/104 [00:04<00:13,  5.99it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  25%|████████████▊                                      | 26/104 [00:04<00:13,  5.98it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  25%|████████████▊                                      | 26/104 [00:04<00:13,  5.98it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  26%|█████████████▏                                     | 27/104 [00:04<00:13,  5.85it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  26%|█████████████▏                                     | 27/104 [00:04<00:13,  5.85it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  27%|█████████████▋                                     | 28/104 [00:04<00:12,  5.88it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  27%|█████████████▋                                     | 28/104 [00:05<00:12,  5.88it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  28%|██████████████▏                                    | 29/104 [00:05<00:12,  5.91it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  28%|██████████████▏                                    | 29/104 [00:05<00:12,  5.91it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  29%|██████████████▋                                    | 30/104 [00:05<00:12,  5.94it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  29%|██████████████▋                                    | 30/104 [00:05<00:12,  5.94it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  30%|███████████████▏                                   | 31/104 [00:05<00:12,  5.93it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  30%|███████████████▏                                   | 31/104 [00:05<00:12,  5.93it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  31%|███████████████▋                                   | 32/104 [00:05<00:12,  5.97it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  31%|███████████████▋                                   | 32/104 [00:05<00:12,  5.97it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  32%|████████████████▏                                  | 33/104 [00:05<00:12,  5.83it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  32%|████████████████▏                                  | 33/104 [00:05<00:12,  5.83it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:  33%|████████████████▋                                  | 34/104 [00:05<00:11,  6.02it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:  33%|████████████████▋                                  | 34/104 [00:06<00:11,  6.02it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▏                                 | 35/104 [00:06<00:11,  5.83it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▏                                 | 35/104 [00:06<00:11,  5.83it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▋                                 | 36/104 [00:06<00:11,  5.88it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▋                                 | 36/104 [00:06<00:11,  5.88it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▏                                | 37/104 [00:06<00:11,  5.93it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▏                                | 37/104 [00:06<00:11,  5.93it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  37%|██████████████████▋                                | 38/104 [00:06<00:11,  5.95it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  37%|██████████████████▋                                | 38/104 [00:06<00:11,  5.95it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▏                               | 39/104 [00:06<00:10,  5.98it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▏                               | 39/104 [00:07<00:10,  5.98it/s, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▌                               | 40/104 [00:07<00:10,  5.95it/s, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▌                               | 40/104 [00:07<00:10,  5.95it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  39%|████████████████████                               | 41/104 [00:07<00:10,  5.97it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  39%|████████████████████                               | 41/104 [00:07<00:10,  5.97it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▌                              | 42/104 [00:07<00:10,  5.97it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▌                              | 42/104 [00:07<00:10,  5.97it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                              | 43/104 [00:07<00:10,  5.79it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                              | 43/104 [00:07<00:10,  5.79it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▌                             | 44/104 [00:07<00:10,  5.88it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▌                             | 44/104 [00:07<00:10,  5.88it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████                             | 45/104 [00:07<00:09,  5.93it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████                             | 45/104 [00:08<00:09,  5.93it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▌                            | 46/104 [00:08<00:09,  5.92it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▌                            | 46/104 [00:08<00:09,  5.92it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  45%|███████████████████████                            | 47/104 [00:08<00:09,  5.94it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  45%|███████████████████████                            | 47/104 [00:08<00:09,  5.94it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▌                           | 48/104 [00:08<00:09,  5.97it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▌                           | 48/104 [00:08<00:09,  5.97it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  47%|████████████████████████                           | 49/104 [00:08<00:09,  5.80it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  47%|████████████████████████                           | 49/104 [00:08<00:09,  5.80it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████▌                          | 50/104 [00:08<00:09,  5.86it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████▌                          | 50/104 [00:08<00:09,  5.86it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████                          | 51/104 [00:08<00:08,  5.91it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████                          | 51/104 [00:09<00:08,  5.91it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████▌                         | 52/104 [00:09<00:08,  5.91it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████▌                         | 52/104 [00:09<00:08,  5.91it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████▉                         | 53/104 [00:09<00:08,  5.94it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████▉                         | 53/104 [00:09<00:08,  5.94it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████▍                        | 54/104 [00:09<00:08,  5.79it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████▍                        | 54/104 [00:09<00:08,  5.79it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████▉                        | 55/104 [00:09<00:08,  5.85it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████▉                        | 55/104 [00:09<00:08,  5.85it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▍                       | 56/104 [00:09<00:08,  5.80it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▍                       | 56/104 [00:09<00:08,  5.80it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████▉                       | 57/104 [00:09<00:07,  5.95it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████▉                       | 57/104 [00:10<00:07,  5.95it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████▍                      | 58/104 [00:10<00:07,  5.95it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████▍                      | 58/104 [00:10<00:07,  5.95it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████▉                      | 59/104 [00:10<00:07,  5.80it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████▉                      | 59/104 [00:10<00:07,  5.80it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▍                     | 60/104 [00:10<00:07,  5.86it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▍                     | 60/104 [00:10<00:07,  5.86it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  59%|█████████████████████████████▉                     | 61/104 [00:10<00:07,  5.90it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  59%|█████████████████████████████▉                     | 61/104 [00:10<00:07,  5.90it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████▍                    | 62/104 [00:10<00:07,  5.85it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████▍                    | 62/104 [00:10<00:07,  5.85it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  61%|██████████████████████████████▉                    | 63/104 [00:10<00:06,  5.96it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  61%|██████████████████████████████▉                    | 63/104 [00:11<00:06,  5.96it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████▍                   | 64/104 [00:11<00:06,  5.97it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████▍                   | 64/104 [00:11<00:06,  5.97it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████▉                   | 65/104 [00:11<00:06,  5.98it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████▉                   | 65/104 [00:11<00:06,  5.98it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  63%|████████████████████████████████▎                  | 66/104 [00:11<00:06,  5.81it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  63%|████████████████████████████████▎                  | 66/104 [00:11<00:06,  5.81it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████▊                  | 67/104 [00:11<00:06,  5.87it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████▊                  | 67/104 [00:11<00:06,  5.87it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▎                 | 68/104 [00:11<00:06,  5.89it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▎                 | 68/104 [00:11<00:06,  5.89it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████▊                 | 69/104 [00:11<00:06,  5.80it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████▊                 | 69/104 [00:12<00:06,  5.80it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  67%|██████████████████████████████████▎                | 70/104 [00:12<00:05,  5.82it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  67%|██████████████████████████████████▎                | 70/104 [00:12<00:05,  5.82it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████████████████▊                | 71/104 [00:12<00:05,  5.84it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████████████████▊                | 71/104 [00:12<00:05,  5.84it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████▎               | 72/104 [00:12<00:05,  5.92it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████▎               | 72/104 [00:12<00:05,  5.92it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████████████████▊               | 73/104 [00:12<00:05,  5.84it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████████████████▊               | 73/104 [00:12<00:05,  5.84it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  71%|████████████████████████████████████▎              | 74/104 [00:12<00:05,  5.85it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  71%|████████████████████████████████████▎              | 74/104 [00:12<00:05,  5.85it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████▊              | 75/104 [00:12<00:04,  6.02it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████▊              | 75/104 [00:13<00:04,  6.02it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▎             | 76/104 [00:13<00:04,  5.85it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▎             | 76/104 [00:13<00:04,  5.85it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████▊             | 77/104 [00:13<00:04,  5.88it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████▊             | 77/104 [00:13<00:04,  5.88it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████▎            | 78/104 [00:13<00:04,  5.91it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████▎            | 78/104 [00:13<00:04,  5.91it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████▋            | 79/104 [00:13<00:04,  5.94it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████▋            | 79/104 [00:13<00:04,  5.94it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████▏           | 80/104 [00:13<00:04,  5.81it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████▏           | 80/104 [00:13<00:04,  5.81it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  78%|███████████████████████████████████████▋           | 81/104 [00:13<00:03,  5.85it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  78%|███████████████████████████████████████▋           | 81/104 [00:14<00:03,  5.85it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████▏          | 82/104 [00:14<00:03,  5.88it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████▏          | 82/104 [00:14<00:03,  5.88it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  80%|████████████████████████████████████████▋          | 83/104 [00:14<00:03,  5.91it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  80%|████████████████████████████████████████▋          | 83/104 [00:14<00:03,  5.91it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  81%|█████████████████████████████████████████▏         | 84/104 [00:14<00:03,  5.92it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  81%|█████████████████████████████████████████▏         | 84/104 [00:14<00:03,  5.92it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  82%|█████████████████████████████████████████▋         | 85/104 [00:14<00:03,  5.96it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  82%|█████████████████████████████████████████▋         | 85/104 [00:14<00:03,  5.96it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  83%|██████████████████████████████████████████▏        | 86/104 [00:14<00:03,  5.82it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  83%|██████████████████████████████████████████▏        | 86/104 [00:15<00:03,  5.82it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  84%|██████████████████████████████████████████▋        | 87/104 [00:15<00:02,  5.85it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  84%|██████████████████████████████████████████▋        | 87/104 [00:15<00:02,  5.85it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  85%|███████████████████████████████████████████▏       | 88/104 [00:15<00:02,  5.80it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  85%|███████████████████████████████████████████▏       | 88/104 [00:15<00:02,  5.80it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  86%|███████████████████████████████████████████▋       | 89/104 [00:15<00:02,  5.94it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  86%|███████████████████████████████████████████▋       | 89/104 [00:15<00:02,  5.94it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  87%|████████████████████████████████████████████▏      | 90/104 [00:15<00:02,  5.96it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  87%|████████████████████████████████████████████▏      | 90/104 [00:15<00:02,  5.96it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████▋      | 91/104 [00:15<00:02,  5.81it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████▋      | 91/104 [00:15<00:02,  5.81it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  88%|█████████████████████████████████████████████      | 92/104 [00:15<00:02,  5.87it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  88%|█████████████████████████████████████████████      | 92/104 [00:16<00:02,  5.87it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  89%|█████████████████████████████████████████████▌     | 93/104 [00:16<00:01,  5.89it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  89%|█████████████████████████████████████████████▌     | 93/104 [00:16<00:01,  5.89it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  90%|██████████████████████████████████████████████     | 94/104 [00:16<00:01,  5.80it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  90%|██████████████████████████████████████████████     | 94/104 [00:16<00:01,  5.80it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  91%|██████████████████████████████████████████████▌    | 95/104 [00:16<00:01,  5.96it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  91%|██████████████████████████████████████████████▌    | 95/104 [00:16<00:01,  5.96it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████    | 96/104 [00:16<00:01,  5.80it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████    | 96/104 [00:16<00:01,  5.80it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████▌   | 97/104 [00:16<00:01,  5.86it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████▌   | 97/104 [00:16<00:01,  5.86it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  94%|████████████████████████████████████████████████   | 98/104 [00:16<00:01,  5.90it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  94%|████████████████████████████████████████████████   | 98/104 [00:17<00:01,  5.90it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████▌  | 99/104 [00:17<00:00,  5.94it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████▌  | 99/104 [00:17<00:00,  5.94it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████  | 100/104 [00:17<00:00,  5.80it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████  | 100/104 [00:17<00:00,  5.80it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▌ | 101/104 [00:17<00:00,  5.83it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▌ | 101/104 [00:17<00:00,  5.83it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  98%|█████████████████████████████████████████████████ | 102/104 [00:17<00:00,  5.89it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  98%|█████████████████████████████████████████████████ | 102/104 [00:17<00:00,  5.89it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▌| 103/104 [00:17<00:00,  5.82it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▌| 103/104 [00:17<00:00,  5.82it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████| 104/104 [00:17<00:00,  6.33it/s, training_loss=0.327]\u001b[A\n",
      "Epoch Progress:  50%|██████████████████████████████████                                  | 1/2 [00:17<00:17, 17.87s/it]\u001b[A\n",
      "Epoch 2:   0%|                                                                                 | 0/104 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:   0%|                                                            | 0/104 [00:00<?, ?it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 2:   1%|▌                                                   | 1/104 [00:00<00:17,  5.91it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 2:   1%|▌                                                   | 1/104 [00:00<00:17,  5.91it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 2:   2%|█                                                   | 2/104 [00:00<00:16,  6.22it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 2:   2%|█                                                   | 2/104 [00:00<00:16,  6.22it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 2:   3%|█▌                                                  | 3/104 [00:00<00:17,  5.87it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 2:   3%|█▌                                                  | 3/104 [00:00<00:17,  5.87it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:   4%|██                                                  | 4/104 [00:00<00:16,  5.90it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:   4%|██                                                  | 4/104 [00:00<00:16,  5.90it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:   5%|██▌                                                 | 5/104 [00:00<00:16,  5.93it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:   5%|██▌                                                 | 5/104 [00:01<00:16,  5.93it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 2:   6%|███                                                 | 6/104 [00:01<00:16,  5.95it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 2:   6%|███                                                 | 6/104 [00:01<00:16,  5.95it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 2:   7%|███▌                                                | 7/104 [00:01<00:16,  5.95it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 2:   7%|███▌                                                | 7/104 [00:01<00:16,  5.95it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 2:   8%|████                                                | 8/104 [00:01<00:16,  5.80it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 2:   8%|████                                                | 8/104 [00:01<00:16,  5.80it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:   9%|████▌                                               | 9/104 [00:01<00:16,  5.85it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:   9%|████▌                                               | 9/104 [00:01<00:16,  5.85it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  10%|████▉                                              | 10/104 [00:01<00:15,  5.89it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  10%|████▉                                              | 10/104 [00:01<00:15,  5.89it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 2:  11%|█████▍                                             | 11/104 [00:01<00:16,  5.81it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 2:  11%|█████▍                                             | 11/104 [00:02<00:16,  5.81it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 2:  12%|█████▉                                             | 12/104 [00:02<00:15,  5.82it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 2:  12%|█████▉                                             | 12/104 [00:02<00:15,  5.82it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 2:  12%|██████▍                                            | 13/104 [00:02<00:15,  5.86it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 2:  12%|██████▍                                            | 13/104 [00:02<00:15,  5.86it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 2:  13%|██████▊                                            | 14/104 [00:02<00:15,  5.90it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 2:  13%|██████▊                                            | 14/104 [00:02<00:15,  5.90it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  14%|███████▎                                           | 15/104 [00:02<00:15,  5.93it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  14%|███████▎                                           | 15/104 [00:02<00:15,  5.93it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 2:  15%|███████▊                                           | 16/104 [00:02<00:14,  5.94it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 2:  15%|███████▊                                           | 16/104 [00:02<00:14,  5.94it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  16%|████████▎                                          | 17/104 [00:02<00:15,  5.79it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  16%|████████▎                                          | 17/104 [00:03<00:15,  5.79it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  17%|████████▊                                          | 18/104 [00:03<00:14,  5.87it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  17%|████████▊                                          | 18/104 [00:03<00:14,  5.87it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  18%|█████████▎                                         | 19/104 [00:03<00:14,  5.88it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  18%|█████████▎                                         | 19/104 [00:03<00:14,  5.88it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  19%|█████████▊                                         | 20/104 [00:03<00:14,  5.91it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  19%|█████████▊                                         | 20/104 [00:03<00:14,  5.91it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  20%|██████████▎                                        | 21/104 [00:03<00:14,  5.77it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  20%|██████████▎                                        | 21/104 [00:03<00:14,  5.77it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 2:  21%|██████████▊                                        | 22/104 [00:03<00:14,  5.79it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 2:  21%|██████████▊                                        | 22/104 [00:03<00:14,  5.79it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  22%|███████████▎                                       | 23/104 [00:03<00:13,  5.89it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  22%|███████████▎                                       | 23/104 [00:04<00:13,  5.89it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 2:  23%|███████████▊                                       | 24/104 [00:04<00:13,  5.80it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 2:  23%|███████████▊                                       | 24/104 [00:04<00:13,  5.80it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 2:  24%|████████████▎                                      | 25/104 [00:04<00:13,  5.82it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 2:  24%|████████████▎                                      | 25/104 [00:04<00:13,  5.82it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 2:  25%|████████████▊                                      | 26/104 [00:04<00:13,  5.86it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 2:  25%|████████████▊                                      | 26/104 [00:04<00:13,  5.86it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 2:  26%|█████████████▏                                     | 27/104 [00:04<00:13,  5.90it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 2:  26%|█████████████▏                                     | 27/104 [00:04<00:13,  5.90it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  27%|█████████████▋                                     | 28/104 [00:04<00:12,  5.93it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  27%|█████████████▋                                     | 28/104 [00:04<00:12,  5.93it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 2:  28%|██████████████▏                                    | 29/104 [00:04<00:12,  5.80it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 2:  28%|██████████████▏                                    | 29/104 [00:05<00:12,  5.80it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:  29%|██████████████▋                                    | 30/104 [00:05<00:12,  5.83it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:  29%|██████████████▋                                    | 30/104 [00:05<00:12,  5.83it/s, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  30%|███████████████▏                                   | 31/104 [00:05<00:12,  5.88it/s, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  30%|███████████████▏                                   | 31/104 [00:05<00:12,  5.88it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  31%|███████████████▋                                   | 32/104 [00:05<00:12,  5.91it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  31%|███████████████▋                                   | 32/104 [00:05<00:12,  5.91it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  32%|████████████████▏                                  | 33/104 [00:05<00:12,  5.77it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  32%|████████████████▏                                  | 33/104 [00:05<00:12,  5.77it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  33%|████████████████▋                                  | 34/104 [00:05<00:12,  5.83it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  33%|████████████████▋                                  | 34/104 [00:05<00:12,  5.83it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▏                                 | 35/104 [00:05<00:11,  5.89it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▏                                 | 35/104 [00:06<00:11,  5.89it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▋                                 | 36/104 [00:06<00:11,  5.91it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▋                                 | 36/104 [00:06<00:11,  5.91it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▏                                | 37/104 [00:06<00:11,  5.93it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▏                                | 37/104 [00:06<00:11,  5.93it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 2:  37%|██████████████████▋                                | 38/104 [00:06<00:11,  5.94it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 2:  37%|██████████████████▋                                | 38/104 [00:06<00:11,  5.94it/s, training_loss=0.252]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▏                               | 39/104 [00:06<00:11,  5.79it/s, training_loss=0.252]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▏                               | 39/104 [00:06<00:11,  5.79it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▌                               | 40/104 [00:06<00:10,  5.85it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▌                               | 40/104 [00:06<00:10,  5.85it/s, training_loss=0.261]\u001b[A\n",
      "Epoch 2:  39%|████████████████████                               | 41/104 [00:06<00:10,  5.77it/s, training_loss=0.261]\u001b[A\n",
      "Epoch 2:  39%|████████████████████                               | 41/104 [00:07<00:10,  5.77it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▌                              | 42/104 [00:07<00:10,  5.94it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▌                              | 42/104 [00:07<00:10,  5.94it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 2:  41%|█████████████████████                              | 43/104 [00:07<00:10,  5.83it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 2:  41%|█████████████████████                              | 43/104 [00:07<00:10,  5.83it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▌                             | 44/104 [00:07<00:10,  5.85it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▌                             | 44/104 [00:07<00:10,  5.85it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  43%|██████████████████████                             | 45/104 [00:07<00:10,  5.89it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 2:  43%|██████████████████████                             | 45/104 [00:07<00:10,  5.89it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  44%|██████████████████████▌                            | 46/104 [00:07<00:09,  5.92it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  44%|██████████████████████▌                            | 46/104 [00:07<00:09,  5.92it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  45%|███████████████████████                            | 47/104 [00:08<00:09,  5.77it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  45%|███████████████████████                            | 47/104 [00:08<00:09,  5.77it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▌                           | 48/104 [00:08<00:09,  5.84it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▌                           | 48/104 [00:08<00:09,  5.84it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  47%|████████████████████████                           | 49/104 [00:08<00:09,  5.86it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  47%|████████████████████████                           | 49/104 [00:08<00:09,  5.86it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████▌                          | 50/104 [00:08<00:09,  5.92it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████▌                          | 50/104 [00:08<00:09,  5.92it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 2:  49%|█████████████████████████                          | 51/104 [00:08<00:08,  5.93it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 2:  49%|█████████████████████████                          | 51/104 [00:08<00:08,  5.93it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  50%|█████████████████████████▌                         | 52/104 [00:08<00:08,  5.78it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  50%|█████████████████████████▌                         | 52/104 [00:09<00:08,  5.78it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 2:  51%|█████████████████████████▉                         | 53/104 [00:09<00:08,  5.85it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 2:  51%|█████████████████████████▉                         | 53/104 [00:09<00:08,  5.85it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:  52%|██████████████████████████▍                        | 54/104 [00:09<00:08,  5.88it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 2:  52%|██████████████████████████▍                        | 54/104 [00:09<00:08,  5.88it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  53%|██████████████████████████▉                        | 55/104 [00:09<00:08,  5.91it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  53%|██████████████████████████▉                        | 55/104 [00:09<00:08,  5.91it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▍                       | 56/104 [00:09<00:08,  5.78it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▍                       | 56/104 [00:09<00:08,  5.78it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 2:  55%|███████████████████████████▉                       | 57/104 [00:09<00:08,  5.84it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 2:  55%|███████████████████████████▉                       | 57/104 [00:09<00:08,  5.84it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 2:  56%|████████████████████████████▍                      | 58/104 [00:09<00:07,  5.86it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 2:  56%|████████████████████████████▍                      | 58/104 [00:10<00:07,  5.86it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  57%|████████████████████████████▉                      | 59/104 [00:10<00:07,  5.77it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  57%|████████████████████████████▉                      | 59/104 [00:10<00:07,  5.77it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▍                     | 60/104 [00:10<00:07,  5.82it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▍                     | 60/104 [00:10<00:07,  5.82it/s, training_loss=0.250]\u001b[A\n",
      "Epoch 2:  59%|█████████████████████████████▉                     | 61/104 [00:10<00:07,  5.87it/s, training_loss=0.250]\u001b[A\n",
      "Epoch 2:  59%|█████████████████████████████▉                     | 61/104 [00:10<00:07,  5.87it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 2:  60%|██████████████████████████████▍                    | 62/104 [00:10<00:07,  5.90it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 2:  60%|██████████████████████████████▍                    | 62/104 [00:10<00:07,  5.90it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████▉                    | 63/104 [00:10<00:06,  5.92it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████▉                    | 63/104 [00:10<00:06,  5.92it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▍                   | 64/104 [00:10<00:06,  5.80it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▍                   | 64/104 [00:11<00:06,  5.80it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▉                   | 65/104 [00:11<00:06,  5.84it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▉                   | 65/104 [00:11<00:06,  5.84it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 2:  63%|████████████████████████████████▎                  | 66/104 [00:11<00:06,  5.87it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 2:  63%|████████████████████████████████▎                  | 66/104 [00:11<00:06,  5.87it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 2:  64%|████████████████████████████████▊                  | 67/104 [00:11<00:06,  5.75it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 2:  64%|████████████████████████████████▊                  | 67/104 [00:11<00:06,  5.75it/s, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  65%|█████████████████████████████████▎                 | 68/104 [00:11<00:06,  5.75it/s, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  65%|█████████████████████████████████▎                 | 68/104 [00:11<00:06,  5.75it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  66%|█████████████████████████████████▊                 | 69/104 [00:11<00:05,  5.89it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  66%|█████████████████████████████████▊                 | 69/104 [00:11<00:05,  5.89it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 2:  67%|██████████████████████████████████▎                | 70/104 [00:11<00:05,  5.77it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 2:  67%|██████████████████████████████████▎                | 70/104 [00:12<00:05,  5.77it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 2:  68%|██████████████████████████████████▊                | 71/104 [00:12<00:05,  5.82it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 2:  68%|██████████████████████████████████▊                | 71/104 [00:12<00:05,  5.82it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 2:  69%|███████████████████████████████████▎               | 72/104 [00:12<00:05,  5.86it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 2:  69%|███████████████████████████████████▎               | 72/104 [00:12<00:05,  5.86it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  70%|███████████████████████████████████▊               | 73/104 [00:12<00:05,  5.90it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  70%|███████████████████████████████████▊               | 73/104 [00:12<00:05,  5.90it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  71%|████████████████████████████████████▎              | 74/104 [00:12<00:05,  5.77it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  71%|████████████████████████████████████▎              | 74/104 [00:12<00:05,  5.77it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 2:  72%|████████████████████████████████████▊              | 75/104 [00:12<00:05,  5.74it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 2:  72%|████████████████████████████████████▊              | 75/104 [00:12<00:05,  5.74it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 2:  73%|█████████████████████████████████████▎             | 76/104 [00:12<00:04,  5.91it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 2:  73%|█████████████████████████████████████▎             | 76/104 [00:13<00:04,  5.91it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  74%|█████████████████████████████████████▊             | 77/104 [00:13<00:04,  5.76it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  74%|█████████████████████████████████████▊             | 77/104 [00:13<00:04,  5.76it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  75%|██████████████████████████████████████▎            | 78/104 [00:13<00:04,  5.77it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 2:  75%|██████████████████████████████████████▎            | 78/104 [00:13<00:04,  5.77it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 2:  76%|██████████████████████████████████████▋            | 79/104 [00:13<00:04,  5.88it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 2:  76%|██████████████████████████████████████▋            | 79/104 [00:13<00:04,  5.88it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:  77%|███████████████████████████████████████▏           | 80/104 [00:13<00:04,  5.91it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:  77%|███████████████████████████████████████▏           | 80/104 [00:13<00:04,  5.91it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 2:  78%|███████████████████████████████████████▋           | 81/104 [00:13<00:03,  5.78it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 2:  78%|███████████████████████████████████████▋           | 81/104 [00:14<00:03,  5.78it/s, training_loss=0.222]\u001b[A\n",
      "Epoch 2:  79%|████████████████████████████████████████▏          | 82/104 [00:14<00:03,  5.83it/s, training_loss=0.222]\u001b[A\n",
      "Epoch 2:  79%|████████████████████████████████████████▏          | 82/104 [00:14<00:03,  5.83it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 2:  80%|████████████████████████████████████████▋          | 83/104 [00:14<00:03,  5.77it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 2:  80%|████████████████████████████████████████▋          | 83/104 [00:14<00:03,  5.77it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  81%|█████████████████████████████████████████▏         | 84/104 [00:14<00:03,  5.79it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  81%|█████████████████████████████████████████▏         | 84/104 [00:14<00:03,  5.79it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 2:  82%|█████████████████████████████████████████▋         | 85/104 [00:14<00:03,  5.85it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 2:  82%|█████████████████████████████████████████▋         | 85/104 [00:14<00:03,  5.85it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  83%|██████████████████████████████████████████▏        | 86/104 [00:14<00:03,  5.77it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  83%|██████████████████████████████████████████▏        | 86/104 [00:14<00:03,  5.77it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  84%|██████████████████████████████████████████▋        | 87/104 [00:14<00:02,  5.79it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  84%|██████████████████████████████████████████▋        | 87/104 [00:15<00:02,  5.79it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 2:  85%|███████████████████████████████████████████▏       | 88/104 [00:15<00:02,  5.82it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 2:  85%|███████████████████████████████████████████▏       | 88/104 [00:15<00:02,  5.82it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  86%|███████████████████████████████████████████▋       | 89/104 [00:15<00:02,  5.88it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  86%|███████████████████████████████████████████▋       | 89/104 [00:15<00:02,  5.88it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 2:  87%|████████████████████████████████████████████▏      | 90/104 [00:15<00:02,  5.91it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 2:  87%|████████████████████████████████████████████▏      | 90/104 [00:15<00:02,  5.91it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████████████████████▋      | 91/104 [00:15<00:02,  5.78it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████████████████████▋      | 91/104 [00:15<00:02,  5.78it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 2:  88%|█████████████████████████████████████████████      | 92/104 [00:15<00:02,  5.83it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 2:  88%|█████████████████████████████████████████████      | 92/104 [00:15<00:02,  5.83it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 2:  89%|█████████████████████████████████████████████▌     | 93/104 [00:15<00:01,  5.89it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 2:  89%|█████████████████████████████████████████████▌     | 93/104 [00:16<00:01,  5.89it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  90%|██████████████████████████████████████████████     | 94/104 [00:16<00:01,  5.75it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 2:  90%|██████████████████████████████████████████████     | 94/104 [00:16<00:01,  5.75it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 2:  91%|██████████████████████████████████████████████▌    | 95/104 [00:16<00:01,  5.84it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 2:  91%|██████████████████████████████████████████████▌    | 95/104 [00:16<00:01,  5.84it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 2:  92%|███████████████████████████████████████████████    | 96/104 [00:16<00:01,  5.77it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 2:  92%|███████████████████████████████████████████████    | 96/104 [00:16<00:01,  5.77it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  93%|███████████████████████████████████████████████▌   | 97/104 [00:16<00:01,  5.79it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  93%|███████████████████████████████████████████████▌   | 97/104 [00:16<00:01,  5.79it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  94%|████████████████████████████████████████████████   | 98/104 [00:16<00:01,  5.82it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  94%|████████████████████████████████████████████████   | 98/104 [00:16<00:01,  5.82it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  95%|████████████████████████████████████████████████▌  | 99/104 [00:16<00:00,  5.88it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  95%|████████████████████████████████████████████████▌  | 99/104 [00:17<00:00,  5.88it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  96%|████████████████████████████████████████████████  | 100/104 [00:17<00:00,  5.90it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  96%|████████████████████████████████████████████████  | 100/104 [00:17<00:00,  5.90it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 2:  97%|████████████████████████████████████████████████▌ | 101/104 [00:17<00:00,  5.77it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 2:  97%|████████████████████████████████████████████████▌ | 101/104 [00:17<00:00,  5.77it/s, training_loss=0.218]\u001b[A\n",
      "Epoch 2:  98%|█████████████████████████████████████████████████ | 102/104 [00:17<00:00,  5.82it/s, training_loss=0.218]\u001b[A\n",
      "Epoch 2:  98%|█████████████████████████████████████████████████ | 102/104 [00:17<00:00,  5.82it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 2:  99%|█████████████████████████████████████████████████▌| 103/104 [00:17<00:00,  5.87it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 2:  99%|█████████████████████████████████████████████████▌| 103/104 [00:17<00:00,  5.87it/s, training_loss=0.221]\u001b[A\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████| 104/104 [00:17<00:00,  6.20it/s, training_loss=0.221]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:35<00:00, 17.81s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/52 [00:00<?, ?it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:   2%|█                                                    | 1/52 [00:00<00:21,  2.33it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:   2%|█                                                    | 1/52 [00:00<00:21,  2.33it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:   4%|██                                                   | 2/52 [00:00<00:17,  2.85it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:   4%|██                                                   | 2/52 [00:01<00:17,  2.85it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:   6%|███                                                  | 3/52 [00:01<00:15,  3.13it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:   6%|███                                                  | 3/52 [00:01<00:15,  3.13it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:   8%|████                                                 | 4/52 [00:01<00:15,  3.20it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:   8%|████                                                 | 4/52 [00:01<00:15,  3.20it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  10%|█████                                                | 5/52 [00:01<00:14,  3.32it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  10%|█████                                                | 5/52 [00:01<00:14,  3.32it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  12%|██████                                               | 6/52 [00:01<00:13,  3.32it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  12%|██████                                               | 6/52 [00:02<00:13,  3.32it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  13%|███████▏                                             | 7/52 [00:02<00:13,  3.38it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  13%|███████▏                                             | 7/52 [00:02<00:13,  3.38it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  15%|████████▏                                            | 8/52 [00:02<00:13,  3.37it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  15%|████████▏                                            | 8/52 [00:02<00:13,  3.37it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  17%|█████████▏                                           | 9/52 [00:02<00:12,  3.41it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  17%|█████████▏                                           | 9/52 [00:03<00:12,  3.41it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  19%|██████████                                          | 10/52 [00:03<00:12,  3.38it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  19%|██████████                                          | 10/52 [00:03<00:12,  3.38it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  21%|███████████                                         | 11/52 [00:03<00:11,  3.43it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  21%|███████████                                         | 11/52 [00:03<00:11,  3.43it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  23%|████████████                                        | 12/52 [00:03<00:11,  3.40it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  23%|████████████                                        | 12/52 [00:03<00:11,  3.40it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  25%|█████████████                                       | 13/52 [00:03<00:11,  3.44it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  25%|█████████████                                       | 13/52 [00:04<00:11,  3.44it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 14/52 [00:04<00:11,  3.41it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 14/52 [00:04<00:11,  3.41it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  29%|██████████████▉                                     | 15/52 [00:04<00:10,  3.44it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  29%|██████████████▉                                     | 15/52 [00:04<00:10,  3.44it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  31%|████████████████                                    | 16/52 [00:04<00:10,  3.41it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  31%|████████████████                                    | 16/52 [00:05<00:10,  3.41it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  33%|█████████████████                                   | 17/52 [00:05<00:10,  3.42it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  33%|█████████████████                                   | 17/52 [00:05<00:10,  3.42it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  35%|██████████████████                                  | 18/52 [00:05<00:09,  3.41it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  35%|██████████████████                                  | 18/52 [00:05<00:09,  3.41it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  37%|███████████████████                                 | 19/52 [00:05<00:09,  3.38it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  37%|███████████████████                                 | 19/52 [00:05<00:09,  3.38it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  38%|████████████████████                                | 20/52 [00:05<00:09,  3.43it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  38%|████████████████████                                | 20/52 [00:06<00:09,  3.43it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████                               | 21/52 [00:06<00:09,  3.41it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████                               | 21/52 [00:06<00:09,  3.41it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████                              | 22/52 [00:06<00:08,  3.37it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████                              | 22/52 [00:06<00:08,  3.37it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  44%|███████████████████████                             | 23/52 [00:06<00:08,  3.42it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  44%|███████████████████████                             | 23/52 [00:07<00:08,  3.42it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  46%|████████████████████████                            | 24/52 [00:07<00:08,  3.39it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  46%|████████████████████████                            | 24/52 [00:07<00:08,  3.39it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  48%|█████████████████████████                           | 25/52 [00:07<00:07,  3.43it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  48%|█████████████████████████                           | 25/52 [00:07<00:07,  3.43it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  50%|██████████████████████████                          | 26/52 [00:07<00:07,  3.40it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  50%|██████████████████████████                          | 26/52 [00:08<00:07,  3.40it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  52%|███████████████████████████                         | 27/52 [00:08<00:07,  3.38it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  52%|███████████████████████████                         | 27/52 [00:08<00:07,  3.38it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 28/52 [00:08<00:07,  3.42it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 28/52 [00:08<00:07,  3.42it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  56%|█████████████████████████████                       | 29/52 [00:08<00:06,  3.40it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  56%|█████████████████████████████                       | 29/52 [00:08<00:06,  3.40it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▉                      | 30/52 [00:08<00:06,  3.42it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▉                      | 30/52 [00:09<00:06,  3.42it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████                     | 31/52 [00:09<00:06,  3.41it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████                     | 31/52 [00:09<00:06,  3.41it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████                    | 32/52 [00:09<00:05,  3.37it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████                    | 32/52 [00:09<00:05,  3.37it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████                   | 33/52 [00:09<00:05,  3.42it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████                   | 33/52 [00:10<00:05,  3.42it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████                  | 34/52 [00:10<00:05,  3.38it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████                  | 34/52 [00:10<00:05,  3.38it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  67%|███████████████████████████████████                 | 35/52 [00:10<00:05,  3.38it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  67%|███████████████████████████████████                 | 35/52 [00:10<00:05,  3.38it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  69%|████████████████████████████████████                | 36/52 [00:10<00:04,  3.42it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  69%|████████████████████████████████████                | 36/52 [00:10<00:04,  3.42it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  71%|█████████████████████████████████████               | 37/52 [00:10<00:04,  3.39it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  71%|█████████████████████████████████████               | 37/52 [00:11<00:04,  3.39it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  73%|██████████████████████████████████████              | 38/52 [00:11<00:04,  3.38it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  73%|██████████████████████████████████████              | 38/52 [00:11<00:04,  3.38it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  75%|███████████████████████████████████████             | 39/52 [00:11<00:03,  3.42it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  75%|███████████████████████████████████████             | 39/52 [00:11<00:03,  3.42it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  77%|████████████████████████████████████████            | 40/52 [00:11<00:03,  3.38it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  77%|████████████████████████████████████████            | 40/52 [00:12<00:03,  3.38it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████           | 41/52 [00:12<00:03,  3.37it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████           | 41/52 [00:12<00:03,  3.37it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████          | 42/52 [00:12<00:02,  3.35it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████          | 42/52 [00:12<00:02,  3.35it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████         | 43/52 [00:12<00:02,  3.41it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████         | 43/52 [00:13<00:02,  3.41it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  85%|████████████████████████████████████████████        | 44/52 [00:13<00:02,  3.36it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  85%|████████████████████████████████████████████        | 44/52 [00:13<00:02,  3.36it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  87%|█████████████████████████████████████████████       | 45/52 [00:13<00:02,  3.38it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  87%|█████████████████████████████████████████████       | 45/52 [00:13<00:02,  3.38it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  88%|██████████████████████████████████████████████      | 46/52 [00:13<00:01,  3.36it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  88%|██████████████████████████████████████████████      | 46/52 [00:13<00:01,  3.36it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  90%|███████████████████████████████████████████████     | 47/52 [00:13<00:01,  3.39it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  90%|███████████████████████████████████████████████     | 47/52 [00:14<00:01,  3.39it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  92%|████████████████████████████████████████████████    | 48/52 [00:14<00:01,  3.39it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  92%|████████████████████████████████████████████████    | 48/52 [00:14<00:01,  3.39it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  94%|█████████████████████████████████████████████████   | 49/52 [00:14<00:00,  3.37it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  94%|█████████████████████████████████████████████████   | 49/52 [00:14<00:00,  3.37it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  96%|██████████████████████████████████████████████████  | 50/52 [00:14<00:00,  3.40it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  96%|██████████████████████████████████████████████████  | 50/52 [00:15<00:00,  3.40it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  98%|███████████████████████████████████████████████████ | 51/52 [00:15<00:00,  3.38it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  98%|███████████████████████████████████████████████████ | 51/52 [00:15<00:00,  3.38it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 52/52 [00:15<00:00,  3.49it/s, training_loss=0.335]\u001b[A\n",
      "Epoch Progress:  50%|██████████████████████████████████                                  | 1/2 [00:15<00:15, 15.41s/it]\u001b[A\n",
      "Epoch 2:   0%|                                                                                  | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:   0%|                                                             | 0/52 [00:00<?, ?it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:   2%|█                                                    | 1/52 [00:00<00:14,  3.44it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:   2%|█                                                    | 1/52 [00:00<00:14,  3.44it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 2:   4%|██                                                   | 2/52 [00:00<00:14,  3.42it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 2:   4%|██                                                   | 2/52 [00:00<00:14,  3.42it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 2:   6%|███                                                  | 3/52 [00:00<00:14,  3.39it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 2:   6%|███                                                  | 3/52 [00:01<00:14,  3.39it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:   8%|████                                                 | 4/52 [00:01<00:14,  3.35it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:   8%|████                                                 | 4/52 [00:01<00:14,  3.35it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  10%|█████                                                | 5/52 [00:01<00:13,  3.40it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  10%|█████                                                | 5/52 [00:01<00:13,  3.40it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 2:  12%|██████                                               | 6/52 [00:01<00:13,  3.39it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 2:  12%|██████                                               | 6/52 [00:02<00:13,  3.39it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 2:  13%|███████▏                                             | 7/52 [00:02<00:13,  3.36it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 2:  13%|███████▏                                             | 7/52 [00:02<00:13,  3.36it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 2:  15%|████████▏                                            | 8/52 [00:02<00:13,  3.36it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 2:  15%|████████▏                                            | 8/52 [00:02<00:13,  3.36it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 2:  17%|█████████▏                                           | 9/52 [00:02<00:12,  3.38it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 2:  17%|█████████▏                                           | 9/52 [00:02<00:12,  3.38it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  19%|██████████                                          | 10/52 [00:02<00:12,  3.39it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  19%|██████████                                          | 10/52 [00:03<00:12,  3.39it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 2:  21%|███████████                                         | 11/52 [00:03<00:12,  3.36it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 2:  21%|███████████                                         | 11/52 [00:03<00:12,  3.36it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 2:  23%|████████████                                        | 12/52 [00:03<00:11,  3.36it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 2:  23%|████████████                                        | 12/52 [00:03<00:11,  3.36it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  25%|█████████████                                       | 13/52 [00:03<00:11,  3.36it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  25%|█████████████                                       | 13/52 [00:04<00:11,  3.36it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 2:  27%|██████████████                                      | 14/52 [00:04<00:11,  3.34it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 2:  27%|██████████████                                      | 14/52 [00:04<00:11,  3.34it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 2:  29%|██████████████▉                                     | 15/52 [00:04<00:11,  3.35it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 2:  29%|██████████████▉                                     | 15/52 [00:04<00:11,  3.35it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 2:  31%|████████████████                                    | 16/52 [00:04<00:10,  3.38it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 2:  31%|████████████████                                    | 16/52 [00:05<00:10,  3.38it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  33%|█████████████████                                   | 17/52 [00:05<00:10,  3.37it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  33%|█████████████████                                   | 17/52 [00:05<00:10,  3.37it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 2:  35%|██████████████████                                  | 18/52 [00:05<00:10,  3.36it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 2:  35%|██████████████████                                  | 18/52 [00:05<00:10,  3.36it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 2:  37%|███████████████████                                 | 19/52 [00:05<00:09,  3.41it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 2:  37%|███████████████████                                 | 19/52 [00:05<00:09,  3.41it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 2:  38%|████████████████████                                | 20/52 [00:05<00:09,  3.38it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 2:  38%|████████████████████                                | 20/52 [00:06<00:09,  3.38it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 2:  40%|█████████████████████                               | 21/52 [00:06<00:09,  3.36it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 2:  40%|█████████████████████                               | 21/52 [00:06<00:09,  3.36it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  42%|██████████████████████                              | 22/52 [00:06<00:08,  3.35it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  42%|██████████████████████                              | 22/52 [00:06<00:08,  3.35it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 2:  44%|███████████████████████                             | 23/52 [00:06<00:08,  3.41it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 2:  44%|███████████████████████                             | 23/52 [00:07<00:08,  3.41it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 2:  46%|████████████████████████                            | 24/52 [00:07<00:08,  3.39it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 2:  46%|████████████████████████                            | 24/52 [00:07<00:08,  3.39it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 2:  48%|█████████████████████████                           | 25/52 [00:07<00:08,  3.37it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 2:  48%|█████████████████████████                           | 25/52 [00:07<00:08,  3.37it/s, training_loss=0.300]\u001b[A\n",
      "Epoch 2:  50%|██████████████████████████                          | 26/52 [00:07<00:07,  3.34it/s, training_loss=0.300]\u001b[A\n",
      "Epoch 2:  50%|██████████████████████████                          | 26/52 [00:08<00:07,  3.34it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  52%|███████████████████████████                         | 27/52 [00:08<00:07,  3.35it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  52%|███████████████████████████                         | 27/52 [00:08<00:07,  3.35it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 2:  54%|████████████████████████████                        | 28/52 [00:08<00:07,  3.39it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 2:  54%|████████████████████████████                        | 28/52 [00:08<00:07,  3.39it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 2:  56%|█████████████████████████████                       | 29/52 [00:08<00:06,  3.38it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 2:  56%|█████████████████████████████                       | 29/52 [00:08<00:06,  3.38it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▉                      | 30/52 [00:08<00:06,  3.37it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▉                      | 30/52 [00:09<00:06,  3.37it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 2:  60%|███████████████████████████████                     | 31/52 [00:09<00:06,  3.35it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 2:  60%|███████████████████████████████                     | 31/52 [00:09<00:06,  3.35it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 2:  62%|████████████████████████████████                    | 32/52 [00:09<00:05,  3.35it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 2:  62%|████████████████████████████████                    | 32/52 [00:09<00:05,  3.35it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 2:  63%|█████████████████████████████████                   | 33/52 [00:09<00:05,  3.34it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 2:  63%|█████████████████████████████████                   | 33/52 [00:10<00:05,  3.34it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 2:  65%|██████████████████████████████████                  | 34/52 [00:10<00:05,  3.38it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 2:  65%|██████████████████████████████████                  | 34/52 [00:10<00:05,  3.38it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 2:  67%|███████████████████████████████████                 | 35/52 [00:10<00:05,  3.36it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 2:  67%|███████████████████████████████████                 | 35/52 [00:10<00:05,  3.36it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 2:  69%|████████████████████████████████████                | 36/52 [00:10<00:04,  3.35it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 2:  69%|████████████████████████████████████                | 36/52 [00:10<00:04,  3.35it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 2:  71%|█████████████████████████████████████               | 37/52 [00:10<00:04,  3.35it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 2:  71%|█████████████████████████████████████               | 37/52 [00:11<00:04,  3.35it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  73%|██████████████████████████████████████              | 38/52 [00:11<00:04,  3.35it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  73%|██████████████████████████████████████              | 38/52 [00:11<00:04,  3.35it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 2:  75%|███████████████████████████████████████             | 39/52 [00:11<00:03,  3.35it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 2:  75%|███████████████████████████████████████             | 39/52 [00:11<00:03,  3.35it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  77%|████████████████████████████████████████            | 40/52 [00:11<00:03,  3.33it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 2:  77%|████████████████████████████████████████            | 40/52 [00:12<00:03,  3.33it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 2:  79%|█████████████████████████████████████████           | 41/52 [00:12<00:03,  3.34it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 2:  79%|█████████████████████████████████████████           | 41/52 [00:12<00:03,  3.34it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 2:  81%|██████████████████████████████████████████          | 42/52 [00:12<00:03,  3.33it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 2:  81%|██████████████████████████████████████████          | 42/52 [00:12<00:03,  3.33it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 2:  83%|███████████████████████████████████████████         | 43/52 [00:12<00:02,  3.34it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 2:  83%|███████████████████████████████████████████         | 43/52 [00:13<00:02,  3.34it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 2:  85%|████████████████████████████████████████████        | 44/52 [00:13<00:02,  3.39it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 2:  85%|████████████████████████████████████████████        | 44/52 [00:13<00:02,  3.39it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 2:  87%|█████████████████████████████████████████████       | 45/52 [00:13<00:02,  3.36it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 2:  87%|█████████████████████████████████████████████       | 45/52 [00:13<00:02,  3.36it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 2:  88%|██████████████████████████████████████████████      | 46/52 [00:13<00:01,  3.34it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 2:  88%|██████████████████████████████████████████████      | 46/52 [00:13<00:01,  3.34it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:  90%|███████████████████████████████████████████████     | 47/52 [00:13<00:01,  3.36it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:  90%|███████████████████████████████████████████████     | 47/52 [00:14<00:01,  3.36it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 2:  92%|████████████████████████████████████████████████    | 48/52 [00:14<00:01,  3.34it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 2:  92%|████████████████████████████████████████████████    | 48/52 [00:14<00:01,  3.34it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  94%|█████████████████████████████████████████████████   | 49/52 [00:14<00:00,  3.35it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  94%|█████████████████████████████████████████████████   | 49/52 [00:14<00:00,  3.35it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 2:  96%|██████████████████████████████████████████████████  | 50/52 [00:14<00:00,  3.35it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 2:  96%|██████████████████████████████████████████████████  | 50/52 [00:15<00:00,  3.35it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 2:  98%|███████████████████████████████████████████████████ | 51/52 [00:15<00:00,  3.33it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 2:  98%|███████████████████████████████████████████████████ | 51/52 [00:15<00:00,  3.33it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████| 52/52 [00:15<00:00,  3.49it/s, training_loss=0.335]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:30<00:00, 15.42s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/26 [00:00<?, ?it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:   4%|██                                                   | 1/26 [00:00<00:19,  1.30it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:   4%|██                                                   | 1/26 [00:01<00:19,  1.30it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:   8%|████                                                 | 2/26 [00:01<00:15,  1.56it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:   8%|████                                                 | 2/26 [00:01<00:15,  1.56it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  12%|██████                                               | 3/26 [00:01<00:13,  1.67it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  12%|██████                                               | 3/26 [00:02<00:13,  1.67it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  15%|████████▏                                            | 4/26 [00:02<00:12,  1.71it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  15%|████████▏                                            | 4/26 [00:02<00:12,  1.71it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  19%|██████████▏                                          | 5/26 [00:02<00:11,  1.75it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  19%|██████████▏                                          | 5/26 [00:03<00:11,  1.75it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  23%|████████████▏                                        | 6/26 [00:03<00:11,  1.76it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  23%|████████████▏                                        | 6/26 [00:04<00:11,  1.76it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  27%|██████████████▎                                      | 7/26 [00:04<00:10,  1.77it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  27%|██████████████▎                                      | 7/26 [00:04<00:10,  1.77it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  31%|████████████████▎                                    | 8/26 [00:04<00:10,  1.77it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  31%|████████████████▎                                    | 8/26 [00:05<00:10,  1.77it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                  | 9/26 [00:05<00:09,  1.79it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                  | 9/26 [00:05<00:09,  1.79it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  38%|████████████████████                                | 10/26 [00:05<00:08,  1.78it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  38%|████████████████████                                | 10/26 [00:06<00:08,  1.78it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████                              | 11/26 [00:06<00:08,  1.79it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████                              | 11/26 [00:06<00:08,  1.79it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  46%|████████████████████████                            | 12/26 [00:06<00:07,  1.78it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  46%|████████████████████████                            | 12/26 [00:07<00:07,  1.78it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  50%|██████████████████████████                          | 13/26 [00:07<00:07,  1.79it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  50%|██████████████████████████                          | 13/26 [00:08<00:07,  1.79it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 14/26 [00:08<00:06,  1.79it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 14/26 [00:08<00:06,  1.79it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▉                      | 15/26 [00:08<00:06,  1.80it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▉                      | 15/26 [00:09<00:06,  1.80it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████                    | 16/26 [00:09<00:05,  1.80it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████                    | 16/26 [00:09<00:05,  1.80it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████                  | 17/26 [00:09<00:05,  1.79it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████                  | 17/26 [00:10<00:05,  1.79it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  69%|████████████████████████████████████                | 18/26 [00:10<00:04,  1.80it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  69%|████████████████████████████████████                | 18/26 [00:10<00:04,  1.80it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  73%|██████████████████████████████████████              | 19/26 [00:10<00:03,  1.80it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  73%|██████████████████████████████████████              | 19/26 [00:11<00:03,  1.80it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  77%|████████████████████████████████████████            | 20/26 [00:11<00:03,  1.79it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  77%|████████████████████████████████████████            | 20/26 [00:11<00:03,  1.79it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████          | 21/26 [00:11<00:02,  1.80it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████          | 21/26 [00:12<00:02,  1.80it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  85%|████████████████████████████████████████████        | 22/26 [00:12<00:02,  1.80it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  85%|████████████████████████████████████████████        | 22/26 [00:13<00:02,  1.80it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  88%|██████████████████████████████████████████████      | 23/26 [00:13<00:01,  1.79it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  88%|██████████████████████████████████████████████      | 23/26 [00:13<00:01,  1.79it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  92%|████████████████████████████████████████████████    | 24/26 [00:13<00:01,  1.79it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  92%|████████████████████████████████████████████████    | 24/26 [00:14<00:01,  1.79it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  96%|██████████████████████████████████████████████████  | 25/26 [00:14<00:00,  1.79it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  96%|██████████████████████████████████████████████████  | 25/26 [00:14<00:00,  1.79it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 26/26 [00:14<00:00,  1.83it/s, training_loss=0.369]\u001b[A\n",
      "Epoch Progress:  50%|██████████████████████████████████                                  | 1/2 [00:14<00:14, 14.67s/it]\u001b[A\n",
      "Epoch 2:   0%|                                                                                  | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:   0%|                                                             | 0/26 [00:00<?, ?it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 2:   4%|██                                                   | 1/26 [00:00<00:13,  1.79it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 2:   4%|██                                                   | 1/26 [00:01<00:13,  1.79it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 2:   8%|████                                                 | 2/26 [00:01<00:13,  1.81it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 2:   8%|████                                                 | 2/26 [00:01<00:13,  1.81it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  12%|██████                                               | 3/26 [00:01<00:12,  1.79it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 2:  12%|██████                                               | 3/26 [00:02<00:12,  1.79it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  15%|████████▏                                            | 4/26 [00:02<00:12,  1.80it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  15%|████████▏                                            | 4/26 [00:02<00:12,  1.80it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 2:  19%|██████████▏                                          | 5/26 [00:02<00:11,  1.80it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 2:  19%|██████████▏                                          | 5/26 [00:03<00:11,  1.80it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  23%|████████████▏                                        | 6/26 [00:03<00:11,  1.79it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  23%|████████████▏                                        | 6/26 [00:03<00:11,  1.79it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  27%|██████████████▎                                      | 7/26 [00:03<00:10,  1.79it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 2:  27%|██████████████▎                                      | 7/26 [00:04<00:10,  1.79it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 2:  31%|████████████████▎                                    | 8/26 [00:04<00:10,  1.78it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 2:  31%|████████████████▎                                    | 8/26 [00:05<00:10,  1.78it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 2:  35%|██████████████████▎                                  | 9/26 [00:05<00:09,  1.78it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 2:  35%|██████████████████▎                                  | 9/26 [00:05<00:09,  1.78it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 2:  38%|████████████████████                                | 10/26 [00:05<00:08,  1.79it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 2:  38%|████████████████████                                | 10/26 [00:06<00:08,  1.79it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 2:  42%|██████████████████████                              | 11/26 [00:06<00:08,  1.78it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 2:  42%|██████████████████████                              | 11/26 [00:06<00:08,  1.78it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 2:  46%|████████████████████████                            | 12/26 [00:06<00:07,  1.79it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 2:  46%|████████████████████████                            | 12/26 [00:07<00:07,  1.79it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 2:  50%|██████████████████████████                          | 13/26 [00:07<00:07,  1.79it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 2:  50%|██████████████████████████                          | 13/26 [00:07<00:07,  1.79it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 2:  54%|████████████████████████████                        | 14/26 [00:07<00:06,  1.80it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 2:  54%|████████████████████████████                        | 14/26 [00:08<00:06,  1.80it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▉                      | 15/26 [00:08<00:06,  1.78it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▉                      | 15/26 [00:08<00:06,  1.78it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  62%|████████████████████████████████                    | 16/26 [00:08<00:05,  1.79it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  62%|████████████████████████████████                    | 16/26 [00:09<00:05,  1.79it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 2:  65%|██████████████████████████████████                  | 17/26 [00:09<00:05,  1.79it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 2:  65%|██████████████████████████████████                  | 17/26 [00:10<00:05,  1.79it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 2:  69%|████████████████████████████████████                | 18/26 [00:10<00:04,  1.78it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 2:  69%|████████████████████████████████████                | 18/26 [00:10<00:04,  1.78it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:  73%|██████████████████████████████████████              | 19/26 [00:10<00:03,  1.79it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 2:  73%|██████████████████████████████████████              | 19/26 [00:11<00:03,  1.79it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  77%|████████████████████████████████████████            | 20/26 [00:11<00:03,  1.78it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 2:  77%|████████████████████████████████████████            | 20/26 [00:11<00:03,  1.78it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 2:  81%|██████████████████████████████████████████          | 21/26 [00:11<00:02,  1.79it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 2:  81%|██████████████████████████████████████████          | 21/26 [00:12<00:02,  1.79it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 2:  85%|████████████████████████████████████████████        | 22/26 [00:12<00:02,  1.79it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 2:  85%|████████████████████████████████████████████        | 22/26 [00:12<00:02,  1.79it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  88%|██████████████████████████████████████████████      | 23/26 [00:12<00:01,  1.78it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 2:  88%|██████████████████████████████████████████████      | 23/26 [00:13<00:01,  1.78it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 2:  92%|████████████████████████████████████████████████    | 24/26 [00:13<00:01,  1.79it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 2:  92%|████████████████████████████████████████████████    | 24/26 [00:13<00:01,  1.79it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 2:  96%|██████████████████████████████████████████████████  | 25/26 [00:13<00:00,  1.78it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 2:  96%|██████████████████████████████████████████████████  | 25/26 [00:14<00:00,  1.78it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████| 26/26 [00:14<00:00,  1.82it/s, training_loss=0.335]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:29<00:00, 14.60s/it]\u001b[A\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/2 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/13 [00:01<?, ?it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   8%|████                                                 | 1/13 [00:01<00:15,  1.25s/it, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   8%|████                                                 | 1/13 [00:02<00:15,  1.25s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  15%|████████▏                                            | 2/13 [00:02<00:12,  1.14s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  15%|████████▏                                            | 2/13 [00:03<00:12,  1.14s/it, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  23%|████████████▏                                        | 3/13 [00:03<00:11,  1.11s/it, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  23%|████████████▏                                        | 3/13 [00:04<00:11,  1.11s/it, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  31%|████████████████▎                                    | 4/13 [00:04<00:09,  1.09s/it, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  31%|████████████████▎                                    | 4/13 [00:05<00:09,  1.09s/it, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  38%|████████████████████▍                                | 5/13 [00:05<00:08,  1.09s/it, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  38%|████████████████████▍                                | 5/13 [00:06<00:08,  1.09s/it, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  46%|████████████████████████▍                            | 6/13 [00:06<00:07,  1.09s/it, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  46%|████████████████████████▍                            | 6/13 [00:07<00:07,  1.09s/it, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████▌                        | 7/13 [00:07<00:06,  1.08s/it, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████▌                        | 7/13 [00:08<00:06,  1.08s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▌                    | 8/13 [00:08<00:05,  1.08s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▌                    | 8/13 [00:09<00:05,  1.08s/it, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  69%|████████████████████████████████████▋                | 9/13 [00:09<00:04,  1.08s/it, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  69%|████████████████████████████████████▋                | 9/13 [00:10<00:04,  1.08s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  77%|████████████████████████████████████████            | 10/13 [00:10<00:03,  1.07s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  77%|████████████████████████████████████████            | 10/13 [00:11<00:03,  1.07s/it, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  85%|████████████████████████████████████████████        | 11/13 [00:11<00:02,  1.08s/it, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  85%|████████████████████████████████████████████        | 11/13 [00:13<00:02,  1.08s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  92%|████████████████████████████████████████████████    | 12/13 [00:13<00:01,  1.08s/it, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  92%|████████████████████████████████████████████████    | 12/13 [00:14<00:01,  1.08s/it, training_loss=0.355]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 13/13 [00:14<00:00,  1.07s/it, training_loss=0.355]\u001b[A\n",
      "Epoch Progress:  50%|██████████████████████████████████                                  | 1/2 [00:14<00:14, 14.11s/it]\u001b[A\n",
      "Epoch 2:   0%|                                                                                  | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:   0%|                                                             | 0/13 [00:01<?, ?it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 2:   8%|████                                                 | 1/13 [00:01<00:12,  1.07s/it, training_loss=0.339]\u001b[A\n",
      "Epoch 2:   8%|████                                                 | 1/13 [00:02<00:12,  1.07s/it, training_loss=0.344]\u001b[A\n",
      "Epoch 2:  15%|████████▏                                            | 2/13 [00:02<00:11,  1.08s/it, training_loss=0.344]\u001b[A\n",
      "Epoch 2:  15%|████████▏                                            | 2/13 [00:03<00:11,  1.08s/it, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  23%|████████████▏                                        | 3/13 [00:03<00:10,  1.08s/it, training_loss=0.354]\u001b[A\n",
      "Epoch 2:  23%|████████████▏                                        | 3/13 [00:04<00:10,  1.08s/it, training_loss=0.363]\u001b[A\n",
      "Epoch 2:  31%|████████████████▎                                    | 4/13 [00:04<00:09,  1.08s/it, training_loss=0.363]\u001b[A\n",
      "Epoch 2:  31%|████████████████▎                                    | 4/13 [00:05<00:09,  1.08s/it, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  38%|████████████████████▍                                | 5/13 [00:05<00:08,  1.08s/it, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  38%|████████████████████▍                                | 5/13 [00:06<00:08,  1.08s/it, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  46%|████████████████████████▍                            | 6/13 [00:06<00:07,  1.08s/it, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  46%|████████████████████████▍                            | 6/13 [00:07<00:07,  1.08s/it, training_loss=0.348]\u001b[A\n",
      "Epoch 2:  54%|████████████████████████████▌                        | 7/13 [00:07<00:06,  1.08s/it, training_loss=0.348]\u001b[A\n",
      "Epoch 2:  54%|████████████████████████████▌                        | 7/13 [00:08<00:06,  1.08s/it, training_loss=0.349]\u001b[A\n",
      "Epoch 2:  62%|████████████████████████████████▌                    | 8/13 [00:08<00:05,  1.09s/it, training_loss=0.349]\u001b[A\n",
      "Epoch 2:  62%|████████████████████████████████▌                    | 8/13 [00:09<00:05,  1.09s/it, training_loss=0.349]\u001b[A\n",
      "Epoch 2:  69%|████████████████████████████████████▋                | 9/13 [00:09<00:04,  1.08s/it, training_loss=0.349]\u001b[A\n",
      "Epoch 2:  69%|████████████████████████████████████▋                | 9/13 [00:10<00:04,  1.08s/it, training_loss=0.355]\u001b[A\n",
      "Epoch 2:  77%|████████████████████████████████████████            | 10/13 [00:10<00:03,  1.08s/it, training_loss=0.355]\u001b[A\n",
      "Epoch 2:  77%|████████████████████████████████████████            | 10/13 [00:11<00:03,  1.08s/it, training_loss=0.356]\u001b[A\n",
      "Epoch 2:  85%|████████████████████████████████████████████        | 11/13 [00:11<00:02,  1.08s/it, training_loss=0.356]\u001b[A\n",
      "Epoch 2:  85%|████████████████████████████████████████████        | 11/13 [00:12<00:02,  1.08s/it, training_loss=0.346]\u001b[A\n",
      "Epoch 2:  92%|████████████████████████████████████████████████    | 12/13 [00:12<00:01,  1.08s/it, training_loss=0.346]\u001b[A\n",
      "Epoch 2:  92%|████████████████████████████████████████████████    | 12/13 [00:14<00:01,  1.08s/it, training_loss=0.343]\u001b[A\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████| 13/13 [00:14<00:00,  1.07s/it, training_loss=0.343]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:28<00:00, 14.07s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#data split function\n",
    "def custom_train_val_test_split(X, y, test_size=0.1, val_size=0.1, random_state=None):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    # Find classes with only one or two instances\n",
    "    small_classes = classes[counts < 5]\n",
    "\n",
    "    # Separate out the instances of small classes\n",
    "    large_class_mask = ~np.isin(y, small_classes)\n",
    "    X_large = X[large_class_mask]\n",
    "    y_large = y[large_class_mask]\n",
    "    X_small = X[~large_class_mask]\n",
    "    y_small = y[~large_class_mask]\n",
    "\n",
    "    # Perform stratified split on the larger classes dataset\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_large, y_large, test_size=(val_size + test_size),\n",
    "        random_state=random_state, stratify=y_large\n",
    "    )\n",
    "\n",
    "    # Split the remaining data into validation and testing sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=(test_size / (test_size + val_size)),\n",
    "        random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # Randomly assign instances of small classes to training, validation, or testing sets\n",
    "    for i in range(len(X_small)):\n",
    "        rand_choice = np.random.rand()\n",
    "        if rand_choice < test_size:\n",
    "            X_test = np.vstack([X_test, X_small[i]])\n",
    "            y_test = np.hstack([y_test, y_small[i]])\n",
    "        elif rand_choice < (test_size + val_size):\n",
    "            X_val = np.vstack([X_val, X_small[i]])\n",
    "            y_val = np.hstack([y_val, y_small[i]])\n",
    "        else:\n",
    "            X_train = np.vstack([X_train, X_small[i]])\n",
    "            y_train = np.hstack([y_train, y_small[i]])\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "#evaluation\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "\n",
    "    return accuracy_dict, count_dict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='combine_top3_finetuning_batchsize.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "cadec_csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "# read data from smm4h\n",
    "smm4h_csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n",
    "\n",
    "top3SMM4H = [10037175, 10018065,10029205]\n",
    "top3label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2\n",
    "}\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"ade\", \"soc_code\"]\n",
    "smm4h_all = pd.read_csv(smm4h_csv_url,names=column_names, sep = '\\t', header=None)\n",
    "\n",
    "smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n",
    "smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n",
    "smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# print(\"smm4h data:\",smm4h_all.shape)\n",
    "smm4h_soc_code_counts = smm4h_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",smm4h_soc_code_counts)\n",
    "# Filter DataFrame\n",
    "smm4h_filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(top3SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "top3inSMM4H = smm4h_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "top3inSMM4H.loc[:, 'label'] = top3inSMM4H['soc_code'].map(top3label_dict)\n",
    "\n",
    "# print(\"top3 in SMM4H:\",top3inSMM4H)\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(cadec_csv_url,names=column_names, header=None)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "# print(\"cadec raw data:\",cadec_all.shape)\n",
    "\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(\"clean cadec data:\",cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "cadec_soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",cadec_soc_code_counts)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "cadec_filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(top3SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECtop3inSMM4H = cadec_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "\n",
    "# print(\"CADEC top3 in SMM4H:\",CADECtop3inSMM4H)\n",
    "\n",
    "df1 = top3inSMM4H\n",
    "df2 = CADECtop3inSMM4H\n",
    "df1.loc[:, 'label'] = df1['soc_code'].map(top3label_dict)\n",
    "df2.loc[:, 'label'] = df2['soc_code'].map(top3label_dict)\n",
    "\n",
    "print(\"SMM4H top 3\",df1)\n",
    "print(\"CADEC top 3\",df2)\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 12, 2))\n",
    "\n",
    "# fix batch_size 16, learning rate 1e-5; Finetuning epochs\n",
    "# learning_rates = [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "# batch_sizes = [8, 16, 32, 64]\n",
    "# epochs_list = [10, 20, 30, 40]\n",
    "learning_rates = [1e-5]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "epochs_list = [20]\n",
    "# Results storage\n",
    "results = []\n",
    "# Store the training losses across all splits\n",
    "all_training_losses = []\n",
    "all_val_losses = []\n",
    "\n",
    "# Store results for plotting\n",
    "macro_precisions = {bs: [] for bs in batch_sizes}\n",
    "macro_recalls = {bs: [] for bs in batch_sizes}\n",
    "macro_f1s = {bs: [] for bs in batch_sizes}\n",
    "\n",
    "# Main loop over seed values\n",
    "for seed_val in seed_values:\n",
    "    # Set seeds\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Placeholder for accuracies\n",
    "    all_accuracies = {label: [] for label in range(len(top3label_dict))}\n",
    "    # Store the training losses for this split\n",
    "    training_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "\n",
    "    # Data preparation\n",
    "    # X_train, X_val, y_train, y_val = custom_train_test_split(df.index.values, df.label.values, test_size=0.2, random_state=seed_val)\n",
    "    # X_train, X_val, X_test, y_train, y_val, y_test = custom_train_val_test_split(df.index.values, df.label.values, test_size=0.1, val_size=0.1, random_state=seed_val)\n",
    "\n",
    "    # df['data_type'] = ['not_set'] * df.shape[0]\n",
    "    # df.loc[X_train, 'data_type'] = 'train'\n",
    "    # df.loc[X_val, 'data_type'] = 'val'\n",
    "    # df.loc[X_test, 'data_type'] = 'test'\n",
    "    # logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "\n",
    "    #add \n",
    "    X_train_idx1, X_val_idx1, X_test_idx1, y_train1, y_val1, y_test1 = custom_train_val_test_split(df1.index.values, df1.label.values, test_size=0.1, val_size=0.1, random_state=seed_val)\n",
    "    \n",
    "    # Perform train-test split on df2\n",
    "    X_train_idx2, X_val_idx2, X_test_idx2, y_train2, y_val2, y_test2 = custom_train_val_test_split(df2.index.values, df2.label.values, test_size=0.1, val_size=0.1, random_state=seed_val)\n",
    "\n",
    "    \n",
    "    # Combine the training indices and labels from df1 and df2\n",
    "    X_train_combined = np.concatenate((X_train_idx1, X_train_idx2))\n",
    "    y_train_combined = np.concatenate((y_train1, y_train2))\n",
    "    \n",
    "    # Combine the validation indices and labels from df1 and df2\n",
    "    X_val_combined = np.concatenate((X_val_idx1, X_val_idx2))\n",
    "    y_val_combined = np.concatenate((y_val1, y_val2))\n",
    "\n",
    "    # Combine the validation indices and labels from df1 and df2\n",
    "    X_test_combined = np.concatenate((X_test_idx1, X_test_idx2))\n",
    "    y_test_combined = np.concatenate((y_test1, y_test2))\n",
    "    \n",
    "    # Optionally, you can set the 'data_type' column for df1 and df2\n",
    "    df1['data_type'] = 'not_set'\n",
    "    df2['data_type'] = 'not_set'\n",
    "    \n",
    "    df1.loc[X_train_idx1, 'data_type'] = 'train'\n",
    "    df1.loc[X_val_idx1, 'data_type'] = 'val'\n",
    "    df1.loc[X_test_idx1, 'data_type'] = 'test'\n",
    "    \n",
    "    df2.loc[X_train_idx2, 'data_type'] = 'train'\n",
    "    df2.loc[X_val_idx2, 'data_type'] = 'val'\n",
    "    df2.loc[X_test_idx2, 'data_type'] = 'test'\n",
    "    \n",
    "    # If you want to combine df1 and df2 into a single dataframe:\n",
    "    df = pd.concat([df1, df2])\n",
    "\n",
    "\n",
    "    \n",
    "    # Training loop for grid search\n",
    "    for lr in learning_rates:\n",
    "      for batch_size in batch_sizes:\n",
    "          for epochs in epochs_list:\n",
    "            logger.info(f\"Seed: {seed_val}, Learning Rate: {lr}, Batch Size: {batch_size}, Epochs: {epochs}\")\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "            encoded_data_train = tokenizer.batch_encode_plus(\n",
    "                df[df.data_type == 'train'].ade.values,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=True,\n",
    "                pad_to_max_length=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            encoded_data_val = tokenizer.batch_encode_plus(\n",
    "                df[df.data_type == 'val'].ade.values,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=True,\n",
    "                pad_to_max_length=True,\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids_train = encoded_data_train['input_ids']\n",
    "            attention_masks_train = encoded_data_train['attention_mask']\n",
    "            labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "            input_ids_val = encoded_data_val['input_ids']\n",
    "            attention_masks_val = encoded_data_val['attention_mask']\n",
    "            labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n",
    "\n",
    "            dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "            dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "            model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top3label_dict), output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "            dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "            dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "            optimizer = AdamW(model.parameters(), lr=lr, eps=1e-8)\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model.to(device)\n",
    "            logger.info(f\"Device used: {device}\")\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "                model.train()\n",
    "                loss_train_total = 0\n",
    "\n",
    "                progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "                for batch in progress_bar:\n",
    "                    model.zero_grad()\n",
    "                    batch = tuple(b.to(device) for b in batch)\n",
    "                    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "                    outputs = model(**inputs)\n",
    "                    loss = outputs[0]\n",
    "                    loss_train_total += loss.item()\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "\n",
    "                # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n",
    "\n",
    "                logger.info(f'\\nEpoch {epoch}')\n",
    "                loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "                logger.info(f'Training loss: {loss_train_avg}')\n",
    "                training_losses.append(loss_train_avg)\n",
    "\n",
    "            val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(true_vals, np.argmax(predictions, axis=1), average=None)\n",
    "            macro_precision = np.mean(precision)\n",
    "            macro_recall = np.mean(recall)\n",
    "            macro_f1 = np.mean(f1)\n",
    "\n",
    "            # Store metrics\n",
    "            macro_precisions[batch_size].append(macro_precision)\n",
    "            macro_recalls[batch_size].append(macro_recall)\n",
    "            macro_f1s[batch_size].append(macro_f1)\n",
    "\n",
    "\n",
    "            #store the training loss\n",
    "            all_training_losses.append(training_losses)\n",
    "            #store the val loss\n",
    "            all_val_losses.append(val_losses)\n",
    "\n",
    "            #store the prediction\n",
    "            accuracy_dict, count_dict = accuracy_per_class(predictions, true_vals)\n",
    "\n",
    "            for label, accuracy in accuracy_dict.items():\n",
    "                all_accuracies[label].append(accuracy)\n",
    "\n",
    "\n",
    "            # Calculate the average accuracy for each label\n",
    "            avg_accuracy = {label: np.mean(accs) for label, accs in all_accuracies.items()}\n",
    "\n",
    "            # Calculate the overall average accuracy across all labels\n",
    "            overall_avg_accuracy = np.mean(list(avg_accuracy.values()))\n",
    "\n",
    "            logger.info(f'Seed {seed_val} - Accuracy: {overall_avg_accuracy} - Count: {count_dict} - lr: {lr} -batchsize:{batch_size} -epochs:{epochs}')\n",
    "            #store results\n",
    "            results.append((lr, batch_size, epochs, overall_avg_accuracy))\n",
    "\n",
    "\n",
    "# Compute means and standard deviations\n",
    "mean_precisions = {bs: np.mean(macro_precisions[bs]) for bs in batch_sizes}\n",
    "std_precisions = {bs: np.std(macro_precisions[bs]) for bs in batch_sizes}\n",
    "mean_recalls = {bs: np.mean(macro_recalls[bs]) for bs in batch_sizes}\n",
    "std_recalls = {bs: np.std(macro_recalls[bs]) for bs in batch_sizes}\n",
    "mean_f1s = {bs: np.mean(macro_f1s[bs]) for bs in batch_sizes}\n",
    "std_f1s = {bs: np.std(macro_f1s[bs]) for bs in batch_sizes}\n",
    "\n",
    "# Open a file to write the results\n",
    "with open('combine_finetuning_batchsize.txt', 'w') as file:\n",
    "    file.write(\"Batch Size\\tMacro Precision\\tMacro Recall\\tMacro F1 Score\\n\")\n",
    "    for bs in batch_sizes:\n",
    "        precisions = ', '.join(map(str, macro_precisions[bs]))\n",
    "        recalls = ', '.join(map(str, macro_recalls[bs]))\n",
    "        f1s = ', '.join(map(str, macro_f1s[bs]))\n",
    "        file.write(f\"{bs}\\t{precisions}\\t{recalls}\\t{f1s}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert batch sizes to strings for categorical x-axis\n",
    "batch_size_labels = list(map(str, batch_sizes))\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Precision with a specific color\n",
    "plt.errorbar(batch_size_labels, [mean_precisions[bs] for bs in batch_sizes], \n",
    "             yerr=[std_precisions[bs] for bs in batch_sizes], \n",
    "             label='Macro Precision', fmt='-o', color='blue')\n",
    "\n",
    "# Recall with a different color\n",
    "plt.errorbar(batch_size_labels, [mean_recalls[bs] for bs in batch_sizes], \n",
    "             yerr=[std_recalls[bs] for bs in batch_sizes], \n",
    "             label='Macro Recall', fmt='-o', color='green')\n",
    "\n",
    "# F1 Score with another color\n",
    "plt.errorbar(batch_size_labels, [mean_f1s[bs] for bs in batch_sizes], \n",
    "             yerr=[std_f1s[bs] for bs in batch_sizes], \n",
    "             label='Macro F1 Score', fmt='-o', color='red')\n",
    "\n",
    "# Add labels and title with larger font size\n",
    "plt.xlabel('Batch Size', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.title('Macro Average Precision, Recall, and F1 Score vs. Batch Size', fontsize=16)\n",
    "\n",
    "# Add legend with larger font size\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Grid with default settings\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot to a file with higher resolution\n",
    "plt.savefig('combine_top3_f1_vs_batchsize.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
