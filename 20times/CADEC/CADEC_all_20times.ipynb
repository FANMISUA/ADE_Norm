{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff23d1-4019-4c66-959a-3b3b89390067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "smm4h data: (1712, 2)\n",
      "smm4h data after filtering: (1710, 2)\n",
      "SOC count in SMM4H:  soc_code\n",
      "10037175    287\n",
      "10018065    235\n",
      "10029205    212\n",
      "10017947     63\n",
      "10028395     58\n",
      "10022891     54\n",
      "10027433     48\n",
      "10040785     28\n",
      "10038738     22\n",
      "10022117     16\n",
      "10015919     16\n",
      "10038604     10\n",
      "10047065     10\n",
      "10021428      8\n",
      "10041244      7\n",
      "10007541      7\n",
      "10038359      6\n",
      "10021881      5\n",
      "10013993      4\n",
      "10019805      2\n",
      "10042613      2\n",
      "10029104      2\n",
      "10077536      1\n",
      "10010331      1\n",
      "10014698      1\n",
      "Name: count, dtype: Int64\n",
      "SOC count in CADEC:  soc_code\n",
      "10028395    962\n",
      "10018065    654\n",
      "10037175    401\n",
      "10017947    300\n",
      "10029205    286\n",
      "10040785    184\n",
      "10007541     92\n",
      "10038738     91\n",
      "10022891     82\n",
      "10015919     67\n",
      "10038604     59\n",
      "10038359     50\n",
      "10022117     35\n",
      "10047065     25\n",
      "10013993     16\n",
      "10019805     15\n",
      "10041244      7\n",
      "10027433      6\n",
      "10021881      5\n",
      "10021428      4\n",
      "10014698      3\n",
      "10005329      3\n",
      "10029104      1\n",
      "Name: count, dtype: int64\n",
      "SMM4H :                             ade  soc_code  label\n",
      "1                     allergies  10021428     13\n",
      "2               HURT YOUR Liver  10019805     19\n",
      "3                            AD  10037175      0\n",
      "4                         focus  10029205      2\n",
      "5                          died  10018065      1\n",
      "...                         ...       ...    ...\n",
      "1703                 chest hurt  10018065      1\n",
      "1704   got ten minutes of sleep  10037175      0\n",
      "1706                  Nosebleed  10038738      8\n",
      "1708  never have another orgasm  10037175      0\n",
      "1710        gain so much weight  10022891      5\n",
      "\n",
      "[1105 rows x 3 columns]\n",
      "CADEC :                                    ade  soc_code  label\n",
      "3                      ankles swelling  10007541     15\n",
      "4             sever swelling of ankles  10007541     15\n",
      "5                      Edema of ankles  10007541     15\n",
      "6                    severe arrythmias  10007541     15\n",
      "7                            arrythmia  10007541     15\n",
      "...                                ...       ...    ...\n",
      "5955                      hypertension  10047065     12\n",
      "5956           Elevated blood pressure  10047065     12\n",
      "5959  LITTLE CIRCULATION IN MY FINGERS  10047065     12\n",
      "5960                  going into shock  10047065     12\n",
      "5961     vein in my one leg is bulging  10047065     12\n",
      "\n",
      "[3345 rows x 3 columns]\n",
      "                          ade\n",
      "soc_code label data_type     \n",
      "10007541 15    train       73\n",
      "               val         19\n",
      "10013993 18    train       13\n",
      "               val          3\n",
      "10014698 24    train        2\n",
      "               val          1\n",
      "10015919 10    train       54\n",
      "               val         13\n",
      "10017947 3     train      240\n",
      "               val         60\n",
      "10018065 1     train      523\n",
      "               val        131\n",
      "10019805 19    train       12\n",
      "               val          3\n",
      "10021428 13    train        3\n",
      "               val          1\n",
      "10021881 17    train        4\n",
      "               val          1\n",
      "10022117 9     train       28\n",
      "               val          7\n",
      "10022891 5     train       65\n",
      "               val         17\n",
      "10027433 6     train        5\n",
      "               val          1\n",
      "10028395 4     train      769\n",
      "               val        193\n",
      "10029104 21    val          1\n",
      "10029205 2     train      229\n",
      "               val         57\n",
      "10037175 0     train      321\n",
      "               val         80\n",
      "10038359 16    train       40\n",
      "               val         10\n",
      "10038604 11    train       47\n",
      "               val         12\n",
      "10038738 8     train       73\n",
      "               val         18\n",
      "10040785 7     train      147\n",
      "               val         37\n",
      "10041244 14    train        6\n",
      "               val          1\n",
      "10047065 12    train       20\n",
      "               val          5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/4 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                 | 0/168 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                            | 0/168 [00:00<?, ?it/s, training_loss=1.040]\u001b[A\n",
      "Epoch 1:   1%|▎                                                   | 1/168 [00:00<01:03,  2.64it/s, training_loss=1.040]\u001b[A\n",
      "Epoch 1:   1%|▎                                                   | 1/168 [00:00<01:03,  2.64it/s, training_loss=1.054]\u001b[A\n",
      "Epoch 1:   1%|▌                                                   | 2/168 [00:00<00:41,  3.96it/s, training_loss=1.054]\u001b[A\n",
      "Epoch 1:   1%|▌                                                   | 2/168 [00:00<00:41,  3.96it/s, training_loss=0.967]\u001b[A\n",
      "Epoch 1:   2%|▉                                                   | 3/168 [00:00<00:34,  4.72it/s, training_loss=0.967]\u001b[A\n",
      "Epoch 1:   2%|▉                                                   | 3/168 [00:00<00:34,  4.72it/s, training_loss=0.949]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                  | 4/168 [00:00<00:31,  5.18it/s, training_loss=0.949]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                  | 4/168 [00:01<00:31,  5.18it/s, training_loss=0.925]\u001b[A\n",
      "Epoch 1:   3%|█▌                                                  | 5/168 [00:01<00:29,  5.50it/s, training_loss=0.925]\u001b[A\n",
      "Epoch 1:   3%|█▌                                                  | 5/168 [00:01<00:29,  5.50it/s, training_loss=1.023]\u001b[A\n",
      "Epoch 1:   4%|█▊                                                  | 6/168 [00:01<00:28,  5.65it/s, training_loss=1.023]\u001b[A\n",
      "Epoch 1:   4%|█▊                                                  | 6/168 [00:01<00:28,  5.65it/s, training_loss=1.015]\u001b[A\n",
      "Epoch 1:   4%|██▏                                                 | 7/168 [00:01<00:28,  5.68it/s, training_loss=1.015]\u001b[A\n",
      "Epoch 1:   4%|██▏                                                 | 7/168 [00:01<00:28,  5.68it/s, training_loss=0.867]\u001b[A\n",
      "Epoch 1:   5%|██▍                                                 | 8/168 [00:01<00:27,  5.81it/s, training_loss=0.867]\u001b[A\n",
      "Epoch 1:   5%|██▍                                                 | 8/168 [00:01<00:27,  5.81it/s, training_loss=0.928]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                 | 9/168 [00:01<00:27,  5.79it/s, training_loss=0.928]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                 | 9/168 [00:01<00:27,  5.79it/s, training_loss=0.946]\u001b[A\n",
      "Epoch 1:   6%|███                                                | 10/168 [00:01<00:26,  5.87it/s, training_loss=0.946]\u001b[A\n",
      "Epoch 1:   6%|███                                                | 10/168 [00:02<00:26,  5.87it/s, training_loss=0.889]\u001b[A\n",
      "Epoch 1:   7%|███▎                                               | 11/168 [00:02<00:26,  5.89it/s, training_loss=0.889]\u001b[A\n",
      "Epoch 1:   7%|███▎                                               | 11/168 [00:02<00:26,  5.89it/s, training_loss=0.959]\u001b[A\n",
      "Epoch 1:   7%|███▋                                               | 12/168 [00:02<00:26,  5.86it/s, training_loss=0.959]\u001b[A\n",
      "Epoch 1:   7%|███▋                                               | 12/168 [00:02<00:26,  5.86it/s, training_loss=0.875]\u001b[A\n",
      "Epoch 1:   8%|███▉                                               | 13/168 [00:02<00:26,  5.93it/s, training_loss=0.875]\u001b[A\n",
      "Epoch 1:   8%|███▉                                               | 13/168 [00:02<00:26,  5.93it/s, training_loss=0.942]\u001b[A\n",
      "Epoch 1:   8%|████▎                                              | 14/168 [00:02<00:25,  5.99it/s, training_loss=0.942]\u001b[A\n",
      "Epoch 1:   8%|████▎                                              | 14/168 [00:02<00:25,  5.99it/s, training_loss=0.727]\u001b[A\n",
      "Epoch 1:   9%|████▌                                              | 15/168 [00:02<00:25,  6.02it/s, training_loss=0.727]\u001b[A\n",
      "Epoch 1:   9%|████▌                                              | 15/168 [00:02<00:25,  6.02it/s, training_loss=0.760]\u001b[A\n",
      "Epoch 1:  10%|████▊                                              | 16/168 [00:02<00:25,  6.05it/s, training_loss=0.760]\u001b[A\n",
      "Epoch 1:  10%|████▊                                              | 16/168 [00:03<00:25,  6.05it/s, training_loss=0.790]\u001b[A\n",
      "Epoch 1:  10%|█████▏                                             | 17/168 [00:03<00:24,  6.07it/s, training_loss=0.790]\u001b[A\n",
      "Epoch 1:  10%|█████▏                                             | 17/168 [00:03<00:24,  6.07it/s, training_loss=0.641]\u001b[A\n",
      "Epoch 1:  11%|█████▍                                             | 18/168 [00:03<00:24,  6.08it/s, training_loss=0.641]\u001b[A\n",
      "Epoch 1:  11%|█████▍                                             | 18/168 [00:03<00:24,  6.08it/s, training_loss=0.695]\u001b[A\n",
      "Epoch 1:  11%|█████▊                                             | 19/168 [00:03<00:24,  6.09it/s, training_loss=0.695]\u001b[A\n",
      "Epoch 1:  11%|█████▊                                             | 19/168 [00:03<00:24,  6.09it/s, training_loss=0.791]\u001b[A\n",
      "Epoch 1:  12%|██████                                             | 20/168 [00:03<00:24,  5.98it/s, training_loss=0.791]\u001b[A\n",
      "Epoch 1:  12%|██████                                             | 20/168 [00:03<00:24,  5.98it/s, training_loss=0.796]\u001b[A\n",
      "Epoch 1:  12%|██████▍                                            | 21/168 [00:03<00:24,  5.98it/s, training_loss=0.796]\u001b[A\n",
      "Epoch 1:  12%|██████▍                                            | 21/168 [00:03<00:24,  5.98it/s, training_loss=0.782]\u001b[A\n",
      "Epoch 1:  13%|██████▋                                            | 22/168 [00:03<00:24,  6.00it/s, training_loss=0.782]\u001b[A\n",
      "Epoch 1:  13%|██████▋                                            | 22/168 [00:04<00:24,  6.00it/s, training_loss=0.768]\u001b[A\n",
      "Epoch 1:  14%|██████▉                                            | 23/168 [00:04<00:24,  6.04it/s, training_loss=0.768]\u001b[A\n",
      "Epoch 1:  14%|██████▉                                            | 23/168 [00:04<00:24,  6.04it/s, training_loss=0.817]\u001b[A\n",
      "Epoch 1:  14%|███████▎                                           | 24/168 [00:04<00:23,  6.07it/s, training_loss=0.817]\u001b[A\n",
      "Epoch 1:  14%|███████▎                                           | 24/168 [00:04<00:23,  6.07it/s, training_loss=0.775]\u001b[A\n",
      "Epoch 1:  15%|███████▌                                           | 25/168 [00:04<00:23,  6.09it/s, training_loss=0.775]\u001b[A\n",
      "Epoch 1:  15%|███████▌                                           | 25/168 [00:04<00:23,  6.09it/s, training_loss=0.630]\u001b[A\n",
      "Epoch 1:  15%|███████▉                                           | 26/168 [00:04<00:23,  6.01it/s, training_loss=0.630]\u001b[A\n",
      "Epoch 1:  15%|███████▉                                           | 26/168 [00:04<00:23,  6.01it/s, training_loss=0.759]\u001b[A\n",
      "Epoch 1:  16%|████████▏                                          | 27/168 [00:04<00:23,  5.97it/s, training_loss=0.759]\u001b[A\n",
      "Epoch 1:  16%|████████▏                                          | 27/168 [00:04<00:23,  5.97it/s, training_loss=0.714]\u001b[A\n",
      "Epoch 1:  17%|████████▌                                          | 28/168 [00:04<00:23,  5.99it/s, training_loss=0.714]\u001b[A\n",
      "Epoch 1:  17%|████████▌                                          | 28/168 [00:05<00:23,  5.99it/s, training_loss=0.622]\u001b[A\n",
      "Epoch 1:  17%|████████▊                                          | 29/168 [00:05<00:23,  5.92it/s, training_loss=0.622]\u001b[A\n",
      "Epoch 1:  17%|████████▊                                          | 29/168 [00:05<00:23,  5.92it/s, training_loss=0.685]\u001b[A\n",
      "Epoch 1:  18%|█████████                                          | 30/168 [00:05<00:23,  5.90it/s, training_loss=0.685]\u001b[A\n",
      "Epoch 1:  18%|█████████                                          | 30/168 [00:05<00:23,  5.90it/s, training_loss=0.553]\u001b[A\n",
      "Epoch 1:  18%|█████████▍                                         | 31/168 [00:05<00:23,  5.87it/s, training_loss=0.553]\u001b[A\n",
      "Epoch 1:  18%|█████████▍                                         | 31/168 [00:05<00:23,  5.87it/s, training_loss=0.795]\u001b[A\n",
      "Epoch 1:  19%|█████████▋                                         | 32/168 [00:05<00:22,  5.93it/s, training_loss=0.795]\u001b[A\n",
      "Epoch 1:  19%|█████████▋                                         | 32/168 [00:05<00:22,  5.93it/s, training_loss=0.731]\u001b[A\n",
      "Epoch 1:  20%|██████████                                         | 33/168 [00:05<00:22,  5.94it/s, training_loss=0.731]\u001b[A\n",
      "Epoch 1:  20%|██████████                                         | 33/168 [00:05<00:22,  5.94it/s, training_loss=0.695]\u001b[A\n",
      "Epoch 1:  20%|██████████▎                                        | 34/168 [00:05<00:22,  5.89it/s, training_loss=0.695]\u001b[A\n",
      "Epoch 1:  20%|██████████▎                                        | 34/168 [00:06<00:22,  5.89it/s, training_loss=0.689]\u001b[A\n",
      "Epoch 1:  21%|██████████▋                                        | 35/168 [00:06<00:22,  5.94it/s, training_loss=0.689]\u001b[A\n",
      "Epoch 1:  21%|██████████▋                                        | 35/168 [00:06<00:22,  5.94it/s, training_loss=0.569]\u001b[A\n",
      "Epoch 1:  21%|██████████▉                                        | 36/168 [00:06<00:22,  5.99it/s, training_loss=0.569]\u001b[A\n",
      "Epoch 1:  21%|██████████▉                                        | 36/168 [00:06<00:22,  5.99it/s, training_loss=0.685]\u001b[A\n",
      "Epoch 1:  22%|███████████▏                                       | 37/168 [00:06<00:21,  6.00it/s, training_loss=0.685]\u001b[A\n",
      "Epoch 1:  22%|███████████▏                                       | 37/168 [00:06<00:21,  6.00it/s, training_loss=0.563]\u001b[A\n",
      "Epoch 1:  23%|███████████▌                                       | 38/168 [00:06<00:21,  5.92it/s, training_loss=0.563]\u001b[A\n",
      "Epoch 1:  23%|███████████▌                                       | 38/168 [00:06<00:21,  5.92it/s, training_loss=0.808]\u001b[A\n",
      "Epoch 1:  23%|███████████▊                                       | 39/168 [00:06<00:21,  5.95it/s, training_loss=0.808]\u001b[A\n",
      "Epoch 1:  23%|███████████▊                                       | 39/168 [00:06<00:21,  5.95it/s, training_loss=0.684]\u001b[A\n",
      "Epoch 1:  24%|████████████▏                                      | 40/168 [00:06<00:21,  5.92it/s, training_loss=0.684]\u001b[A\n",
      "Epoch 1:  24%|████████████▏                                      | 40/168 [00:07<00:21,  5.92it/s, training_loss=0.872]\u001b[A\n",
      "Epoch 1:  24%|████████████▍                                      | 41/168 [00:07<00:21,  5.92it/s, training_loss=0.872]\u001b[A\n",
      "Epoch 1:  24%|████████████▍                                      | 41/168 [00:07<00:21,  5.92it/s, training_loss=0.765]\u001b[A\n",
      "Epoch 1:  25%|████████████▊                                      | 42/168 [00:07<00:21,  5.95it/s, training_loss=0.765]\u001b[A\n",
      "Epoch 1:  25%|████████████▊                                      | 42/168 [00:07<00:21,  5.95it/s, training_loss=0.596]\u001b[A\n",
      "Epoch 1:  26%|█████████████                                      | 43/168 [00:07<00:20,  6.00it/s, training_loss=0.596]\u001b[A\n",
      "Epoch 1:  26%|█████████████                                      | 43/168 [00:07<00:20,  6.00it/s, training_loss=0.661]\u001b[A\n",
      "Epoch 1:  26%|█████████████▎                                     | 44/168 [00:07<00:20,  6.03it/s, training_loss=0.661]\u001b[A\n",
      "Epoch 1:  26%|█████████████▎                                     | 44/168 [00:07<00:20,  6.03it/s, training_loss=0.604]\u001b[A\n",
      "Epoch 1:  27%|█████████████▋                                     | 45/168 [00:07<00:20,  5.98it/s, training_loss=0.604]\u001b[A\n",
      "Epoch 1:  27%|█████████████▋                                     | 45/168 [00:07<00:20,  5.98it/s, training_loss=0.629]\u001b[A\n",
      "Epoch 1:  27%|█████████████▉                                     | 46/168 [00:07<00:20,  5.99it/s, training_loss=0.629]\u001b[A\n",
      "Epoch 1:  27%|█████████████▉                                     | 46/168 [00:08<00:20,  5.99it/s, training_loss=0.664]\u001b[A\n",
      "Epoch 1:  28%|██████████████▎                                    | 47/168 [00:08<00:20,  6.02it/s, training_loss=0.664]\u001b[A\n",
      "Epoch 1:  28%|██████████████▎                                    | 47/168 [00:08<00:20,  6.02it/s, training_loss=0.641]\u001b[A\n",
      "Epoch 1:  29%|██████████████▌                                    | 48/168 [00:08<00:19,  6.05it/s, training_loss=0.641]\u001b[A\n",
      "Epoch 1:  29%|██████████████▌                                    | 48/168 [00:08<00:19,  6.05it/s, training_loss=0.635]\u001b[A\n",
      "Epoch 1:  29%|██████████████▉                                    | 49/168 [00:08<00:19,  6.06it/s, training_loss=0.635]\u001b[A\n",
      "Epoch 1:  29%|██████████████▉                                    | 49/168 [00:08<00:19,  6.06it/s, training_loss=0.599]\u001b[A\n",
      "Epoch 1:  30%|███████████████▏                                   | 50/168 [00:08<00:19,  6.07it/s, training_loss=0.599]\u001b[A\n",
      "Epoch 1:  30%|███████████████▏                                   | 50/168 [00:08<00:19,  6.07it/s, training_loss=0.583]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                   | 51/168 [00:08<00:19,  6.08it/s, training_loss=0.583]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                   | 51/168 [00:08<00:19,  6.08it/s, training_loss=0.634]\u001b[A\n",
      "Epoch 1:  31%|███████████████▊                                   | 52/168 [00:08<00:19,  6.10it/s, training_loss=0.634]\u001b[A\n",
      "Epoch 1:  31%|███████████████▊                                   | 52/168 [00:09<00:19,  6.10it/s, training_loss=0.531]\u001b[A\n",
      "Epoch 1:  32%|████████████████                                   | 53/168 [00:09<00:18,  6.10it/s, training_loss=0.531]\u001b[A\n",
      "Epoch 1:  32%|████████████████                                   | 53/168 [00:09<00:18,  6.10it/s, training_loss=0.637]\u001b[A\n",
      "Epoch 1:  32%|████████████████▍                                  | 54/168 [00:09<00:18,  6.06it/s, training_loss=0.637]\u001b[A\n",
      "Epoch 1:  32%|████████████████▍                                  | 54/168 [00:09<00:18,  6.06it/s, training_loss=0.658]\u001b[A\n",
      "Epoch 1:  33%|████████████████▋                                  | 55/168 [00:09<00:18,  6.07it/s, training_loss=0.658]\u001b[A\n",
      "Epoch 1:  33%|████████████████▋                                  | 55/168 [00:09<00:18,  6.07it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  33%|█████████████████                                  | 56/168 [00:09<00:18,  6.05it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  33%|█████████████████                                  | 56/168 [00:09<00:18,  6.05it/s, training_loss=0.594]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▎                                 | 57/168 [00:09<00:18,  6.04it/s, training_loss=0.594]\u001b[A\n",
      "Epoch 1:  34%|█████████████████▎                                 | 57/168 [00:09<00:18,  6.04it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▌                                 | 58/168 [00:09<00:18,  6.05it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▌                                 | 58/168 [00:10<00:18,  6.05it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▉                                 | 59/168 [00:10<00:17,  6.06it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▉                                 | 59/168 [00:10<00:17,  6.06it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▏                                | 60/168 [00:10<00:17,  6.04it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▏                                | 60/168 [00:10<00:17,  6.04it/s, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▌                                | 61/168 [00:10<00:17,  5.99it/s, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▌                                | 61/168 [00:10<00:17,  5.99it/s, training_loss=0.436]\u001b[A\n",
      "Epoch 1:  37%|██████████████████▊                                | 62/168 [00:10<00:17,  6.00it/s, training_loss=0.436]\u001b[A\n",
      "Epoch 1:  37%|██████████████████▊                                | 62/168 [00:10<00:17,  6.00it/s, training_loss=0.549]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▏                               | 63/168 [00:10<00:17,  6.02it/s, training_loss=0.549]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▏                               | 63/168 [00:10<00:17,  6.02it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▍                               | 64/168 [00:10<00:17,  6.00it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▍                               | 64/168 [00:11<00:17,  6.00it/s, training_loss=0.635]\u001b[A\n",
      "Epoch 1:  39%|███████████████████▋                               | 65/168 [00:11<00:17,  6.01it/s, training_loss=0.635]\u001b[A\n",
      "Epoch 1:  39%|███████████████████▋                               | 65/168 [00:11<00:17,  6.01it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  39%|████████████████████                               | 66/168 [00:11<00:16,  6.00it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  39%|████████████████████                               | 66/168 [00:11<00:16,  6.00it/s, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▎                              | 67/168 [00:11<00:16,  5.99it/s, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▎                              | 67/168 [00:11<00:16,  5.99it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▋                              | 68/168 [00:11<00:16,  6.00it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▋                              | 68/168 [00:11<00:16,  6.00it/s, training_loss=0.623]\u001b[A\n",
      "Epoch 1:  41%|████████████████████▉                              | 69/168 [00:11<00:16,  6.02it/s, training_loss=0.623]\u001b[A\n",
      "Epoch 1:  41%|████████████████████▉                              | 69/168 [00:11<00:16,  6.02it/s, training_loss=0.636]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▎                             | 70/168 [00:11<00:16,  5.93it/s, training_loss=0.636]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▎                             | 70/168 [00:12<00:16,  5.93it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▌                             | 71/168 [00:12<00:16,  5.92it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▌                             | 71/168 [00:12<00:16,  5.92it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  43%|█████████████████████▊                             | 72/168 [00:12<00:16,  5.95it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  43%|█████████████████████▊                             | 72/168 [00:12<00:16,  5.95it/s, training_loss=0.598]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▏                            | 73/168 [00:12<00:15,  5.97it/s, training_loss=0.598]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▏                            | 73/168 [00:12<00:15,  5.97it/s, training_loss=0.582]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▍                            | 74/168 [00:12<00:15,  6.00it/s, training_loss=0.582]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▍                            | 74/168 [00:12<00:15,  6.00it/s, training_loss=0.409]\u001b[A\n",
      "Epoch 1:  45%|██████████████████████▊                            | 75/168 [00:12<00:15,  6.01it/s, training_loss=0.409]\u001b[A\n",
      "Epoch 1:  45%|██████████████████████▊                            | 75/168 [00:12<00:15,  6.01it/s, training_loss=0.514]\u001b[A\n",
      "Epoch 1:  45%|███████████████████████                            | 76/168 [00:12<00:15,  6.02it/s, training_loss=0.514]\u001b[A\n",
      "Epoch 1:  45%|███████████████████████                            | 76/168 [00:13<00:15,  6.02it/s, training_loss=0.736]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▍                           | 77/168 [00:13<00:15,  6.04it/s, training_loss=0.736]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▍                           | 77/168 [00:13<00:15,  6.04it/s, training_loss=0.620]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▋                           | 78/168 [00:13<00:14,  6.04it/s, training_loss=0.620]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▋                           | 78/168 [00:13<00:14,  6.04it/s, training_loss=0.581]\u001b[A\n",
      "Epoch 1:  47%|███████████████████████▉                           | 79/168 [00:13<00:14,  6.04it/s, training_loss=0.581]\u001b[A\n",
      "Epoch 1:  47%|███████████████████████▉                           | 79/168 [00:13<00:14,  6.04it/s, training_loss=0.535]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████▎                          | 80/168 [00:13<00:14,  6.06it/s, training_loss=0.535]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████▎                          | 80/168 [00:13<00:14,  6.06it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████▌                          | 81/168 [00:13<00:14,  6.07it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████▌                          | 81/168 [00:13<00:14,  6.07it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████▉                          | 82/168 [00:13<00:14,  6.06it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████▉                          | 82/168 [00:14<00:14,  6.06it/s, training_loss=0.525]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▏                         | 83/168 [00:14<00:14,  5.99it/s, training_loss=0.525]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▏                         | 83/168 [00:14<00:14,  5.99it/s, training_loss=0.479]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████▌                         | 84/168 [00:14<00:14,  5.96it/s, training_loss=0.479]\u001b[A\n",
      "Epoch 1:  50%|█████████████████████████▌                         | 84/168 [00:14<00:14,  5.96it/s, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████▊                         | 85/168 [00:14<00:13,  6.00it/s, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████▊                         | 85/168 [00:14<00:13,  6.00it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████                         | 86/168 [00:14<00:13,  6.03it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████                         | 86/168 [00:14<00:13,  6.03it/s, training_loss=0.561]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████▍                        | 87/168 [00:14<00:13,  6.05it/s, training_loss=0.561]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████▍                        | 87/168 [00:14<00:13,  6.05it/s, training_loss=0.521]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████▋                        | 88/168 [00:14<00:13,  6.08it/s, training_loss=0.521]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████▋                        | 88/168 [00:15<00:13,  6.08it/s, training_loss=0.474]\u001b[A\n",
      "Epoch 1:  53%|███████████████████████████                        | 89/168 [00:15<00:12,  6.08it/s, training_loss=0.474]\u001b[A\n",
      "Epoch 1:  53%|███████████████████████████                        | 89/168 [00:15<00:12,  6.08it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▎                       | 90/168 [00:15<00:12,  6.08it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▎                       | 90/168 [00:15<00:12,  6.08it/s, training_loss=0.527]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▌                       | 91/168 [00:15<00:12,  6.10it/s, training_loss=0.527]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████▌                       | 91/168 [00:15<00:12,  6.10it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████▉                       | 92/168 [00:15<00:12,  6.10it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████▉                       | 92/168 [00:15<00:12,  6.10it/s, training_loss=0.481]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████▏                      | 93/168 [00:15<00:12,  6.07it/s, training_loss=0.481]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████▏                      | 93/168 [00:15<00:12,  6.07it/s, training_loss=0.841]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████▌                      | 94/168 [00:15<00:12,  6.06it/s, training_loss=0.841]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████▌                      | 94/168 [00:16<00:12,  6.06it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████▊                      | 95/168 [00:16<00:12,  6.03it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████▊                      | 95/168 [00:16<00:12,  6.03it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▏                     | 96/168 [00:16<00:11,  6.04it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▏                     | 96/168 [00:16<00:11,  6.04it/s, training_loss=0.429]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▍                     | 97/168 [00:16<00:11,  6.04it/s, training_loss=0.429]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▍                     | 97/168 [00:16<00:11,  6.04it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▊                     | 98/168 [00:16<00:11,  6.06it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████▊                     | 98/168 [00:16<00:11,  6.06it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████                     | 99/168 [00:16<00:11,  6.04it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████                     | 99/168 [00:16<00:11,  6.04it/s, training_loss=0.524]\u001b[A\n",
      "Epoch 1:  60%|█████████████████████████████▊                    | 100/168 [00:16<00:11,  6.04it/s, training_loss=0.524]\u001b[A\n",
      "Epoch 1:  60%|█████████████████████████████▊                    | 100/168 [00:17<00:11,  6.04it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████                    | 101/168 [00:17<00:11,  6.02it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████                    | 101/168 [00:17<00:11,  6.02it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  61%|██████████████████████████████▎                   | 102/168 [00:17<00:10,  6.05it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  61%|██████████████████████████████▎                   | 102/168 [00:17<00:10,  6.05it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  61%|██████████████████████████████▋                   | 103/168 [00:17<00:10,  6.05it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  61%|██████████████████████████████▋                   | 103/168 [00:17<00:10,  6.05it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  62%|██████████████████████████████▉                   | 104/168 [00:17<00:10,  6.05it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  62%|██████████████████████████████▉                   | 104/168 [00:17<00:10,  6.05it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████▎                  | 105/168 [00:17<00:10,  6.01it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████▎                  | 105/168 [00:17<00:10,  6.01it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  63%|███████████████████████████████▌                  | 106/168 [00:17<00:10,  5.91it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  63%|███████████████████████████████▌                  | 106/168 [00:18<00:10,  5.91it/s, training_loss=0.479]\u001b[A\n",
      "Epoch 1:  64%|███████████████████████████████▊                  | 107/168 [00:18<00:10,  5.93it/s, training_loss=0.479]\u001b[A\n",
      "Epoch 1:  64%|███████████████████████████████▊                  | 107/168 [00:18<00:10,  5.93it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████▏                 | 108/168 [00:18<00:10,  5.91it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████▏                 | 108/168 [00:18<00:10,  5.91it/s, training_loss=0.558]\u001b[A\n",
      "Epoch 1:  65%|████████████████████████████████▍                 | 109/168 [00:18<00:09,  5.91it/s, training_loss=0.558]\u001b[A\n",
      "Epoch 1:  65%|████████████████████████████████▍                 | 109/168 [00:18<00:09,  5.91it/s, training_loss=0.418]\u001b[A\n",
      "Epoch 1:  65%|████████████████████████████████▋                 | 110/168 [00:18<00:09,  5.95it/s, training_loss=0.418]\u001b[A\n",
      "Epoch 1:  65%|████████████████████████████████▋                 | 110/168 [00:18<00:09,  5.95it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████                 | 111/168 [00:18<00:09,  5.99it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████████████████                 | 111/168 [00:18<00:09,  5.99it/s, training_loss=0.417]\u001b[A\n",
      "Epoch 1:  67%|█████████████████████████████████▎                | 112/168 [00:18<00:09,  5.94it/s, training_loss=0.417]\u001b[A\n",
      "Epoch 1:  67%|█████████████████████████████████▎                | 112/168 [00:19<00:09,  5.94it/s, training_loss=0.495]\u001b[A\n",
      "Epoch 1:  67%|█████████████████████████████████▋                | 113/168 [00:19<00:09,  5.92it/s, training_loss=0.495]\u001b[A\n",
      "Epoch 1:  67%|█████████████████████████████████▋                | 113/168 [00:19<00:09,  5.92it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████▉                | 114/168 [00:19<00:09,  5.96it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████▉                | 114/168 [00:19<00:09,  5.96it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████████████████▏               | 115/168 [00:19<00:08,  6.01it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████████████████▏               | 115/168 [00:19<00:08,  6.01it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████▌               | 116/168 [00:19<00:08,  6.05it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████▌               | 116/168 [00:19<00:08,  6.05it/s, training_loss=0.418]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████▊               | 117/168 [00:19<00:08,  6.01it/s, training_loss=0.418]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████▊               | 117/168 [00:19<00:08,  6.01it/s, training_loss=0.475]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████████████████               | 118/168 [00:19<00:08,  5.97it/s, training_loss=0.475]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████████████████               | 118/168 [00:20<00:08,  5.97it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████▍              | 119/168 [00:20<00:08,  6.00it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████▍              | 119/168 [00:20<00:08,  6.00it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████▋              | 120/168 [00:20<00:08,  5.90it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████▋              | 120/168 [00:20<00:08,  5.90it/s, training_loss=0.189]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████              | 121/168 [00:20<00:07,  5.91it/s, training_loss=0.189]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████              | 121/168 [00:20<00:07,  5.91it/s, training_loss=0.465]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████▎             | 122/168 [00:20<00:07,  5.96it/s, training_loss=0.465]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████▎             | 122/168 [00:20<00:07,  5.96it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████▌             | 123/168 [00:20<00:07,  6.01it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████▌             | 123/168 [00:20<00:07,  6.01it/s, training_loss=0.253]\u001b[A\n",
      "Epoch 1:  74%|████████████████████████████████████▉             | 124/168 [00:20<00:07,  5.93it/s, training_loss=0.253]\u001b[A\n",
      "Epoch 1:  74%|████████████████████████████████████▉             | 124/168 [00:21<00:07,  5.93it/s, training_loss=0.483]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████▏            | 125/168 [00:21<00:07,  5.93it/s, training_loss=0.483]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████▏            | 125/168 [00:21<00:07,  5.93it/s, training_loss=0.546]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████▌            | 126/168 [00:21<00:07,  5.86it/s, training_loss=0.546]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████▌            | 126/168 [00:21<00:07,  5.86it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 1:  76%|█████████████████████████████████████▊            | 127/168 [00:21<00:06,  5.87it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 1:  76%|█████████████████████████████████████▊            | 127/168 [00:21<00:06,  5.87it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████            | 128/168 [00:21<00:06,  5.93it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████            | 128/168 [00:21<00:06,  5.93it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████▍           | 129/168 [00:21<00:06,  5.98it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████▍           | 129/168 [00:21<00:06,  5.98it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████▋           | 130/168 [00:21<00:06,  6.02it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 1:  77%|██████████████████████████████████████▋           | 130/168 [00:22<00:06,  6.02it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  78%|██████████████████████████████████████▉           | 131/168 [00:22<00:06,  6.05it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  78%|██████████████████████████████████████▉           | 131/168 [00:22<00:06,  6.05it/s, training_loss=0.463]\u001b[A\n",
      "Epoch 1:  79%|███████████████████████████████████████▎          | 132/168 [00:22<00:05,  6.06it/s, training_loss=0.463]\u001b[A\n",
      "Epoch 1:  79%|███████████████████████████████████████▎          | 132/168 [00:22<00:05,  6.06it/s, training_loss=0.516]\u001b[A\n",
      "Epoch 1:  79%|███████████████████████████████████████▌          | 133/168 [00:22<00:05,  6.05it/s, training_loss=0.516]\u001b[A\n",
      "Epoch 1:  79%|███████████████████████████████████████▌          | 133/168 [00:22<00:05,  6.05it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 1:  80%|███████████████████████████████████████▉          | 134/168 [00:22<00:05,  5.97it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 1:  80%|███████████████████████████████████████▉          | 134/168 [00:22<00:05,  5.97it/s, training_loss=0.442]\u001b[A\n",
      "Epoch 1:  80%|████████████████████████████████████████▏         | 135/168 [00:22<00:05,  5.95it/s, training_loss=0.442]\u001b[A\n",
      "Epoch 1:  80%|████████████████████████████████████████▏         | 135/168 [00:22<00:05,  5.95it/s, training_loss=0.429]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████▍         | 136/168 [00:22<00:05,  6.00it/s, training_loss=0.429]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████▍         | 136/168 [00:23<00:05,  6.00it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 1:  82%|████████████████████████████████████████▊         | 137/168 [00:23<00:05,  5.94it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 1:  82%|████████████████████████████████████████▊         | 137/168 [00:23<00:05,  5.94it/s, training_loss=0.539]\u001b[A\n",
      "Epoch 1:  82%|█████████████████████████████████████████         | 138/168 [00:23<00:05,  5.91it/s, training_loss=0.539]\u001b[A\n",
      "Epoch 1:  82%|█████████████████████████████████████████         | 138/168 [00:23<00:05,  5.91it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████▎        | 139/168 [00:23<00:04,  5.96it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████▎        | 139/168 [00:23<00:04,  5.96it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████▋        | 140/168 [00:23<00:04,  6.00it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████▋        | 140/168 [00:23<00:04,  6.00it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  84%|█████████████████████████████████████████▉        | 141/168 [00:23<00:04,  6.03it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  84%|█████████████████████████████████████████▉        | 141/168 [00:23<00:04,  6.03it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████▎       | 142/168 [00:23<00:04,  5.94it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████▎       | 142/168 [00:24<00:04,  5.94it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████▌       | 143/168 [00:24<00:04,  5.87it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████▌       | 143/168 [00:24<00:04,  5.87it/s, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████▊       | 144/168 [00:24<00:04,  5.91it/s, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████▊       | 144/168 [00:24<00:04,  5.91it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  86%|███████████████████████████████████████████▏      | 145/168 [00:24<00:03,  5.96it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  86%|███████████████████████████████████████████▏      | 145/168 [00:24<00:03,  5.96it/s, training_loss=0.461]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▍      | 146/168 [00:24<00:03,  5.92it/s, training_loss=0.461]\u001b[A\n",
      "Epoch 1:  87%|███████████████████████████████████████████▍      | 146/168 [00:24<00:03,  5.92it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████▊      | 147/168 [00:24<00:03,  5.89it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████▊      | 147/168 [00:24<00:03,  5.89it/s, training_loss=0.574]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████      | 148/168 [00:24<00:03,  5.94it/s, training_loss=0.574]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████      | 148/168 [00:25<00:03,  5.94it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████▎     | 149/168 [00:25<00:03,  5.97it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████▎     | 149/168 [00:25<00:03,  5.97it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████▋     | 150/168 [00:25<00:03,  5.88it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████▋     | 150/168 [00:25<00:03,  5.88it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████▉     | 151/168 [00:25<00:02,  5.92it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████▉     | 151/168 [00:25<00:02,  5.92it/s, training_loss=0.408]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████████████████████▏    | 152/168 [00:25<00:02,  5.97it/s, training_loss=0.408]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████████████████████▏    | 152/168 [00:25<00:02,  5.97it/s, training_loss=0.413]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████▌    | 153/168 [00:25<00:02,  6.00it/s, training_loss=0.413]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████▌    | 153/168 [00:25<00:02,  6.00it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  92%|█████████████████████████████████████████████▊    | 154/168 [00:25<00:02,  6.02it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  92%|█████████████████████████████████████████████▊    | 154/168 [00:26<00:02,  6.02it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████▏   | 155/168 [00:26<00:02,  6.04it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████▏   | 155/168 [00:26<00:02,  6.04it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▍   | 156/168 [00:26<00:01,  6.05it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▍   | 156/168 [00:26<00:01,  6.05it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▋   | 157/168 [00:26<00:01,  6.06it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████▋   | 157/168 [00:26<00:01,  6.06it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████   | 158/168 [00:26<00:01,  6.07it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████   | 158/168 [00:26<00:01,  6.07it/s, training_loss=0.421]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████▎  | 159/168 [00:26<00:01,  6.08it/s, training_loss=0.421]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████▎  | 159/168 [00:26<00:01,  6.08it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████▌  | 160/168 [00:26<00:01,  6.06it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████▌  | 160/168 [00:27<00:01,  6.06it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  96%|███████████████████████████████████████████████▉  | 161/168 [00:27<00:01,  6.08it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  96%|███████████████████████████████████████████████▉  | 161/168 [00:27<00:01,  6.08it/s, training_loss=0.208]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████▏ | 162/168 [00:27<00:00,  6.09it/s, training_loss=0.208]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████▏ | 162/168 [00:27<00:00,  6.09it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▌ | 163/168 [00:27<00:00,  6.09it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████▌ | 163/168 [00:27<00:00,  6.09it/s, training_loss=0.223]\u001b[A\n",
      "Epoch 1:  98%|████████████████████████████████████████████████▊ | 164/168 [00:27<00:00,  6.07it/s, training_loss=0.223]\u001b[A\n",
      "Epoch 1:  98%|████████████████████████████████████████████████▊ | 164/168 [00:27<00:00,  6.07it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  98%|█████████████████████████████████████████████████ | 165/168 [00:27<00:00,  6.08it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  98%|█████████████████████████████████████████████████ | 165/168 [00:27<00:00,  6.08it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▍| 166/168 [00:27<00:00,  6.08it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▍| 166/168 [00:28<00:00,  6.08it/s, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▋| 167/168 [00:28<00:00,  6.09it/s, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████▋| 167/168 [00:28<00:00,  6.09it/s, training_loss=0.475]\u001b[A\n",
      "Epoch Progress:  25%|█████████████████                                                   | 1/4 [00:28<01:24, 28.09s/it]\u001b[A\n",
      "Epoch 2:   0%|                                                                                 | 0/168 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:   0%|                                                            | 0/168 [00:00<?, ?it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 2:   1%|▎                                                   | 1/168 [00:00<00:27,  6.15it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 2:   1%|▎                                                   | 1/168 [00:00<00:27,  6.15it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 2:   1%|▌                                                   | 2/168 [00:00<00:27,  6.11it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 2:   1%|▌                                                   | 2/168 [00:00<00:27,  6.11it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 2:   2%|▉                                                   | 3/168 [00:00<00:27,  6.08it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 2:   2%|▉                                                   | 3/168 [00:00<00:27,  6.08it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 2:   2%|█▏                                                  | 4/168 [00:00<00:26,  6.09it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 2:   2%|█▏                                                  | 4/168 [00:00<00:26,  6.09it/s, training_loss=0.222]\u001b[A\n",
      "Epoch 2:   3%|█▌                                                  | 5/168 [00:00<00:26,  6.09it/s, training_loss=0.222]\u001b[A\n",
      "Epoch 2:   3%|█▌                                                  | 5/168 [00:00<00:26,  6.09it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 2:   4%|█▊                                                  | 6/168 [00:00<00:26,  6.07it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 2:   4%|█▊                                                  | 6/168 [00:01<00:26,  6.07it/s, training_loss=0.183]\u001b[A\n",
      "Epoch 2:   4%|██▏                                                 | 7/168 [00:01<00:26,  6.07it/s, training_loss=0.183]\u001b[A\n",
      "Epoch 2:   4%|██▏                                                 | 7/168 [00:01<00:26,  6.07it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 2:   5%|██▍                                                 | 8/168 [00:01<00:26,  6.02it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 2:   5%|██▍                                                 | 8/168 [00:01<00:26,  6.02it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:   5%|██▊                                                 | 9/168 [00:01<00:26,  5.97it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:   5%|██▊                                                 | 9/168 [00:01<00:26,  5.97it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 2:   6%|███                                                | 10/168 [00:01<00:26,  6.00it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 2:   6%|███                                                | 10/168 [00:01<00:26,  6.00it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 2:   7%|███▎                                               | 11/168 [00:01<00:25,  6.07it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 2:   7%|███▎                                               | 11/168 [00:01<00:25,  6.07it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 2:   7%|███▋                                               | 12/168 [00:01<00:25,  6.03it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 2:   7%|███▋                                               | 12/168 [00:02<00:25,  6.03it/s, training_loss=0.148]\u001b[A\n",
      "Epoch 2:   8%|███▉                                               | 13/168 [00:02<00:25,  6.02it/s, training_loss=0.148]\u001b[A\n",
      "Epoch 2:   8%|███▉                                               | 13/168 [00:02<00:25,  6.02it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 2:   8%|████▎                                              | 14/168 [00:02<00:25,  6.04it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 2:   8%|████▎                                              | 14/168 [00:02<00:25,  6.04it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 2:   9%|████▌                                              | 15/168 [00:02<00:25,  6.04it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 2:   9%|████▌                                              | 15/168 [00:02<00:25,  6.04it/s, training_loss=0.158]\u001b[A\n",
      "Epoch 2:  10%|████▊                                              | 16/168 [00:02<00:25,  6.03it/s, training_loss=0.158]\u001b[A\n",
      "Epoch 2:  10%|████▊                                              | 16/168 [00:02<00:25,  6.03it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  10%|█████▏                                             | 17/168 [00:02<00:25,  6.03it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 2:  10%|█████▏                                             | 17/168 [00:02<00:25,  6.03it/s, training_loss=0.134]\u001b[A\n",
      "Epoch 2:  11%|█████▍                                             | 18/168 [00:02<00:24,  6.03it/s, training_loss=0.134]\u001b[A\n",
      "Epoch 2:  11%|█████▍                                             | 18/168 [00:03<00:24,  6.03it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 2:  11%|█████▊                                             | 19/168 [00:03<00:24,  6.04it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 2:  11%|█████▊                                             | 19/168 [00:03<00:24,  6.04it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 2:  12%|██████                                             | 20/168 [00:03<00:24,  6.04it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 2:  12%|██████                                             | 20/168 [00:03<00:24,  6.04it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 2:  12%|██████▍                                            | 21/168 [00:03<00:24,  6.04it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 2:  12%|██████▍                                            | 21/168 [00:03<00:24,  6.04it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 2:  13%|██████▋                                            | 22/168 [00:03<00:24,  6.04it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 2:  13%|██████▋                                            | 22/168 [00:03<00:24,  6.04it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  14%|██████▉                                            | 23/168 [00:03<00:24,  6.03it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  14%|██████▉                                            | 23/168 [00:03<00:24,  6.03it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 2:  14%|███████▎                                           | 24/168 [00:03<00:23,  6.03it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 2:  14%|███████▎                                           | 24/168 [00:04<00:23,  6.03it/s, training_loss=0.176]\u001b[A\n",
      "Epoch 2:  15%|███████▌                                           | 25/168 [00:04<00:23,  6.05it/s, training_loss=0.176]\u001b[A\n",
      "Epoch 2:  15%|███████▌                                           | 25/168 [00:04<00:23,  6.05it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 2:  15%|███████▉                                           | 26/168 [00:04<00:23,  6.05it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 2:  15%|███████▉                                           | 26/168 [00:04<00:23,  6.05it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 2:  16%|████████▏                                          | 27/168 [00:04<00:23,  6.04it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 2:  16%|████████▏                                          | 27/168 [00:04<00:23,  6.04it/s, training_loss=0.172]\u001b[A\n",
      "Epoch 2:  17%|████████▌                                          | 28/168 [00:04<00:23,  6.04it/s, training_loss=0.172]\u001b[A\n",
      "Epoch 2:  17%|████████▌                                          | 28/168 [00:04<00:23,  6.04it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  17%|████████▊                                          | 29/168 [00:04<00:22,  6.05it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  17%|████████▊                                          | 29/168 [00:04<00:22,  6.05it/s, training_loss=0.171]\u001b[A\n",
      "Epoch 2:  18%|█████████                                          | 30/168 [00:04<00:22,  6.05it/s, training_loss=0.171]\u001b[A\n",
      "Epoch 2:  18%|█████████                                          | 30/168 [00:05<00:22,  6.05it/s, training_loss=0.143]\u001b[A\n",
      "Epoch 2:  18%|█████████▍                                         | 31/168 [00:05<00:22,  6.02it/s, training_loss=0.143]\u001b[A\n",
      "Epoch 2:  18%|█████████▍                                         | 31/168 [00:05<00:22,  6.02it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 2:  19%|█████████▋                                         | 32/168 [00:05<00:22,  6.02it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 2:  19%|█████████▋                                         | 32/168 [00:05<00:22,  6.02it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 2:  20%|██████████                                         | 33/168 [00:05<00:22,  6.04it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 2:  20%|██████████                                         | 33/168 [00:05<00:22,  6.04it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 2:  20%|██████████▎                                        | 34/168 [00:05<00:22,  6.02it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 2:  20%|██████████▎                                        | 34/168 [00:05<00:22,  6.02it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 2:  21%|██████████▋                                        | 35/168 [00:05<00:22,  6.03it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 2:  21%|██████████▋                                        | 35/168 [00:05<00:22,  6.03it/s, training_loss=0.157]\u001b[A\n",
      "Epoch 2:  21%|██████████▉                                        | 36/168 [00:05<00:21,  6.06it/s, training_loss=0.157]\u001b[A\n",
      "Epoch 2:  21%|██████████▉                                        | 36/168 [00:06<00:21,  6.06it/s, training_loss=0.224]\u001b[A\n",
      "Epoch 2:  22%|███████████▏                                       | 37/168 [00:06<00:21,  6.04it/s, training_loss=0.224]\u001b[A\n",
      "Epoch 2:  22%|███████████▏                                       | 37/168 [00:06<00:21,  6.04it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 2:  23%|███████████▌                                       | 38/168 [00:06<00:21,  6.04it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 2:  23%|███████████▌                                       | 38/168 [00:06<00:21,  6.04it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 2:  23%|███████████▊                                       | 39/168 [00:06<00:21,  6.05it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 2:  23%|███████████▊                                       | 39/168 [00:06<00:21,  6.05it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 2:  24%|████████████▏                                      | 40/168 [00:06<00:21,  6.05it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 2:  24%|████████████▏                                      | 40/168 [00:06<00:21,  6.05it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 2:  24%|████████████▍                                      | 41/168 [00:06<00:21,  6.03it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 2:  24%|████████████▍                                      | 41/168 [00:06<00:21,  6.03it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  25%|████████████▊                                      | 42/168 [00:06<00:20,  6.04it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  25%|████████████▊                                      | 42/168 [00:07<00:20,  6.04it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 2:  26%|█████████████                                      | 43/168 [00:07<00:20,  6.06it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 2:  26%|█████████████                                      | 43/168 [00:07<00:20,  6.06it/s, training_loss=0.445]\u001b[A\n",
      "Epoch 2:  26%|█████████████▎                                     | 44/168 [00:07<00:20,  6.04it/s, training_loss=0.445]\u001b[A\n",
      "Epoch 2:  26%|█████████████▎                                     | 44/168 [00:07<00:20,  6.04it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 2:  27%|█████████████▋                                     | 45/168 [00:07<00:20,  6.02it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 2:  27%|█████████████▋                                     | 45/168 [00:07<00:20,  6.02it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 2:  27%|█████████████▉                                     | 46/168 [00:07<00:20,  6.04it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 2:  27%|█████████████▉                                     | 46/168 [00:07<00:20,  6.04it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 2:  28%|██████████████▎                                    | 47/168 [00:07<00:20,  6.04it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 2:  28%|██████████████▎                                    | 47/168 [00:07<00:20,  6.04it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  29%|██████████████▌                                    | 48/168 [00:07<00:19,  6.04it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  29%|██████████████▌                                    | 48/168 [00:08<00:19,  6.04it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  29%|██████████████▉                                    | 49/168 [00:08<00:19,  6.05it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  29%|██████████████▉                                    | 49/168 [00:08<00:19,  6.05it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  30%|███████████████▏                                   | 50/168 [00:08<00:19,  6.04it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  30%|███████████████▏                                   | 50/168 [00:08<00:19,  6.04it/s, training_loss=0.107]\u001b[A\n",
      "Epoch 2:  30%|███████████████▍                                   | 51/168 [00:08<00:19,  6.05it/s, training_loss=0.107]\u001b[A\n",
      "Epoch 2:  30%|███████████████▍                                   | 51/168 [00:08<00:19,  6.05it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 2:  31%|███████████████▊                                   | 52/168 [00:08<00:19,  6.03it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 2:  31%|███████████████▊                                   | 52/168 [00:08<00:19,  6.03it/s, training_loss=0.170]\u001b[A\n",
      "Epoch 2:  32%|████████████████                                   | 53/168 [00:08<00:19,  6.05it/s, training_loss=0.170]\u001b[A\n",
      "Epoch 2:  32%|████████████████                                   | 53/168 [00:08<00:19,  6.05it/s, training_loss=0.179]\u001b[A\n",
      "Epoch 2:  32%|████████████████▍                                  | 54/168 [00:08<00:18,  6.03it/s, training_loss=0.179]\u001b[A\n",
      "Epoch 2:  32%|████████████████▍                                  | 54/168 [00:09<00:18,  6.03it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  33%|████████████████▋                                  | 55/168 [00:09<00:18,  6.05it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  33%|████████████████▋                                  | 55/168 [00:09<00:18,  6.05it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  33%|█████████████████                                  | 56/168 [00:09<00:18,  6.03it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 2:  33%|█████████████████                                  | 56/168 [00:09<00:18,  6.03it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▎                                 | 57/168 [00:09<00:18,  5.94it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  34%|█████████████████▎                                 | 57/168 [00:09<00:18,  5.94it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▌                                 | 58/168 [00:09<00:18,  5.96it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▌                                 | 58/168 [00:09<00:18,  5.96it/s, training_loss=0.209]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▉                                 | 59/168 [00:09<00:18,  5.92it/s, training_loss=0.209]\u001b[A\n",
      "Epoch 2:  35%|█████████████████▉                                 | 59/168 [00:09<00:18,  5.92it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▏                                | 60/168 [00:09<00:18,  5.86it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▏                                | 60/168 [00:10<00:18,  5.86it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▌                                | 61/168 [00:10<00:18,  5.82it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 2:  36%|██████████████████▌                                | 61/168 [00:10<00:18,  5.82it/s, training_loss=0.170]\u001b[A\n",
      "Epoch 2:  37%|██████████████████▊                                | 62/168 [00:10<00:18,  5.80it/s, training_loss=0.170]\u001b[A\n",
      "Epoch 2:  37%|██████████████████▊                                | 62/168 [00:10<00:18,  5.80it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▏                               | 63/168 [00:10<00:17,  5.84it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▏                               | 63/168 [00:10<00:17,  5.84it/s, training_loss=0.178]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▍                               | 64/168 [00:10<00:17,  5.89it/s, training_loss=0.178]\u001b[A\n",
      "Epoch 2:  38%|███████████████████▍                               | 64/168 [00:10<00:17,  5.89it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 2:  39%|███████████████████▋                               | 65/168 [00:10<00:17,  5.92it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 2:  39%|███████████████████▋                               | 65/168 [00:10<00:17,  5.92it/s, training_loss=0.211]\u001b[A\n",
      "Epoch 2:  39%|████████████████████                               | 66/168 [00:10<00:17,  5.93it/s, training_loss=0.211]\u001b[A\n",
      "Epoch 2:  39%|████████████████████                               | 66/168 [00:11<00:17,  5.93it/s, training_loss=0.211]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▎                              | 67/168 [00:11<00:17,  5.85it/s, training_loss=0.211]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▎                              | 67/168 [00:11<00:17,  5.85it/s, training_loss=0.147]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▋                              | 68/168 [00:11<00:17,  5.84it/s, training_loss=0.147]\u001b[A\n",
      "Epoch 2:  40%|████████████████████▋                              | 68/168 [00:11<00:17,  5.84it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:  41%|████████████████████▉                              | 69/168 [00:11<00:16,  5.83it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 2:  41%|████████████████████▉                              | 69/168 [00:11<00:16,  5.83it/s, training_loss=0.192]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▎                             | 70/168 [00:11<00:16,  5.88it/s, training_loss=0.192]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▎                             | 70/168 [00:11<00:16,  5.88it/s, training_loss=0.261]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▌                             | 71/168 [00:11<00:16,  5.92it/s, training_loss=0.261]\u001b[A\n",
      "Epoch 2:  42%|█████████████████████▌                             | 71/168 [00:12<00:16,  5.92it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 2:  43%|█████████████████████▊                             | 72/168 [00:12<00:16,  5.86it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 2:  43%|█████████████████████▊                             | 72/168 [00:12<00:16,  5.86it/s, training_loss=0.128]\u001b[A\n",
      "Epoch 2:  43%|██████████████████████▏                            | 73/168 [00:12<00:16,  5.89it/s, training_loss=0.128]\u001b[A\n",
      "Epoch 2:  43%|██████████████████████▏                            | 73/168 [00:12<00:16,  5.89it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 2:  44%|██████████████████████▍                            | 74/168 [00:12<00:16,  5.87it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 2:  44%|██████████████████████▍                            | 74/168 [00:12<00:16,  5.87it/s, training_loss=0.241]\u001b[A\n",
      "Epoch 2:  45%|██████████████████████▊                            | 75/168 [00:12<00:16,  5.80it/s, training_loss=0.241]\u001b[A\n",
      "Epoch 2:  45%|██████████████████████▊                            | 75/168 [00:12<00:16,  5.80it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  45%|███████████████████████                            | 76/168 [00:12<00:15,  5.84it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  45%|███████████████████████                            | 76/168 [00:12<00:15,  5.84it/s, training_loss=0.118]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▍                           | 77/168 [00:12<00:15,  5.89it/s, training_loss=0.118]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▍                           | 77/168 [00:13<00:15,  5.89it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▋                           | 78/168 [00:13<00:15,  5.86it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 2:  46%|███████████████████████▋                           | 78/168 [00:13<00:15,  5.86it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  47%|███████████████████████▉                           | 79/168 [00:13<00:15,  5.91it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  47%|███████████████████████▉                           | 79/168 [00:13<00:15,  5.91it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████▎                          | 80/168 [00:13<00:14,  5.90it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████▎                          | 80/168 [00:13<00:14,  5.90it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████▌                          | 81/168 [00:13<00:14,  5.85it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 2:  48%|████████████████████████▌                          | 81/168 [00:13<00:14,  5.85it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 2:  49%|████████████████████████▉                          | 82/168 [00:13<00:14,  5.85it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 2:  49%|████████████████████████▉                          | 82/168 [00:13<00:14,  5.85it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  49%|█████████████████████████▏                         | 83/168 [00:13<00:14,  5.84it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 2:  49%|█████████████████████████▏                         | 83/168 [00:14<00:14,  5.84it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 2:  50%|█████████████████████████▌                         | 84/168 [00:14<00:14,  5.82it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 2:  50%|█████████████████████████▌                         | 84/168 [00:14<00:14,  5.82it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  51%|█████████████████████████▊                         | 85/168 [00:14<00:14,  5.91it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 2:  51%|█████████████████████████▊                         | 85/168 [00:14<00:14,  5.91it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 2:  51%|██████████████████████████                         | 86/168 [00:14<00:13,  5.87it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 2:  51%|██████████████████████████                         | 86/168 [00:14<00:13,  5.87it/s, training_loss=0.132]\u001b[A\n",
      "Epoch 2:  52%|██████████████████████████▍                        | 87/168 [00:14<00:13,  5.82it/s, training_loss=0.132]\u001b[A\n",
      "Epoch 2:  52%|██████████████████████████▍                        | 87/168 [00:14<00:13,  5.82it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  52%|██████████████████████████▋                        | 88/168 [00:14<00:13,  5.79it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 2:  52%|██████████████████████████▋                        | 88/168 [00:14<00:13,  5.79it/s, training_loss=0.148]\u001b[A\n",
      "Epoch 2:  53%|███████████████████████████                        | 89/168 [00:14<00:13,  5.81it/s, training_loss=0.148]\u001b[A\n",
      "Epoch 2:  53%|███████████████████████████                        | 89/168 [00:15<00:13,  5.81it/s, training_loss=0.287]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▎                       | 90/168 [00:15<00:13,  5.82it/s, training_loss=0.287]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▎                       | 90/168 [00:15<00:13,  5.82it/s, training_loss=0.124]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▌                       | 91/168 [00:15<00:13,  5.81it/s, training_loss=0.124]\u001b[A\n",
      "Epoch 2:  54%|███████████████████████████▌                       | 91/168 [00:15<00:13,  5.81it/s, training_loss=0.042]\u001b[A\n",
      "Epoch 2:  55%|███████████████████████████▉                       | 92/168 [00:15<00:12,  5.87it/s, training_loss=0.042]\u001b[A\n",
      "Epoch 2:  55%|███████████████████████████▉                       | 92/168 [00:15<00:12,  5.87it/s, training_loss=0.086]\u001b[A\n",
      "Epoch 2:  55%|████████████████████████████▏                      | 93/168 [00:15<00:12,  5.89it/s, training_loss=0.086]\u001b[A\n",
      "Epoch 2:  55%|████████████████████████████▏                      | 93/168 [00:15<00:12,  5.89it/s, training_loss=0.122]\u001b[A\n",
      "Epoch 2:  56%|████████████████████████████▌                      | 94/168 [00:15<00:12,  5.92it/s, training_loss=0.122]\u001b[A\n",
      "Epoch 2:  56%|████████████████████████████▌                      | 94/168 [00:15<00:12,  5.92it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  57%|████████████████████████████▊                      | 95/168 [00:15<00:12,  5.90it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  57%|████████████████████████████▊                      | 95/168 [00:16<00:12,  5.90it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 2:  57%|█████████████████████████████▏                     | 96/168 [00:16<00:12,  5.93it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 2:  57%|█████████████████████████████▏                     | 96/168 [00:16<00:12,  5.93it/s, training_loss=0.190]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▍                     | 97/168 [00:16<00:12,  5.91it/s, training_loss=0.190]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▍                     | 97/168 [00:16<00:12,  5.91it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▊                     | 98/168 [00:16<00:11,  5.87it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 2:  58%|█████████████████████████████▊                     | 98/168 [00:16<00:11,  5.87it/s, training_loss=0.417]\u001b[A\n",
      "Epoch 2:  59%|██████████████████████████████                     | 99/168 [00:16<00:11,  5.90it/s, training_loss=0.417]\u001b[A\n",
      "Epoch 2:  59%|██████████████████████████████                     | 99/168 [00:16<00:11,  5.90it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 2:  60%|█████████████████████████████▊                    | 100/168 [00:16<00:11,  5.93it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 2:  60%|█████████████████████████████▊                    | 100/168 [00:16<00:11,  5.93it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 2:  60%|██████████████████████████████                    | 101/168 [00:16<00:11,  5.92it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 2:  60%|██████████████████████████████                    | 101/168 [00:17<00:11,  5.92it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████▎                   | 102/168 [00:17<00:11,  5.86it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████▎                   | 102/168 [00:17<00:11,  5.86it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████▋                   | 103/168 [00:17<00:11,  5.84it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 2:  61%|██████████████████████████████▋                   | 103/168 [00:17<00:11,  5.84it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 2:  62%|██████████████████████████████▉                   | 104/168 [00:17<00:10,  5.85it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 2:  62%|██████████████████████████████▉                   | 104/168 [00:17<00:10,  5.85it/s, training_loss=0.158]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▎                  | 105/168 [00:17<00:10,  5.87it/s, training_loss=0.158]\u001b[A\n",
      "Epoch 2:  62%|███████████████████████████████▎                  | 105/168 [00:17<00:10,  5.87it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  63%|███████████████████████████████▌                  | 106/168 [00:17<00:10,  5.89it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 2:  63%|███████████████████████████████▌                  | 106/168 [00:17<00:10,  5.89it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  64%|███████████████████████████████▊                  | 107/168 [00:17<00:10,  5.90it/s, training_loss=0.202]\u001b[A\n",
      "Epoch 2:  64%|███████████████████████████████▊                  | 107/168 [00:18<00:10,  5.90it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  64%|████████████████████████████████▏                 | 108/168 [00:18<00:10,  5.90it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  64%|████████████████████████████████▏                 | 108/168 [00:18<00:10,  5.90it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 2:  65%|████████████████████████████████▍                 | 109/168 [00:18<00:10,  5.86it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 2:  65%|████████████████████████████████▍                 | 109/168 [00:18<00:10,  5.86it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 2:  65%|████████████████████████████████▋                 | 110/168 [00:18<00:09,  5.85it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 2:  65%|████████████████████████████████▋                 | 110/168 [00:18<00:09,  5.85it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 2:  66%|█████████████████████████████████                 | 111/168 [00:18<00:09,  5.83it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 2:  66%|█████████████████████████████████                 | 111/168 [00:18<00:09,  5.83it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  67%|█████████████████████████████████▎                | 112/168 [00:18<00:09,  5.86it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 2:  67%|█████████████████████████████████▎                | 112/168 [00:18<00:09,  5.86it/s, training_loss=0.199]\u001b[A\n",
      "Epoch 2:  67%|█████████████████████████████████▋                | 113/168 [00:18<00:09,  5.89it/s, training_loss=0.199]\u001b[A\n",
      "Epoch 2:  67%|█████████████████████████████████▋                | 113/168 [00:19<00:09,  5.89it/s, training_loss=0.209]\u001b[A\n",
      "Epoch 2:  68%|█████████████████████████████████▉                | 114/168 [00:19<00:09,  5.91it/s, training_loss=0.209]\u001b[A\n",
      "Epoch 2:  68%|█████████████████████████████████▉                | 114/168 [00:19<00:09,  5.91it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 2:  68%|██████████████████████████████████▏               | 115/168 [00:19<00:08,  5.90it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 2:  68%|██████████████████████████████████▏               | 115/168 [00:19<00:08,  5.90it/s, training_loss=0.079]\u001b[A\n",
      "Epoch 2:  69%|██████████████████████████████████▌               | 116/168 [00:19<00:08,  5.88it/s, training_loss=0.079]\u001b[A\n",
      "Epoch 2:  69%|██████████████████████████████████▌               | 116/168 [00:19<00:08,  5.88it/s, training_loss=0.174]\u001b[A\n",
      "Epoch 2:  70%|██████████████████████████████████▊               | 117/168 [00:19<00:08,  5.89it/s, training_loss=0.174]\u001b[A\n",
      "Epoch 2:  70%|██████████████████████████████████▊               | 117/168 [00:19<00:08,  5.89it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 2:  70%|███████████████████████████████████               | 118/168 [00:19<00:08,  5.90it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 2:  70%|███████████████████████████████████               | 118/168 [00:20<00:08,  5.90it/s, training_loss=0.254]\u001b[A\n",
      "Epoch 2:  71%|███████████████████████████████████▍              | 119/168 [00:20<00:08,  5.91it/s, training_loss=0.254]\u001b[A\n",
      "Epoch 2:  71%|███████████████████████████████████▍              | 119/168 [00:20<00:08,  5.91it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  71%|███████████████████████████████████▋              | 120/168 [00:20<00:08,  5.85it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  71%|███████████████████████████████████▋              | 120/168 [00:20<00:08,  5.85it/s, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  72%|████████████████████████████████████              | 121/168 [00:20<00:08,  5.80it/s, training_loss=0.235]\u001b[A\n",
      "Epoch 2:  72%|████████████████████████████████████              | 121/168 [00:20<00:08,  5.80it/s, training_loss=0.158]\u001b[A\n",
      "Epoch 2:  73%|████████████████████████████████████▎             | 122/168 [00:20<00:07,  5.81it/s, training_loss=0.158]\u001b[A\n",
      "Epoch 2:  73%|████████████████████████████████████▎             | 122/168 [00:20<00:07,  5.81it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 2:  73%|████████████████████████████████████▌             | 123/168 [00:20<00:07,  5.85it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 2:  73%|████████████████████████████████████▌             | 123/168 [00:20<00:07,  5.85it/s, training_loss=0.196]\u001b[A\n",
      "Epoch 2:  74%|████████████████████████████████████▉             | 124/168 [00:20<00:07,  5.79it/s, training_loss=0.196]\u001b[A\n",
      "Epoch 2:  74%|████████████████████████████████████▉             | 124/168 [00:21<00:07,  5.79it/s, training_loss=0.094]\u001b[A\n",
      "Epoch 2:  74%|█████████████████████████████████████▏            | 125/168 [00:21<00:07,  5.81it/s, training_loss=0.094]\u001b[A\n",
      "Epoch 2:  74%|█████████████████████████████████████▏            | 125/168 [00:21<00:07,  5.81it/s, training_loss=0.215]\u001b[A\n",
      "Epoch 2:  75%|█████████████████████████████████████▌            | 126/168 [00:21<00:07,  5.82it/s, training_loss=0.215]\u001b[A\n",
      "Epoch 2:  75%|█████████████████████████████████████▌            | 126/168 [00:21<00:07,  5.82it/s, training_loss=0.084]\u001b[A\n",
      "Epoch 2:  76%|█████████████████████████████████████▊            | 127/168 [00:21<00:07,  5.82it/s, training_loss=0.084]\u001b[A\n",
      "Epoch 2:  76%|█████████████████████████████████████▊            | 127/168 [00:21<00:07,  5.82it/s, training_loss=0.092]\u001b[A\n",
      "Epoch 2:  76%|██████████████████████████████████████            | 128/168 [00:21<00:06,  5.83it/s, training_loss=0.092]\u001b[A\n",
      "Epoch 2:  76%|██████████████████████████████████████            | 128/168 [00:21<00:06,  5.83it/s, training_loss=0.064]\u001b[A\n",
      "Epoch 2:  77%|██████████████████████████████████████▍           | 129/168 [00:21<00:06,  5.83it/s, training_loss=0.064]\u001b[A\n",
      "Epoch 2:  77%|██████████████████████████████████████▍           | 129/168 [00:21<00:06,  5.83it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 2:  77%|██████████████████████████████████████▋           | 130/168 [00:21<00:06,  5.86it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 2:  77%|██████████████████████████████████████▋           | 130/168 [00:22<00:06,  5.86it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 2:  78%|██████████████████████████████████████▉           | 131/168 [00:22<00:06,  5.81it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 2:  78%|██████████████████████████████████████▉           | 131/168 [00:22<00:06,  5.81it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 2:  79%|███████████████████████████████████████▎          | 132/168 [00:22<00:06,  5.80it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 2:  79%|███████████████████████████████████████▎          | 132/168 [00:22<00:06,  5.80it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 2:  79%|███████████████████████████████████████▌          | 133/168 [00:22<00:06,  5.78it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 2:  79%|███████████████████████████████████████▌          | 133/168 [00:22<00:06,  5.78it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 2:  80%|███████████████████████████████████████▉          | 134/168 [00:22<00:05,  5.73it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 2:  80%|███████████████████████████████████████▉          | 134/168 [00:22<00:05,  5.73it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 2:  80%|████████████████████████████████████████▏         | 135/168 [00:22<00:05,  5.68it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 2:  80%|████████████████████████████████████████▏         | 135/168 [00:22<00:05,  5.68it/s, training_loss=0.218]\u001b[A\n",
      "Epoch 2:  81%|████████████████████████████████████████▍         | 136/168 [00:22<00:05,  5.74it/s, training_loss=0.218]\u001b[A\n",
      "Epoch 2:  81%|████████████████████████████████████████▍         | 136/168 [00:23<00:05,  5.74it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 2:  82%|████████████████████████████████████████▊         | 137/168 [00:23<00:05,  5.80it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 2:  82%|████████████████████████████████████████▊         | 137/168 [00:23<00:05,  5.80it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  82%|█████████████████████████████████████████         | 138/168 [00:23<00:05,  5.82it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  82%|█████████████████████████████████████████         | 138/168 [00:23<00:05,  5.82it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  83%|█████████████████████████████████████████▎        | 139/168 [00:23<00:04,  5.85it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 2:  83%|█████████████████████████████████████████▎        | 139/168 [00:23<00:04,  5.85it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 2:  83%|█████████████████████████████████████████▋        | 140/168 [00:23<00:04,  5.87it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 2:  83%|█████████████████████████████████████████▋        | 140/168 [00:23<00:04,  5.87it/s, training_loss=0.128]\u001b[A\n",
      "Epoch 2:  84%|█████████████████████████████████████████▉        | 141/168 [00:23<00:04,  5.88it/s, training_loss=0.128]\u001b[A\n",
      "Epoch 2:  84%|█████████████████████████████████████████▉        | 141/168 [00:23<00:04,  5.88it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  85%|██████████████████████████████████████████▎       | 142/168 [00:23<00:04,  5.89it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 2:  85%|██████████████████████████████████████████▎       | 142/168 [00:24<00:04,  5.89it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 2:  85%|██████████████████████████████████████████▌       | 143/168 [00:24<00:04,  5.91it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 2:  85%|██████████████████████████████████████████▌       | 143/168 [00:24<00:04,  5.91it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  86%|██████████████████████████████████████████▊       | 144/168 [00:24<00:04,  5.92it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  86%|██████████████████████████████████████████▊       | 144/168 [00:24<00:04,  5.92it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 2:  86%|███████████████████████████████████████████▏      | 145/168 [00:24<00:03,  5.92it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 2:  86%|███████████████████████████████████████████▏      | 145/168 [00:24<00:03,  5.92it/s, training_loss=0.211]\u001b[A\n",
      "Epoch 2:  87%|███████████████████████████████████████████▍      | 146/168 [00:24<00:03,  5.88it/s, training_loss=0.211]\u001b[A\n",
      "Epoch 2:  87%|███████████████████████████████████████████▍      | 146/168 [00:24<00:03,  5.88it/s, training_loss=0.442]\u001b[A\n",
      "Epoch 2:  88%|███████████████████████████████████████████▊      | 147/168 [00:24<00:03,  5.81it/s, training_loss=0.442]\u001b[A\n",
      "Epoch 2:  88%|███████████████████████████████████████████▊      | 147/168 [00:24<00:03,  5.81it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████████████████████      | 148/168 [00:24<00:03,  5.80it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████████████████████      | 148/168 [00:25<00:03,  5.80it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 2:  89%|████████████████████████████████████████████▎     | 149/168 [00:25<00:03,  5.83it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 2:  89%|████████████████████████████████████████████▎     | 149/168 [00:25<00:03,  5.83it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  89%|████████████████████████████████████████████▋     | 150/168 [00:25<00:03,  5.85it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 2:  89%|████████████████████████████████████████████▋     | 150/168 [00:25<00:03,  5.85it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 2:  90%|████████████████████████████████████████████▉     | 151/168 [00:25<00:02,  5.88it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 2:  90%|████████████████████████████████████████████▉     | 151/168 [00:25<00:02,  5.88it/s, training_loss=0.193]\u001b[A\n",
      "Epoch 2:  90%|█████████████████████████████████████████████▏    | 152/168 [00:25<00:02,  5.84it/s, training_loss=0.193]\u001b[A\n",
      "Epoch 2:  90%|█████████████████████████████████████████████▏    | 152/168 [00:25<00:02,  5.84it/s, training_loss=0.059]\u001b[A\n",
      "Epoch 2:  91%|█████████████████████████████████████████████▌    | 153/168 [00:25<00:02,  5.84it/s, training_loss=0.059]\u001b[A\n",
      "Epoch 2:  91%|█████████████████████████████████████████████▌    | 153/168 [00:26<00:02,  5.84it/s, training_loss=0.146]\u001b[A\n",
      "Epoch 2:  92%|█████████████████████████████████████████████▊    | 154/168 [00:26<00:02,  5.87it/s, training_loss=0.146]\u001b[A\n",
      "Epoch 2:  92%|█████████████████████████████████████████████▊    | 154/168 [00:26<00:02,  5.87it/s, training_loss=0.090]\u001b[A\n",
      "Epoch 2:  92%|██████████████████████████████████████████████▏   | 155/168 [00:26<00:02,  5.83it/s, training_loss=0.090]\u001b[A\n",
      "Epoch 2:  92%|██████████████████████████████████████████████▏   | 155/168 [00:26<00:02,  5.83it/s, training_loss=0.139]\u001b[A\n",
      "Epoch 2:  93%|██████████████████████████████████████████████▍   | 156/168 [00:26<00:02,  5.80it/s, training_loss=0.139]\u001b[A\n",
      "Epoch 2:  93%|██████████████████████████████████████████████▍   | 156/168 [00:26<00:02,  5.80it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 2:  93%|██████████████████████████████████████████████▋   | 157/168 [00:26<00:01,  5.77it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 2:  93%|██████████████████████████████████████████████▋   | 157/168 [00:26<00:01,  5.77it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 2:  94%|███████████████████████████████████████████████   | 158/168 [00:26<00:01,  5.73it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 2:  94%|███████████████████████████████████████████████   | 158/168 [00:26<00:01,  5.73it/s, training_loss=0.135]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████████████████████▎  | 159/168 [00:26<00:01,  5.74it/s, training_loss=0.135]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████████████████████▎  | 159/168 [00:27<00:01,  5.74it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████████████████████▌  | 160/168 [00:27<00:01,  5.79it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████████████████████▌  | 160/168 [00:27<00:01,  5.79it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 2:  96%|███████████████████████████████████████████████▉  | 161/168 [00:27<00:01,  5.76it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 2:  96%|███████████████████████████████████████████████▉  | 161/168 [00:27<00:01,  5.76it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 2:  96%|████████████████████████████████████████████████▏ | 162/168 [00:27<00:01,  5.74it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 2:  96%|████████████████████████████████████████████████▏ | 162/168 [00:27<00:01,  5.74it/s, training_loss=0.176]\u001b[A\n",
      "Epoch 2:  97%|████████████████████████████████████████████████▌ | 163/168 [00:27<00:00,  5.79it/s, training_loss=0.176]\u001b[A\n",
      "Epoch 2:  97%|████████████████████████████████████████████████▌ | 163/168 [00:27<00:00,  5.79it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 2:  98%|████████████████████████████████████████████████▊ | 164/168 [00:27<00:00,  5.81it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 2:  98%|████████████████████████████████████████████████▊ | 164/168 [00:27<00:00,  5.81it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  98%|█████████████████████████████████████████████████ | 165/168 [00:27<00:00,  5.83it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  98%|█████████████████████████████████████████████████ | 165/168 [00:28<00:00,  5.83it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 2:  99%|█████████████████████████████████████████████████▍| 166/168 [00:28<00:00,  5.84it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 2:  99%|█████████████████████████████████████████████████▍| 166/168 [00:28<00:00,  5.84it/s, training_loss=0.087]\u001b[A\n",
      "Epoch 2:  99%|█████████████████████████████████████████████████▋| 167/168 [00:28<00:00,  5.82it/s, training_loss=0.087]\u001b[A\n",
      "Epoch 2:  99%|█████████████████████████████████████████████████▋| 167/168 [00:28<00:00,  5.82it/s, training_loss=0.045]\u001b[A\n",
      "Epoch Progress:  50%|██████████████████████████████████                                  | 2/4 [00:56<00:56, 28.23s/it]\u001b[A\n",
      "Epoch 3:   0%|                                                                                 | 0/168 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:   0%|                                                            | 0/168 [00:00<?, ?it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 3:   1%|▎                                                   | 1/168 [00:00<00:28,  5.86it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 3:   1%|▎                                                   | 1/168 [00:00<00:28,  5.86it/s, training_loss=0.076]\u001b[A\n",
      "Epoch 3:   1%|▌                                                   | 2/168 [00:00<00:28,  5.90it/s, training_loss=0.076]\u001b[A\n",
      "Epoch 3:   1%|▌                                                   | 2/168 [00:00<00:28,  5.90it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 3:   2%|▉                                                   | 3/168 [00:00<00:27,  5.90it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 3:   2%|▉                                                   | 3/168 [00:00<00:27,  5.90it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 3:   2%|█▏                                                  | 4/168 [00:00<00:27,  5.87it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 3:   2%|█▏                                                  | 4/168 [00:00<00:27,  5.87it/s, training_loss=0.203]\u001b[A\n",
      "Epoch 3:   3%|█▌                                                  | 5/168 [00:00<00:27,  5.88it/s, training_loss=0.203]\u001b[A\n",
      "Epoch 3:   3%|█▌                                                  | 5/168 [00:01<00:27,  5.88it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 3:   4%|█▊                                                  | 6/168 [00:01<00:27,  5.89it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 3:   4%|█▊                                                  | 6/168 [00:01<00:27,  5.89it/s, training_loss=0.038]\u001b[A\n",
      "Epoch 3:   4%|██▏                                                 | 7/168 [00:01<00:27,  5.91it/s, training_loss=0.038]\u001b[A\n",
      "Epoch 3:   4%|██▏                                                 | 7/168 [00:01<00:27,  5.91it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 3:   5%|██▍                                                 | 8/168 [00:01<00:27,  5.90it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 3:   5%|██▍                                                 | 8/168 [00:01<00:27,  5.90it/s, training_loss=0.099]\u001b[A\n",
      "Epoch 3:   5%|██▊                                                 | 9/168 [00:01<00:27,  5.88it/s, training_loss=0.099]\u001b[A\n",
      "Epoch 3:   5%|██▊                                                 | 9/168 [00:01<00:27,  5.88it/s, training_loss=0.121]\u001b[A\n",
      "Epoch 3:   6%|███                                                | 10/168 [00:01<00:26,  5.88it/s, training_loss=0.121]\u001b[A\n",
      "Epoch 3:   6%|███                                                | 10/168 [00:01<00:26,  5.88it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 3:   7%|███▎                                               | 11/168 [00:01<00:26,  5.89it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 3:   7%|███▎                                               | 11/168 [00:02<00:26,  5.89it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 3:   7%|███▋                                               | 12/168 [00:02<00:26,  5.85it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 3:   7%|███▋                                               | 12/168 [00:02<00:26,  5.85it/s, training_loss=0.093]\u001b[A\n",
      "Epoch 3:   8%|███▉                                               | 13/168 [00:02<00:26,  5.86it/s, training_loss=0.093]\u001b[A\n",
      "Epoch 3:   8%|███▉                                               | 13/168 [00:02<00:26,  5.86it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 3:   8%|████▎                                              | 14/168 [00:02<00:26,  5.88it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 3:   8%|████▎                                              | 14/168 [00:02<00:26,  5.88it/s, training_loss=0.088]\u001b[A\n",
      "Epoch 3:   9%|████▌                                              | 15/168 [00:02<00:25,  5.89it/s, training_loss=0.088]\u001b[A\n",
      "Epoch 3:   9%|████▌                                              | 15/168 [00:02<00:25,  5.89it/s, training_loss=0.042]\u001b[A\n",
      "Epoch 3:  10%|████▊                                              | 16/168 [00:02<00:26,  5.84it/s, training_loss=0.042]\u001b[A\n",
      "Epoch 3:  10%|████▊                                              | 16/168 [00:02<00:26,  5.84it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 3:  10%|█████▏                                             | 17/168 [00:02<00:25,  5.84it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 3:  10%|█████▏                                             | 17/168 [00:03<00:25,  5.84it/s, training_loss=0.184]\u001b[A\n",
      "Epoch 3:  11%|█████▍                                             | 18/168 [00:03<00:25,  5.87it/s, training_loss=0.184]\u001b[A\n",
      "Epoch 3:  11%|█████▍                                             | 18/168 [00:03<00:25,  5.87it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 3:  11%|█████▊                                             | 19/168 [00:03<00:25,  5.86it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 3:  11%|█████▊                                             | 19/168 [00:03<00:25,  5.86it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:  12%|██████                                             | 20/168 [00:03<00:25,  5.90it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:  12%|██████                                             | 20/168 [00:03<00:25,  5.90it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 3:  12%|██████▍                                            | 21/168 [00:03<00:24,  5.90it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 3:  12%|██████▍                                            | 21/168 [00:03<00:24,  5.90it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 3:  13%|██████▋                                            | 22/168 [00:03<00:24,  5.92it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 3:  13%|██████▋                                            | 22/168 [00:03<00:24,  5.92it/s, training_loss=0.122]\u001b[A\n",
      "Epoch 3:  14%|██████▉                                            | 23/168 [00:03<00:24,  5.86it/s, training_loss=0.122]\u001b[A\n",
      "Epoch 3:  14%|██████▉                                            | 23/168 [00:04<00:24,  5.86it/s, training_loss=0.086]\u001b[A\n",
      "Epoch 3:  14%|███████▎                                           | 24/168 [00:04<00:24,  5.88it/s, training_loss=0.086]\u001b[A\n",
      "Epoch 3:  14%|███████▎                                           | 24/168 [00:04<00:24,  5.88it/s, training_loss=0.204]\u001b[A\n",
      "Epoch 3:  15%|███████▌                                           | 25/168 [00:04<00:24,  5.89it/s, training_loss=0.204]\u001b[A\n",
      "Epoch 3:  15%|███████▌                                           | 25/168 [00:04<00:24,  5.89it/s, training_loss=0.155]\u001b[A\n",
      "Epoch 3:  15%|███████▉                                           | 26/168 [00:04<00:24,  5.91it/s, training_loss=0.155]\u001b[A\n",
      "Epoch 3:  15%|███████▉                                           | 26/168 [00:04<00:24,  5.91it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 3:  16%|████████▏                                          | 27/168 [00:04<00:23,  5.91it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 3:  16%|████████▏                                          | 27/168 [00:04<00:23,  5.91it/s, training_loss=0.146]\u001b[A\n",
      "Epoch 3:  17%|████████▌                                          | 28/168 [00:04<00:23,  5.90it/s, training_loss=0.146]\u001b[A\n",
      "Epoch 3:  17%|████████▌                                          | 28/168 [00:04<00:23,  5.90it/s, training_loss=0.080]\u001b[A\n",
      "Epoch 3:  17%|████████▊                                          | 29/168 [00:04<00:23,  5.89it/s, training_loss=0.080]\u001b[A\n",
      "Epoch 3:  17%|████████▊                                          | 29/168 [00:05<00:23,  5.89it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 3:  18%|█████████                                          | 30/168 [00:05<00:23,  5.86it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 3:  18%|█████████                                          | 30/168 [00:05<00:23,  5.86it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:  18%|█████████▍                                         | 31/168 [00:05<00:23,  5.86it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:  18%|█████████▍                                         | 31/168 [00:05<00:23,  5.86it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 3:  19%|█████████▋                                         | 32/168 [00:05<00:23,  5.85it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 3:  19%|█████████▋                                         | 32/168 [00:05<00:23,  5.85it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 3:  20%|██████████                                         | 33/168 [00:05<00:23,  5.85it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 3:  20%|██████████                                         | 33/168 [00:05<00:23,  5.85it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 3:  20%|██████████▎                                        | 34/168 [00:05<00:22,  5.86it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 3:  20%|██████████▎                                        | 34/168 [00:05<00:22,  5.86it/s, training_loss=0.081]\u001b[A\n",
      "Epoch 3:  21%|██████████▋                                        | 35/168 [00:05<00:22,  5.86it/s, training_loss=0.081]\u001b[A\n",
      "Epoch 3:  21%|██████████▋                                        | 35/168 [00:06<00:22,  5.86it/s, training_loss=0.093]\u001b[A\n",
      "Epoch 3:  21%|██████████▉                                        | 36/168 [00:06<00:22,  5.88it/s, training_loss=0.093]\u001b[A\n",
      "Epoch 3:  21%|██████████▉                                        | 36/168 [00:06<00:22,  5.88it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  22%|███████████▏                                       | 37/168 [00:06<00:22,  5.83it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  22%|███████████▏                                       | 37/168 [00:06<00:22,  5.83it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  23%|███████████▌                                       | 38/168 [00:06<00:22,  5.83it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  23%|███████████▌                                       | 38/168 [00:06<00:22,  5.83it/s, training_loss=0.067]\u001b[A\n",
      "Epoch 3:  23%|███████████▊                                       | 39/168 [00:06<00:22,  5.82it/s, training_loss=0.067]\u001b[A\n",
      "Epoch 3:  23%|███████████▊                                       | 39/168 [00:06<00:22,  5.82it/s, training_loss=0.070]\u001b[A\n",
      "Epoch 3:  24%|████████████▏                                      | 40/168 [00:06<00:22,  5.81it/s, training_loss=0.070]\u001b[A\n",
      "Epoch 3:  24%|████████████▏                                      | 40/168 [00:06<00:22,  5.81it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:  24%|████████████▍                                      | 41/168 [00:06<00:21,  5.84it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:  24%|████████████▍                                      | 41/168 [00:07<00:21,  5.84it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 3:  25%|████████████▊                                      | 42/168 [00:07<00:21,  5.81it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 3:  25%|████████████▊                                      | 42/168 [00:07<00:21,  5.81it/s, training_loss=0.148]\u001b[A\n",
      "Epoch 3:  26%|█████████████                                      | 43/168 [00:07<00:21,  5.84it/s, training_loss=0.148]\u001b[A\n",
      "Epoch 3:  26%|█████████████                                      | 43/168 [00:07<00:21,  5.84it/s, training_loss=0.108]\u001b[A\n",
      "Epoch 3:  26%|█████████████▎                                     | 44/168 [00:07<00:21,  5.83it/s, training_loss=0.108]\u001b[A\n",
      "Epoch 3:  26%|█████████████▎                                     | 44/168 [00:07<00:21,  5.83it/s, training_loss=0.053]\u001b[A\n",
      "Epoch 3:  27%|█████████████▋                                     | 45/168 [00:07<00:21,  5.83it/s, training_loss=0.053]\u001b[A\n",
      "Epoch 3:  27%|█████████████▋                                     | 45/168 [00:07<00:21,  5.83it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 3:  27%|█████████████▉                                     | 46/168 [00:07<00:21,  5.80it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 3:  27%|█████████████▉                                     | 46/168 [00:08<00:21,  5.80it/s, training_loss=0.160]\u001b[A\n",
      "Epoch 3:  28%|██████████████▎                                    | 47/168 [00:08<00:20,  5.82it/s, training_loss=0.160]\u001b[A\n",
      "Epoch 3:  28%|██████████████▎                                    | 47/168 [00:08<00:20,  5.82it/s, training_loss=0.101]\u001b[A\n",
      "Epoch 3:  29%|██████████████▌                                    | 48/168 [00:08<00:20,  5.84it/s, training_loss=0.101]\u001b[A\n",
      "Epoch 3:  29%|██████████████▌                                    | 48/168 [00:08<00:20,  5.84it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  29%|██████████████▉                                    | 49/168 [00:08<00:20,  5.72it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  29%|██████████████▉                                    | 49/168 [00:08<00:20,  5.72it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  30%|███████████████▏                                   | 50/168 [00:08<00:20,  5.72it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  30%|███████████████▏                                   | 50/168 [00:08<00:20,  5.72it/s, training_loss=0.136]\u001b[A\n",
      "Epoch 3:  30%|███████████████▍                                   | 51/168 [00:08<00:20,  5.69it/s, training_loss=0.136]\u001b[A\n",
      "Epoch 3:  30%|███████████████▍                                   | 51/168 [00:08<00:20,  5.69it/s, training_loss=0.130]\u001b[A\n",
      "Epoch 3:  31%|███████████████▊                                   | 52/168 [00:08<00:20,  5.67it/s, training_loss=0.130]\u001b[A\n",
      "Epoch 3:  31%|███████████████▊                                   | 52/168 [00:09<00:20,  5.67it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 3:  32%|████████████████                                   | 53/168 [00:09<00:20,  5.67it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 3:  32%|████████████████                                   | 53/168 [00:09<00:20,  5.67it/s, training_loss=0.112]\u001b[A\n",
      "Epoch 3:  32%|████████████████▍                                  | 54/168 [00:09<00:20,  5.68it/s, training_loss=0.112]\u001b[A\n",
      "Epoch 3:  32%|████████████████▍                                  | 54/168 [00:09<00:20,  5.68it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 3:  33%|████████████████▋                                  | 55/168 [00:09<00:19,  5.69it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 3:  33%|████████████████▋                                  | 55/168 [00:09<00:19,  5.69it/s, training_loss=0.070]\u001b[A\n",
      "Epoch 3:  33%|█████████████████                                  | 56/168 [00:09<00:19,  5.61it/s, training_loss=0.070]\u001b[A\n",
      "Epoch 3:  33%|█████████████████                                  | 56/168 [00:09<00:19,  5.61it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 3:  34%|█████████████████▎                                 | 57/168 [00:09<00:19,  5.63it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 3:  34%|█████████████████▎                                 | 57/168 [00:09<00:19,  5.63it/s, training_loss=0.108]\u001b[A\n",
      "Epoch 3:  35%|█████████████████▌                                 | 58/168 [00:09<00:19,  5.71it/s, training_loss=0.108]\u001b[A\n",
      "Epoch 3:  35%|█████████████████▌                                 | 58/168 [00:10<00:19,  5.71it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 3:  35%|█████████████████▉                                 | 59/168 [00:10<00:19,  5.71it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 3:  35%|█████████████████▉                                 | 59/168 [00:10<00:19,  5.71it/s, training_loss=0.117]\u001b[A\n",
      "Epoch 3:  36%|██████████████████▏                                | 60/168 [00:10<00:18,  5.77it/s, training_loss=0.117]\u001b[A\n",
      "Epoch 3:  36%|██████████████████▏                                | 60/168 [00:10<00:18,  5.77it/s, training_loss=0.207]\u001b[A\n",
      "Epoch 3:  36%|██████████████████▌                                | 61/168 [00:10<00:18,  5.81it/s, training_loss=0.207]\u001b[A\n",
      "Epoch 3:  36%|██████████████████▌                                | 61/168 [00:10<00:18,  5.81it/s, training_loss=0.084]\u001b[A\n",
      "Epoch 3:  37%|██████████████████▊                                | 62/168 [00:10<00:18,  5.70it/s, training_loss=0.084]\u001b[A\n",
      "Epoch 3:  37%|██████████████████▊                                | 62/168 [00:10<00:18,  5.70it/s, training_loss=0.157]\u001b[A\n",
      "Epoch 3:  38%|███████████████████▏                               | 63/168 [00:10<00:18,  5.61it/s, training_loss=0.157]\u001b[A\n",
      "Epoch 3:  38%|███████████████████▏                               | 63/168 [00:11<00:18,  5.61it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 3:  38%|███████████████████▍                               | 64/168 [00:11<00:18,  5.60it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 3:  38%|███████████████████▍                               | 64/168 [00:11<00:18,  5.60it/s, training_loss=0.130]\u001b[A\n",
      "Epoch 3:  39%|███████████████████▋                               | 65/168 [00:11<00:18,  5.64it/s, training_loss=0.130]\u001b[A\n",
      "Epoch 3:  39%|███████████████████▋                               | 65/168 [00:11<00:18,  5.64it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  39%|████████████████████                               | 66/168 [00:11<00:18,  5.62it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  39%|████████████████████                               | 66/168 [00:11<00:18,  5.62it/s, training_loss=0.091]\u001b[A\n",
      "Epoch 3:  40%|████████████████████▎                              | 67/168 [00:11<00:17,  5.66it/s, training_loss=0.091]\u001b[A\n",
      "Epoch 3:  40%|████████████████████▎                              | 67/168 [00:11<00:17,  5.66it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  40%|████████████████████▋                              | 68/168 [00:11<00:17,  5.65it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  40%|████████████████████▋                              | 68/168 [00:11<00:17,  5.65it/s, training_loss=0.126]\u001b[A\n",
      "Epoch 3:  41%|████████████████████▉                              | 69/168 [00:11<00:17,  5.67it/s, training_loss=0.126]\u001b[A\n",
      "Epoch 3:  41%|████████████████████▉                              | 69/168 [00:12<00:17,  5.67it/s, training_loss=0.112]\u001b[A\n",
      "Epoch 3:  42%|█████████████████████▎                             | 70/168 [00:12<00:17,  5.68it/s, training_loss=0.112]\u001b[A\n",
      "Epoch 3:  42%|█████████████████████▎                             | 70/168 [00:12<00:17,  5.68it/s, training_loss=0.144]\u001b[A\n",
      "Epoch 3:  42%|█████████████████████▌                             | 71/168 [00:12<00:17,  5.65it/s, training_loss=0.144]\u001b[A\n",
      "Epoch 3:  42%|█████████████████████▌                             | 71/168 [00:12<00:17,  5.65it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 3:  43%|█████████████████████▊                             | 72/168 [00:12<00:16,  5.69it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 3:  43%|█████████████████████▊                             | 72/168 [00:12<00:16,  5.69it/s, training_loss=0.041]\u001b[A\n",
      "Epoch 3:  43%|██████████████████████▏                            | 73/168 [00:12<00:16,  5.74it/s, training_loss=0.041]\u001b[A\n",
      "Epoch 3:  43%|██████████████████████▏                            | 73/168 [00:12<00:16,  5.74it/s, training_loss=0.141]\u001b[A\n",
      "Epoch 3:  44%|██████████████████████▍                            | 74/168 [00:12<00:16,  5.70it/s, training_loss=0.141]\u001b[A\n",
      "Epoch 3:  44%|██████████████████████▍                            | 74/168 [00:12<00:16,  5.70it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 3:  45%|██████████████████████▊                            | 75/168 [00:12<00:16,  5.76it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 3:  45%|██████████████████████▊                            | 75/168 [00:13<00:16,  5.76it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 3:  45%|███████████████████████                            | 76/168 [00:13<00:16,  5.73it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 3:  45%|███████████████████████                            | 76/168 [00:13<00:16,  5.73it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 3:  46%|███████████████████████▍                           | 77/168 [00:13<00:15,  5.77it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 3:  46%|███████████████████████▍                           | 77/168 [00:13<00:15,  5.77it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 3:  46%|███████████████████████▋                           | 78/168 [00:13<00:15,  5.82it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 3:  46%|███████████████████████▋                           | 78/168 [00:13<00:15,  5.82it/s, training_loss=0.041]\u001b[A\n",
      "Epoch 3:  47%|███████████████████████▉                           | 79/168 [00:13<00:15,  5.83it/s, training_loss=0.041]\u001b[A\n",
      "Epoch 3:  47%|███████████████████████▉                           | 79/168 [00:13<00:15,  5.83it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  48%|████████████████████████▎                          | 80/168 [00:13<00:15,  5.83it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  48%|████████████████████████▎                          | 80/168 [00:13<00:15,  5.83it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 3:  48%|████████████████████████▌                          | 81/168 [00:13<00:14,  5.81it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 3:  48%|████████████████████████▌                          | 81/168 [00:14<00:14,  5.81it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 3:  49%|████████████████████████▉                          | 82/168 [00:14<00:14,  5.79it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 3:  49%|████████████████████████▉                          | 82/168 [00:14<00:14,  5.79it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 3:  49%|█████████████████████████▏                         | 83/168 [00:14<00:14,  5.82it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 3:  49%|█████████████████████████▏                         | 83/168 [00:14<00:14,  5.82it/s, training_loss=0.067]\u001b[A\n",
      "Epoch 3:  50%|█████████████████████████▌                         | 84/168 [00:14<00:14,  5.84it/s, training_loss=0.067]\u001b[A\n",
      "Epoch 3:  50%|█████████████████████████▌                         | 84/168 [00:14<00:14,  5.84it/s, training_loss=0.074]\u001b[A\n",
      "Epoch 3:  51%|█████████████████████████▊                         | 85/168 [00:14<00:14,  5.85it/s, training_loss=0.074]\u001b[A\n",
      "Epoch 3:  51%|█████████████████████████▊                         | 85/168 [00:14<00:14,  5.85it/s, training_loss=0.053]\u001b[A\n",
      "Epoch 3:  51%|██████████████████████████                         | 86/168 [00:14<00:14,  5.85it/s, training_loss=0.053]\u001b[A\n",
      "Epoch 3:  51%|██████████████████████████                         | 86/168 [00:14<00:14,  5.85it/s, training_loss=0.064]\u001b[A\n",
      "Epoch 3:  52%|██████████████████████████▍                        | 87/168 [00:15<00:13,  5.84it/s, training_loss=0.064]\u001b[A\n",
      "Epoch 3:  52%|██████████████████████████▍                        | 87/168 [00:15<00:13,  5.84it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 3:  52%|██████████████████████████▋                        | 88/168 [00:15<00:13,  5.79it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 3:  52%|██████████████████████████▋                        | 88/168 [00:15<00:13,  5.79it/s, training_loss=0.053]\u001b[A\n",
      "Epoch 3:  53%|███████████████████████████                        | 89/168 [00:15<00:13,  5.75it/s, training_loss=0.053]\u001b[A\n",
      "Epoch 3:  53%|███████████████████████████                        | 89/168 [00:15<00:13,  5.75it/s, training_loss=0.041]\u001b[A\n",
      "Epoch 3:  54%|███████████████████████████▎                       | 90/168 [00:15<00:13,  5.79it/s, training_loss=0.041]\u001b[A\n",
      "Epoch 3:  54%|███████████████████████████▎                       | 90/168 [00:15<00:13,  5.79it/s, training_loss=0.066]\u001b[A\n",
      "Epoch 3:  54%|███████████████████████████▌                       | 91/168 [00:15<00:13,  5.79it/s, training_loss=0.066]\u001b[A\n",
      "Epoch 3:  54%|███████████████████████████▌                       | 91/168 [00:15<00:13,  5.79it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 3:  55%|███████████████████████████▉                       | 92/168 [00:15<00:13,  5.81it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 3:  55%|███████████████████████████▉                       | 92/168 [00:16<00:13,  5.81it/s, training_loss=0.026]\u001b[A\n",
      "Epoch 3:  55%|████████████████████████████▏                      | 93/168 [00:16<00:12,  5.83it/s, training_loss=0.026]\u001b[A\n",
      "Epoch 3:  55%|████████████████████████████▏                      | 93/168 [00:16<00:12,  5.83it/s, training_loss=0.115]\u001b[A\n",
      "Epoch 3:  56%|████████████████████████████▌                      | 94/168 [00:16<00:12,  5.83it/s, training_loss=0.115]\u001b[A\n",
      "Epoch 3:  56%|████████████████████████████▌                      | 94/168 [00:16<00:12,  5.83it/s, training_loss=0.059]\u001b[A\n",
      "Epoch 3:  57%|████████████████████████████▊                      | 95/168 [00:16<00:12,  5.84it/s, training_loss=0.059]\u001b[A\n",
      "Epoch 3:  57%|████████████████████████████▊                      | 95/168 [00:16<00:12,  5.84it/s, training_loss=0.095]\u001b[A\n",
      "Epoch 3:  57%|█████████████████████████████▏                     | 96/168 [00:16<00:12,  5.85it/s, training_loss=0.095]\u001b[A\n",
      "Epoch 3:  57%|█████████████████████████████▏                     | 96/168 [00:16<00:12,  5.85it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 3:  58%|█████████████████████████████▍                     | 97/168 [00:16<00:12,  5.84it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 3:  58%|█████████████████████████████▍                     | 97/168 [00:16<00:12,  5.84it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 3:  58%|█████████████████████████████▊                     | 98/168 [00:16<00:11,  5.87it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 3:  58%|█████████████████████████████▊                     | 98/168 [00:17<00:11,  5.87it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  59%|██████████████████████████████                     | 99/168 [00:17<00:11,  5.88it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  59%|██████████████████████████████                     | 99/168 [00:17<00:11,  5.88it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 3:  60%|█████████████████████████████▊                    | 100/168 [00:17<00:11,  5.87it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 3:  60%|█████████████████████████████▊                    | 100/168 [00:17<00:11,  5.87it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 3:  60%|██████████████████████████████                    | 101/168 [00:17<00:11,  5.88it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 3:  60%|██████████████████████████████                    | 101/168 [00:17<00:11,  5.88it/s, training_loss=0.056]\u001b[A\n",
      "Epoch 3:  61%|██████████████████████████████▎                   | 102/168 [00:17<00:11,  5.87it/s, training_loss=0.056]\u001b[A\n",
      "Epoch 3:  61%|██████████████████████████████▎                   | 102/168 [00:17<00:11,  5.87it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  61%|██████████████████████████████▋                   | 103/168 [00:17<00:11,  5.88it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  61%|██████████████████████████████▋                   | 103/168 [00:17<00:11,  5.88it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  62%|██████████████████████████████▉                   | 104/168 [00:17<00:10,  5.85it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  62%|██████████████████████████████▉                   | 104/168 [00:18<00:10,  5.85it/s, training_loss=0.092]\u001b[A\n",
      "Epoch 3:  62%|███████████████████████████████▎                  | 105/168 [00:18<00:10,  5.82it/s, training_loss=0.092]\u001b[A\n",
      "Epoch 3:  62%|███████████████████████████████▎                  | 105/168 [00:18<00:10,  5.82it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  63%|███████████████████████████████▌                  | 106/168 [00:18<00:10,  5.83it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  63%|███████████████████████████████▌                  | 106/168 [00:18<00:10,  5.83it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 3:  64%|███████████████████████████████▊                  | 107/168 [00:18<00:10,  5.84it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 3:  64%|███████████████████████████████▊                  | 107/168 [00:18<00:10,  5.84it/s, training_loss=0.099]\u001b[A\n",
      "Epoch 3:  64%|████████████████████████████████▏                 | 108/168 [00:18<00:10,  5.83it/s, training_loss=0.099]\u001b[A\n",
      "Epoch 3:  64%|████████████████████████████████▏                 | 108/168 [00:18<00:10,  5.83it/s, training_loss=0.104]\u001b[A\n",
      "Epoch 3:  65%|████████████████████████████████▍                 | 109/168 [00:18<00:10,  5.81it/s, training_loss=0.104]\u001b[A\n",
      "Epoch 3:  65%|████████████████████████████████▍                 | 109/168 [00:18<00:10,  5.81it/s, training_loss=0.015]\u001b[A\n",
      "Epoch 3:  65%|████████████████████████████████▋                 | 110/168 [00:18<00:09,  5.82it/s, training_loss=0.015]\u001b[A\n",
      "Epoch 3:  65%|████████████████████████████████▋                 | 110/168 [00:19<00:09,  5.82it/s, training_loss=0.176]\u001b[A\n",
      "Epoch 3:  66%|█████████████████████████████████                 | 111/168 [00:19<00:09,  5.82it/s, training_loss=0.176]\u001b[A\n",
      "Epoch 3:  66%|█████████████████████████████████                 | 111/168 [00:19<00:09,  5.82it/s, training_loss=0.140]\u001b[A\n",
      "Epoch 3:  67%|█████████████████████████████████▎                | 112/168 [00:19<00:09,  5.75it/s, training_loss=0.140]\u001b[A\n",
      "Epoch 3:  67%|█████████████████████████████████▎                | 112/168 [00:19<00:09,  5.75it/s, training_loss=0.134]\u001b[A\n",
      "Epoch 3:  67%|█████████████████████████████████▋                | 113/168 [00:19<00:09,  5.76it/s, training_loss=0.134]\u001b[A\n",
      "Epoch 3:  67%|█████████████████████████████████▋                | 113/168 [00:19<00:09,  5.76it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 3:  68%|█████████████████████████████████▉                | 114/168 [00:19<00:09,  5.80it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 3:  68%|█████████████████████████████████▉                | 114/168 [00:19<00:09,  5.80it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 3:  68%|██████████████████████████████████▏               | 115/168 [00:19<00:09,  5.81it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 3:  68%|██████████████████████████████████▏               | 115/168 [00:19<00:09,  5.81it/s, training_loss=0.103]\u001b[A\n",
      "Epoch 3:  69%|██████████████████████████████████▌               | 116/168 [00:19<00:08,  5.81it/s, training_loss=0.103]\u001b[A\n",
      "Epoch 3:  69%|██████████████████████████████████▌               | 116/168 [00:20<00:08,  5.81it/s, training_loss=0.136]\u001b[A\n",
      "Epoch 3:  70%|██████████████████████████████████▊               | 117/168 [00:20<00:08,  5.83it/s, training_loss=0.136]\u001b[A\n",
      "Epoch 3:  70%|██████████████████████████████████▊               | 117/168 [00:20<00:08,  5.83it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  70%|███████████████████████████████████               | 118/168 [00:20<00:08,  5.84it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  70%|███████████████████████████████████               | 118/168 [00:20<00:08,  5.84it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 3:  71%|███████████████████████████████████▍              | 119/168 [00:20<00:08,  5.85it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 3:  71%|███████████████████████████████████▍              | 119/168 [00:20<00:08,  5.85it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 3:  71%|███████████████████████████████████▋              | 120/168 [00:20<00:08,  5.86it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 3:  71%|███████████████████████████████████▋              | 120/168 [00:20<00:08,  5.86it/s, training_loss=0.031]\u001b[A\n",
      "Epoch 3:  72%|████████████████████████████████████              | 121/168 [00:20<00:08,  5.85it/s, training_loss=0.031]\u001b[A\n",
      "Epoch 3:  72%|████████████████████████████████████              | 121/168 [00:21<00:08,  5.85it/s, training_loss=0.088]\u001b[A\n",
      "Epoch 3:  73%|████████████████████████████████████▎             | 122/168 [00:21<00:07,  5.87it/s, training_loss=0.088]\u001b[A\n",
      "Epoch 3:  73%|████████████████████████████████████▎             | 122/168 [00:21<00:07,  5.87it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 3:  73%|████████████████████████████████████▌             | 123/168 [00:21<00:07,  5.87it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 3:  73%|████████████████████████████████████▌             | 123/168 [00:21<00:07,  5.87it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 3:  74%|████████████████████████████████████▉             | 124/168 [00:21<00:07,  5.86it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 3:  74%|████████████████████████████████████▉             | 124/168 [00:21<00:07,  5.86it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 3:  74%|█████████████████████████████████████▏            | 125/168 [00:21<00:07,  5.85it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 3:  74%|█████████████████████████████████████▏            | 125/168 [00:21<00:07,  5.85it/s, training_loss=0.152]\u001b[A\n",
      "Epoch 3:  75%|█████████████████████████████████████▌            | 126/168 [00:21<00:07,  5.86it/s, training_loss=0.152]\u001b[A\n",
      "Epoch 3:  75%|█████████████████████████████████████▌            | 126/168 [00:21<00:07,  5.86it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 3:  76%|█████████████████████████████████████▊            | 127/168 [00:21<00:06,  5.87it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 3:  76%|█████████████████████████████████████▊            | 127/168 [00:22<00:06,  5.87it/s, training_loss=0.119]\u001b[A\n",
      "Epoch 3:  76%|██████████████████████████████████████            | 128/168 [00:22<00:06,  5.86it/s, training_loss=0.119]\u001b[A\n",
      "Epoch 3:  76%|██████████████████████████████████████            | 128/168 [00:22<00:06,  5.86it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 3:  77%|██████████████████████████████████████▍           | 129/168 [00:22<00:06,  5.83it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 3:  77%|██████████████████████████████████████▍           | 129/168 [00:22<00:06,  5.83it/s, training_loss=0.081]\u001b[A\n",
      "Epoch 3:  77%|██████████████████████████████████████▋           | 130/168 [00:22<00:06,  5.83it/s, training_loss=0.081]\u001b[A\n",
      "Epoch 3:  77%|██████████████████████████████████████▋           | 130/168 [00:22<00:06,  5.83it/s, training_loss=0.072]\u001b[A\n",
      "Epoch 3:  78%|██████████████████████████████████████▉           | 131/168 [00:22<00:06,  5.82it/s, training_loss=0.072]\u001b[A\n",
      "Epoch 3:  78%|██████████████████████████████████████▉           | 131/168 [00:22<00:06,  5.82it/s, training_loss=0.122]\u001b[A\n",
      "Epoch 3:  79%|███████████████████████████████████████▎          | 132/168 [00:22<00:06,  5.84it/s, training_loss=0.122]\u001b[A\n",
      "Epoch 3:  79%|███████████████████████████████████████▎          | 132/168 [00:22<00:06,  5.84it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  79%|███████████████████████████████████████▌          | 133/168 [00:22<00:05,  5.85it/s, training_loss=0.195]\u001b[A\n",
      "Epoch 3:  79%|███████████████████████████████████████▌          | 133/168 [00:23<00:05,  5.85it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 3:  80%|███████████████████████████████████████▉          | 134/168 [00:23<00:05,  5.85it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 3:  80%|███████████████████████████████████████▉          | 134/168 [00:23<00:05,  5.85it/s, training_loss=0.103]\u001b[A\n",
      "Epoch 3:  80%|████████████████████████████████████████▏         | 135/168 [00:23<00:05,  5.85it/s, training_loss=0.103]\u001b[A\n",
      "Epoch 3:  80%|████████████████████████████████████████▏         | 135/168 [00:23<00:05,  5.85it/s, training_loss=0.052]\u001b[A\n",
      "Epoch 3:  81%|████████████████████████████████████████▍         | 136/168 [00:23<00:05,  5.86it/s, training_loss=0.052]\u001b[A\n",
      "Epoch 3:  81%|████████████████████████████████████████▍         | 136/168 [00:23<00:05,  5.86it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 3:  82%|████████████████████████████████████████▊         | 137/168 [00:23<00:05,  5.86it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 3:  82%|████████████████████████████████████████▊         | 137/168 [00:23<00:05,  5.86it/s, training_loss=0.069]\u001b[A\n",
      "Epoch 3:  82%|█████████████████████████████████████████         | 138/168 [00:23<00:05,  5.83it/s, training_loss=0.069]\u001b[A\n",
      "Epoch 3:  82%|█████████████████████████████████████████         | 138/168 [00:23<00:05,  5.83it/s, training_loss=0.037]\u001b[A\n",
      "Epoch 3:  83%|█████████████████████████████████████████▎        | 139/168 [00:23<00:04,  5.83it/s, training_loss=0.037]\u001b[A\n",
      "Epoch 3:  83%|█████████████████████████████████████████▎        | 139/168 [00:24<00:04,  5.83it/s, training_loss=0.167]\u001b[A\n",
      "Epoch 3:  83%|█████████████████████████████████████████▋        | 140/168 [00:24<00:04,  5.84it/s, training_loss=0.167]\u001b[A\n",
      "Epoch 3:  83%|█████████████████████████████████████████▋        | 140/168 [00:24<00:04,  5.84it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 3:  84%|█████████████████████████████████████████▉        | 141/168 [00:24<00:04,  5.86it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 3:  84%|█████████████████████████████████████████▉        | 141/168 [00:24<00:04,  5.86it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 3:  85%|██████████████████████████████████████████▎       | 142/168 [00:24<00:04,  5.87it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 3:  85%|██████████████████████████████████████████▎       | 142/168 [00:24<00:04,  5.87it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 3:  85%|██████████████████████████████████████████▌       | 143/168 [00:24<00:04,  5.87it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 3:  85%|██████████████████████████████████████████▌       | 143/168 [00:24<00:04,  5.87it/s, training_loss=0.053]\u001b[A\n",
      "Epoch 3:  86%|██████████████████████████████████████████▊       | 144/168 [00:24<00:04,  5.86it/s, training_loss=0.053]\u001b[A\n",
      "Epoch 3:  86%|██████████████████████████████████████████▊       | 144/168 [00:24<00:04,  5.86it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 3:  86%|███████████████████████████████████████████▏      | 145/168 [00:24<00:03,  5.85it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 3:  86%|███████████████████████████████████████████▏      | 145/168 [00:25<00:03,  5.85it/s, training_loss=0.110]\u001b[A\n",
      "Epoch 3:  87%|███████████████████████████████████████████▍      | 146/168 [00:25<00:03,  5.85it/s, training_loss=0.110]\u001b[A\n",
      "Epoch 3:  87%|███████████████████████████████████████████▍      | 146/168 [00:25<00:03,  5.85it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  88%|███████████████████████████████████████████▊      | 147/168 [00:25<00:03,  5.87it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  88%|███████████████████████████████████████████▊      | 147/168 [00:25<00:03,  5.87it/s, training_loss=0.171]\u001b[A\n",
      "Epoch 3:  88%|████████████████████████████████████████████      | 148/168 [00:25<00:03,  5.86it/s, training_loss=0.171]\u001b[A\n",
      "Epoch 3:  88%|████████████████████████████████████████████      | 148/168 [00:25<00:03,  5.86it/s, training_loss=0.066]\u001b[A\n",
      "Epoch 3:  89%|████████████████████████████████████████████▎     | 149/168 [00:25<00:03,  5.87it/s, training_loss=0.066]\u001b[A\n",
      "Epoch 3:  89%|████████████████████████████████████████████▎     | 149/168 [00:25<00:03,  5.87it/s, training_loss=0.108]\u001b[A\n",
      "Epoch 3:  89%|████████████████████████████████████████████▋     | 150/168 [00:25<00:03,  5.86it/s, training_loss=0.108]\u001b[A\n",
      "Epoch 3:  89%|████████████████████████████████████████████▋     | 150/168 [00:25<00:03,  5.86it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 3:  90%|████████████████████████████████████████████▉     | 151/168 [00:25<00:02,  5.85it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 3:  90%|████████████████████████████████████████████▉     | 151/168 [00:26<00:02,  5.85it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 3:  90%|█████████████████████████████████████████████▏    | 152/168 [00:26<00:02,  5.86it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 3:  90%|█████████████████████████████████████████████▏    | 152/168 [00:26<00:02,  5.86it/s, training_loss=0.024]\u001b[A\n",
      "Epoch 3:  91%|█████████████████████████████████████████████▌    | 153/168 [00:26<00:02,  5.86it/s, training_loss=0.024]\u001b[A\n",
      "Epoch 3:  91%|█████████████████████████████████████████████▌    | 153/168 [00:26<00:02,  5.86it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 3:  92%|█████████████████████████████████████████████▊    | 154/168 [00:26<00:02,  5.86it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 3:  92%|█████████████████████████████████████████████▊    | 154/168 [00:26<00:02,  5.86it/s, training_loss=0.186]\u001b[A\n",
      "Epoch 3:  92%|██████████████████████████████████████████████▏   | 155/168 [00:26<00:02,  5.85it/s, training_loss=0.186]\u001b[A\n",
      "Epoch 3:  92%|██████████████████████████████████████████████▏   | 155/168 [00:26<00:02,  5.85it/s, training_loss=0.097]\u001b[A\n",
      "Epoch 3:  93%|██████████████████████████████████████████████▍   | 156/168 [00:26<00:02,  5.84it/s, training_loss=0.097]\u001b[A\n",
      "Epoch 3:  93%|██████████████████████████████████████████████▍   | 156/168 [00:26<00:02,  5.84it/s, training_loss=0.140]\u001b[A\n",
      "Epoch 3:  93%|██████████████████████████████████████████████▋   | 157/168 [00:26<00:01,  5.85it/s, training_loss=0.140]\u001b[A\n",
      "Epoch 3:  93%|██████████████████████████████████████████████▋   | 157/168 [00:27<00:01,  5.85it/s, training_loss=0.160]\u001b[A\n",
      "Epoch 3:  94%|███████████████████████████████████████████████   | 158/168 [00:27<00:01,  5.86it/s, training_loss=0.160]\u001b[A\n",
      "Epoch 3:  94%|███████████████████████████████████████████████   | 158/168 [00:27<00:01,  5.86it/s, training_loss=0.043]\u001b[A\n",
      "Epoch 3:  95%|███████████████████████████████████████████████▎  | 159/168 [00:27<00:01,  5.86it/s, training_loss=0.043]\u001b[A\n",
      "Epoch 3:  95%|███████████████████████████████████████████████▎  | 159/168 [00:27<00:01,  5.86it/s, training_loss=0.030]\u001b[A\n",
      "Epoch 3:  95%|███████████████████████████████████████████████▌  | 160/168 [00:27<00:01,  5.87it/s, training_loss=0.030]\u001b[A\n",
      "Epoch 3:  95%|███████████████████████████████████████████████▌  | 160/168 [00:27<00:01,  5.87it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 3:  96%|███████████████████████████████████████████████▉  | 161/168 [00:27<00:01,  5.86it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 3:  96%|███████████████████████████████████████████████▉  | 161/168 [00:27<00:01,  5.86it/s, training_loss=0.075]\u001b[A\n",
      "Epoch 3:  96%|████████████████████████████████████████████████▏ | 162/168 [00:27<00:01,  5.87it/s, training_loss=0.075]\u001b[A\n",
      "Epoch 3:  96%|████████████████████████████████████████████████▏ | 162/168 [00:28<00:01,  5.87it/s, training_loss=0.107]\u001b[A\n",
      "Epoch 3:  97%|████████████████████████████████████████████████▌ | 163/168 [00:28<00:00,  5.87it/s, training_loss=0.107]\u001b[A\n",
      "Epoch 3:  97%|████████████████████████████████████████████████▌ | 163/168 [00:28<00:00,  5.87it/s, training_loss=0.129]\u001b[A\n",
      "Epoch 3:  98%|████████████████████████████████████████████████▊ | 164/168 [00:28<00:00,  5.86it/s, training_loss=0.129]\u001b[A\n",
      "Epoch 3:  98%|████████████████████████████████████████████████▊ | 164/168 [00:28<00:00,  5.86it/s, training_loss=0.049]\u001b[A\n",
      "Epoch 3:  98%|█████████████████████████████████████████████████ | 165/168 [00:28<00:00,  5.86it/s, training_loss=0.049]\u001b[A\n",
      "Epoch 3:  98%|█████████████████████████████████████████████████ | 165/168 [00:28<00:00,  5.86it/s, training_loss=0.143]\u001b[A\n",
      "Epoch 3:  99%|█████████████████████████████████████████████████▍| 166/168 [00:28<00:00,  5.84it/s, training_loss=0.143]\u001b[A\n",
      "Epoch 3:  99%|█████████████████████████████████████████████████▍| 166/168 [00:28<00:00,  5.84it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 3:  99%|█████████████████████████████████████████████████▋| 167/168 [00:28<00:00,  5.87it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 3:  99%|█████████████████████████████████████████████████▋| 167/168 [00:28<00:00,  5.87it/s, training_loss=0.067]\u001b[A\n",
      "Epoch Progress:  75%|███████████████████████████████████████████████████                 | 3/4 [01:25<00:28, 28.47s/it]\u001b[A\n",
      "Epoch 4:   0%|                                                                                 | 0/168 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:   0%|                                                            | 0/168 [00:00<?, ?it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 4:   1%|▎                                                   | 1/168 [00:00<00:27,  5.97it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 4:   1%|▎                                                   | 1/168 [00:00<00:27,  5.97it/s, training_loss=0.103]\u001b[A\n",
      "Epoch 4:   1%|▌                                                   | 2/168 [00:00<00:28,  5.91it/s, training_loss=0.103]\u001b[A\n",
      "Epoch 4:   1%|▌                                                   | 2/168 [00:00<00:28,  5.91it/s, training_loss=0.158]\u001b[A\n",
      "Epoch 4:   2%|▉                                                   | 3/168 [00:00<00:28,  5.84it/s, training_loss=0.158]\u001b[A\n",
      "Epoch 4:   2%|▉                                                   | 3/168 [00:00<00:28,  5.84it/s, training_loss=0.057]\u001b[A\n",
      "Epoch 4:   2%|█▏                                                  | 4/168 [00:00<00:28,  5.85it/s, training_loss=0.057]\u001b[A\n",
      "Epoch 4:   2%|█▏                                                  | 4/168 [00:00<00:28,  5.85it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 4:   3%|█▌                                                  | 5/168 [00:00<00:27,  5.84it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 4:   3%|█▌                                                  | 5/168 [00:01<00:27,  5.84it/s, training_loss=0.023]\u001b[A\n",
      "Epoch 4:   4%|█▊                                                  | 6/168 [00:01<00:27,  5.85it/s, training_loss=0.023]\u001b[A\n",
      "Epoch 4:   4%|█▊                                                  | 6/168 [00:01<00:27,  5.85it/s, training_loss=0.067]\u001b[A\n",
      "Epoch 4:   4%|██▏                                                 | 7/168 [00:01<00:27,  5.87it/s, training_loss=0.067]\u001b[A\n",
      "Epoch 4:   4%|██▏                                                 | 7/168 [00:01<00:27,  5.87it/s, training_loss=0.110]\u001b[A\n",
      "Epoch 4:   5%|██▍                                                 | 8/168 [00:01<00:27,  5.86it/s, training_loss=0.110]\u001b[A\n",
      "Epoch 4:   5%|██▍                                                 | 8/168 [00:01<00:27,  5.86it/s, training_loss=0.193]\u001b[A\n",
      "Epoch 4:   5%|██▊                                                 | 9/168 [00:01<00:27,  5.86it/s, training_loss=0.193]\u001b[A\n",
      "Epoch 4:   5%|██▊                                                 | 9/168 [00:01<00:27,  5.86it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 4:   6%|███                                                | 10/168 [00:01<00:26,  5.87it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 4:   6%|███                                                | 10/168 [00:01<00:26,  5.87it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 4:   7%|███▎                                               | 11/168 [00:01<00:26,  5.87it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 4:   7%|███▎                                               | 11/168 [00:02<00:26,  5.87it/s, training_loss=0.107]\u001b[A\n",
      "Epoch 4:   7%|███▋                                               | 12/168 [00:02<00:26,  5.88it/s, training_loss=0.107]\u001b[A\n",
      "Epoch 4:   7%|███▋                                               | 12/168 [00:02<00:26,  5.88it/s, training_loss=0.103]\u001b[A\n",
      "Epoch 4:   8%|███▉                                               | 13/168 [00:02<00:26,  5.89it/s, training_loss=0.103]\u001b[A\n",
      "Epoch 4:   8%|███▉                                               | 13/168 [00:02<00:26,  5.89it/s, training_loss=0.108]\u001b[A\n",
      "Epoch 4:   8%|████▎                                              | 14/168 [00:02<00:26,  5.88it/s, training_loss=0.108]\u001b[A\n",
      "Epoch 4:   8%|████▎                                              | 14/168 [00:02<00:26,  5.88it/s, training_loss=0.106]\u001b[A\n",
      "Epoch 4:   9%|████▌                                              | 15/168 [00:02<00:26,  5.88it/s, training_loss=0.106]\u001b[A\n",
      "Epoch 4:   9%|████▌                                              | 15/168 [00:02<00:26,  5.88it/s, training_loss=0.104]\u001b[A\n",
      "Epoch 4:  10%|████▊                                              | 16/168 [00:02<00:25,  5.88it/s, training_loss=0.104]\u001b[A\n",
      "Epoch 4:  10%|████▊                                              | 16/168 [00:02<00:25,  5.88it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 4:  10%|█████▏                                             | 17/168 [00:02<00:25,  5.81it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 4:  10%|█████▏                                             | 17/168 [00:03<00:25,  5.81it/s, training_loss=0.029]\u001b[A\n",
      "Epoch 4:  11%|█████▍                                             | 18/168 [00:03<00:25,  5.83it/s, training_loss=0.029]\u001b[A\n",
      "Epoch 4:  11%|█████▍                                             | 18/168 [00:03<00:25,  5.83it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 4:  11%|█████▊                                             | 19/168 [00:03<00:26,  5.71it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 4:  11%|█████▊                                             | 19/168 [00:03<00:26,  5.71it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 4:  12%|██████                                             | 20/168 [00:03<00:25,  5.76it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 4:  12%|██████                                             | 20/168 [00:03<00:25,  5.76it/s, training_loss=0.032]\u001b[A\n",
      "Epoch 4:  12%|██████▍                                            | 21/168 [00:03<00:25,  5.80it/s, training_loss=0.032]\u001b[A\n",
      "Epoch 4:  12%|██████▍                                            | 21/168 [00:03<00:25,  5.80it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 4:  13%|██████▋                                            | 22/168 [00:03<00:25,  5.76it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 4:  13%|██████▋                                            | 22/168 [00:03<00:25,  5.76it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 4:  14%|██████▉                                            | 23/168 [00:03<00:25,  5.79it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 4:  14%|██████▉                                            | 23/168 [00:04<00:25,  5.79it/s, training_loss=0.041]\u001b[A\n",
      "Epoch 4:  14%|███████▎                                           | 24/168 [00:04<00:24,  5.81it/s, training_loss=0.041]\u001b[A\n",
      "Epoch 4:  14%|███████▎                                           | 24/168 [00:04<00:24,  5.81it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 4:  15%|███████▌                                           | 25/168 [00:04<00:24,  5.83it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 4:  15%|███████▌                                           | 25/168 [00:04<00:24,  5.83it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 4:  15%|███████▉                                           | 26/168 [00:04<00:24,  5.85it/s, training_loss=0.085]\u001b[A\n",
      "Epoch 4:  15%|███████▉                                           | 26/168 [00:04<00:24,  5.85it/s, training_loss=0.058]\u001b[A\n",
      "Epoch 4:  16%|████████▏                                          | 27/168 [00:04<00:24,  5.85it/s, training_loss=0.058]\u001b[A\n",
      "Epoch 4:  16%|████████▏                                          | 27/168 [00:04<00:24,  5.85it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 4:  17%|████████▌                                          | 28/168 [00:04<00:23,  5.85it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 4:  17%|████████▌                                          | 28/168 [00:04<00:23,  5.85it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 4:  17%|████████▊                                          | 29/168 [00:04<00:23,  5.87it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 4:  17%|████████▊                                          | 29/168 [00:05<00:23,  5.87it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 4:  18%|█████████                                          | 30/168 [00:05<00:23,  5.88it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 4:  18%|█████████                                          | 30/168 [00:05<00:23,  5.88it/s, training_loss=0.047]\u001b[A\n",
      "Epoch 4:  18%|█████████▍                                         | 31/168 [00:05<00:23,  5.86it/s, training_loss=0.047]\u001b[A\n",
      "Epoch 4:  18%|█████████▍                                         | 31/168 [00:05<00:23,  5.86it/s, training_loss=0.040]\u001b[A\n",
      "Epoch 4:  19%|█████████▋                                         | 32/168 [00:05<00:23,  5.85it/s, training_loss=0.040]\u001b[A\n",
      "Epoch 4:  19%|█████████▋                                         | 32/168 [00:05<00:23,  5.85it/s, training_loss=0.092]\u001b[A\n",
      "Epoch 4:  20%|██████████                                         | 33/168 [00:05<00:23,  5.87it/s, training_loss=0.092]\u001b[A\n",
      "Epoch 4:  20%|██████████                                         | 33/168 [00:05<00:23,  5.87it/s, training_loss=0.062]\u001b[A\n",
      "Epoch 4:  20%|██████████▎                                        | 34/168 [00:05<00:22,  5.87it/s, training_loss=0.062]\u001b[A\n",
      "Epoch 4:  20%|██████████▎                                        | 34/168 [00:05<00:22,  5.87it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 4:  21%|██████████▋                                        | 35/168 [00:05<00:22,  5.88it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 4:  21%|██████████▋                                        | 35/168 [00:06<00:22,  5.88it/s, training_loss=0.026]\u001b[A\n",
      "Epoch 4:  21%|██████████▉                                        | 36/168 [00:06<00:22,  5.87it/s, training_loss=0.026]\u001b[A\n",
      "Epoch 4:  21%|██████████▉                                        | 36/168 [00:06<00:22,  5.87it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 4:  22%|███████████▏                                       | 37/168 [00:06<00:22,  5.87it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 4:  22%|███████████▏                                       | 37/168 [00:06<00:22,  5.87it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 4:  23%|███████████▌                                       | 38/168 [00:06<00:22,  5.87it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 4:  23%|███████████▌                                       | 38/168 [00:06<00:22,  5.87it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 4:  23%|███████████▊                                       | 39/168 [00:06<00:21,  5.88it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 4:  23%|███████████▊                                       | 39/168 [00:06<00:21,  5.88it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 4:  24%|████████████▏                                      | 40/168 [00:06<00:21,  5.86it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 4:  24%|████████████▏                                      | 40/168 [00:07<00:21,  5.86it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 4:  24%|████████████▍                                      | 41/168 [00:07<00:21,  5.86it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 4:  24%|████████████▍                                      | 41/168 [00:07<00:21,  5.86it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 4:  25%|████████████▊                                      | 42/168 [00:07<00:21,  5.86it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 4:  25%|████████████▊                                      | 42/168 [00:07<00:21,  5.86it/s, training_loss=0.030]\u001b[A\n",
      "Epoch 4:  26%|█████████████                                      | 43/168 [00:07<00:21,  5.87it/s, training_loss=0.030]\u001b[A\n",
      "Epoch 4:  26%|█████████████                                      | 43/168 [00:07<00:21,  5.87it/s, training_loss=0.019]\u001b[A\n",
      "Epoch 4:  26%|█████████████▎                                     | 44/168 [00:07<00:21,  5.87it/s, training_loss=0.019]\u001b[A\n",
      "Epoch 4:  26%|█████████████▎                                     | 44/168 [00:07<00:21,  5.87it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 4:  27%|█████████████▋                                     | 45/168 [00:07<00:20,  5.86it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 4:  27%|█████████████▋                                     | 45/168 [00:07<00:20,  5.86it/s, training_loss=0.172]\u001b[A\n",
      "Epoch 4:  27%|█████████████▉                                     | 46/168 [00:07<00:20,  5.87it/s, training_loss=0.172]\u001b[A\n",
      "Epoch 4:  27%|█████████████▉                                     | 46/168 [00:08<00:20,  5.87it/s, training_loss=0.037]\u001b[A\n",
      "Epoch 4:  28%|██████████████▎                                    | 47/168 [00:08<00:20,  5.87it/s, training_loss=0.037]\u001b[A\n",
      "Epoch 4:  28%|██████████████▎                                    | 47/168 [00:08<00:20,  5.87it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 4:  29%|██████████████▌                                    | 48/168 [00:08<00:20,  5.88it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 4:  29%|██████████████▌                                    | 48/168 [00:08<00:20,  5.88it/s, training_loss=0.026]\u001b[A\n",
      "Epoch 4:  29%|██████████████▉                                    | 49/168 [00:08<00:20,  5.87it/s, training_loss=0.026]\u001b[A\n",
      "Epoch 4:  29%|██████████████▉                                    | 49/168 [00:08<00:20,  5.87it/s, training_loss=0.097]\u001b[A\n",
      "Epoch 4:  30%|███████████████▏                                   | 50/168 [00:08<00:20,  5.87it/s, training_loss=0.097]\u001b[A\n",
      "Epoch 4:  30%|███████████████▏                                   | 50/168 [00:08<00:20,  5.87it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 4:  30%|███████████████▍                                   | 51/168 [00:08<00:19,  5.86it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 4:  30%|███████████████▍                                   | 51/168 [00:08<00:19,  5.86it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 4:  31%|███████████████▊                                   | 52/168 [00:08<00:20,  5.76it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 4:  31%|███████████████▊                                   | 52/168 [00:09<00:20,  5.76it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 4:  32%|████████████████                                   | 53/168 [00:09<00:19,  5.77it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 4:  32%|████████████████                                   | 53/168 [00:09<00:19,  5.77it/s, training_loss=0.081]\u001b[A\n",
      "Epoch 4:  32%|████████████████▍                                  | 54/168 [00:09<00:19,  5.80it/s, training_loss=0.081]\u001b[A\n",
      "Epoch 4:  32%|████████████████▍                                  | 54/168 [00:09<00:19,  5.80it/s, training_loss=0.161]\u001b[A\n",
      "Epoch 4:  33%|████████████████▋                                  | 55/168 [00:09<00:19,  5.82it/s, training_loss=0.161]\u001b[A\n",
      "Epoch 4:  33%|████████████████▋                                  | 55/168 [00:09<00:19,  5.82it/s, training_loss=0.146]\u001b[A\n",
      "Epoch 4:  33%|█████████████████                                  | 56/168 [00:09<00:19,  5.82it/s, training_loss=0.146]\u001b[A\n",
      "Epoch 4:  33%|█████████████████                                  | 56/168 [00:09<00:19,  5.82it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 4:  34%|█████████████████▎                                 | 57/168 [00:09<00:18,  5.84it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 4:  34%|█████████████████▎                                 | 57/168 [00:09<00:18,  5.84it/s, training_loss=0.102]\u001b[A\n",
      "Epoch 4:  35%|█████████████████▌                                 | 58/168 [00:09<00:18,  5.84it/s, training_loss=0.102]\u001b[A\n",
      "Epoch 4:  35%|█████████████████▌                                 | 58/168 [00:10<00:18,  5.84it/s, training_loss=0.036]\u001b[A\n",
      "Epoch 4:  35%|█████████████████▉                                 | 59/168 [00:10<00:18,  5.85it/s, training_loss=0.036]\u001b[A\n",
      "Epoch 4:  35%|█████████████████▉                                 | 59/168 [00:10<00:18,  5.85it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 4:  36%|██████████████████▏                                | 60/168 [00:10<00:18,  5.83it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 4:  36%|██████████████████▏                                | 60/168 [00:10<00:18,  5.83it/s, training_loss=0.156]\u001b[A\n",
      "Epoch 4:  36%|██████████████████▌                                | 61/168 [00:10<00:18,  5.83it/s, training_loss=0.156]\u001b[A\n",
      "Epoch 4:  36%|██████████████████▌                                | 61/168 [00:10<00:18,  5.83it/s, training_loss=0.035]\u001b[A\n",
      "Epoch 4:  37%|██████████████████▊                                | 62/168 [00:10<00:18,  5.84it/s, training_loss=0.035]\u001b[A\n",
      "Epoch 4:  37%|██████████████████▊                                | 62/168 [00:10<00:18,  5.84it/s, training_loss=0.132]\u001b[A\n",
      "Epoch 4:  38%|███████████████████▏                               | 63/168 [00:10<00:17,  5.85it/s, training_loss=0.132]\u001b[A\n",
      "Epoch 4:  38%|███████████████████▏                               | 63/168 [00:10<00:17,  5.85it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 4:  38%|███████████████████▍                               | 64/168 [00:10<00:17,  5.85it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 4:  38%|███████████████████▍                               | 64/168 [00:11<00:17,  5.85it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 4:  39%|███████████████████▋                               | 65/168 [00:11<00:17,  5.86it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 4:  39%|███████████████████▋                               | 65/168 [00:11<00:17,  5.86it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 4:  39%|████████████████████                               | 66/168 [00:11<00:17,  5.87it/s, training_loss=0.044]\u001b[A\n",
      "Epoch 4:  39%|████████████████████                               | 66/168 [00:11<00:17,  5.87it/s, training_loss=0.071]\u001b[A\n",
      "Epoch 4:  40%|████████████████████▎                              | 67/168 [00:11<00:17,  5.87it/s, training_loss=0.071]\u001b[A\n",
      "Epoch 4:  40%|████████████████████▎                              | 67/168 [00:11<00:17,  5.87it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 4:  40%|████████████████████▋                              | 68/168 [00:11<00:17,  5.87it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 4:  40%|████████████████████▋                              | 68/168 [00:11<00:17,  5.87it/s, training_loss=0.060]\u001b[A\n",
      "Epoch 4:  41%|████████████████████▉                              | 69/168 [00:11<00:16,  5.85it/s, training_loss=0.060]\u001b[A\n",
      "Epoch 4:  41%|████████████████████▉                              | 69/168 [00:11<00:16,  5.85it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 4:  42%|█████████████████████▎                             | 70/168 [00:11<00:16,  5.86it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 4:  42%|█████████████████████▎                             | 70/168 [00:12<00:16,  5.86it/s, training_loss=0.024]\u001b[A\n",
      "Epoch 4:  42%|█████████████████████▌                             | 71/168 [00:12<00:16,  5.85it/s, training_loss=0.024]\u001b[A\n",
      "Epoch 4:  42%|█████████████████████▌                             | 71/168 [00:12<00:16,  5.85it/s, training_loss=0.035]\u001b[A\n",
      "Epoch 4:  43%|█████████████████████▊                             | 72/168 [00:12<00:16,  5.85it/s, training_loss=0.035]\u001b[A\n",
      "Epoch 4:  43%|█████████████████████▊                             | 72/168 [00:12<00:16,  5.85it/s, training_loss=0.039]\u001b[A\n",
      "Epoch 4:  43%|██████████████████████▏                            | 73/168 [00:12<00:16,  5.84it/s, training_loss=0.039]\u001b[A"
     ]
    }
   ],
   "source": [
    " #install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def custom_train_test_split(df, test_size=0.2, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df[['ade', 'soc_code']]\n",
    "    y = df['label']\n",
    "    \n",
    "    # Identify classes and their counts\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    # Identify small classes\n",
    "    small_classes = classes[counts < 5]\n",
    "    \n",
    "    # Initialize lists for train and test sets\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Handle small classes separately\n",
    "    for cls in small_classes:\n",
    "        cls_mask = (y == cls)\n",
    "        cls_X = X[cls_mask]\n",
    "        cls_y = y[cls_mask]\n",
    "        cls_idx = df.index[cls_mask].tolist()\n",
    "        \n",
    "        if len(cls_X) == 1:\n",
    "            # If only one instance, put it in test set\n",
    "            test_indices.append(cls_idx[0])\n",
    "        else:\n",
    "            # Randomly choose one instance for testing\n",
    "            test_idx = np.random.choice(len(cls_X))\n",
    "            test_indices.append(cls_idx[test_idx])\n",
    "            \n",
    "            # Remaining instances go to training\n",
    "            train_indices.extend(np.delete(cls_idx, test_idx))\n",
    "    \n",
    "    # Combine the small class data into test and train sets\n",
    "    test_indices = np.array(test_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "    \n",
    "    X_test = df.loc[test_indices]\n",
    "    y_test = X_test['label']\n",
    "    \n",
    "    X_train = df.loc[train_indices]\n",
    "    y_train = X_train['label']\n",
    "    \n",
    "    # Handle large classes with stratified split\n",
    "    large_class_mask = ~np.isin(y, small_classes)\n",
    "    X_large = X[large_class_mask]\n",
    "    y_large = y[large_class_mask]\n",
    "    \n",
    "    X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "        X_large, y_large, test_size=test_size, random_state=random_state, stratify=y_large\n",
    "    )\n",
    "    \n",
    "    # Combine large class data with the small class data\n",
    "    X_train = pd.concat([X_train, X_train_large], axis=0)\n",
    "    y_train = pd.concat([y_train, y_train_large], axis=0)\n",
    "    \n",
    "    X_test = pd.concat([X_test, X_test_large], axis=0)\n",
    "    y_test = pd.concat([y_test, y_test_large], axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#evaluation\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "\n",
    "    return accuracy_dict, count_dict\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "\n",
    "# Function to calculate precision, recall, and F1 for each label\n",
    "def calculate_metrics(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score per label\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_flat, pred_flat, average=None, labels=np.unique(labels_flat))\n",
    "    \n",
    "    return precision, recall, f1\n",
    "    \n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='cadec_all_training_40ep_16bs_5e-5lr_log_fix.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "cadec_csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "# read data from smm4h\n",
    "smm4h_csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n",
    "\n",
    "allSMM4H = [10037175, 10018065,10029205, 10017947, 10028395, 10022891, 10027433, 10040785, 10038738, 10022117, 10015919, 10038604, 10047065, \n",
    "            10021428,10041244, 10007541, 10038359, 10021881, 10013993, 10019805, 10042613, 10029104, 10077536, 10010331, 10014698]\n",
    "\n",
    "label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2,\n",
    "    10017947: 3,\n",
    "    10028395: 4,\n",
    "    10022891: 5,\n",
    "    10027433: 6,\n",
    "    10040785: 7,\n",
    "    10038738: 8,\n",
    "    10022117: 9,\n",
    "    10015919: 10,\n",
    "    10038604: 11,\n",
    "    10047065: 12,\n",
    "    10021428: 13,\n",
    "    10041244: 14,\n",
    "    10007541: 15,\n",
    "    10038359: 16,\n",
    "    10021881: 17,\n",
    "    10013993: 18,\n",
    "    10019805: 19,\n",
    "    10042613: 20,\n",
    "    10029104: 21,\n",
    "    10077536: 22,\n",
    "    10010331: 23,\n",
    "    10014698: 24\n",
    "}\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"ade\", \"soc_code\"]\n",
    "smm4h_all = pd.read_csv(smm4h_csv_url,names=column_names, sep = '\\t', header=None)\n",
    "print(\"smm4h data:\",smm4h_all.shape)\n",
    "\n",
    "smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n",
    "smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n",
    "\n",
    "smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n",
    "\n",
    "print(\"smm4h data after filtering:\",smm4h_all.shape)\n",
    "smm4h_soc_code_counts = smm4h_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "print(\"SOC count in SMM4H: \",smm4h_soc_code_counts)\n",
    "# Filter DataFrame\n",
    "smm4h_filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(allSMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "allinSMM4H = smm4h_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(cadec_csv_url,names=column_names, header=None)\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(\"clean cadec data:\",cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "cadec_soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "print(\"SOC count in CADEC: \",cadec_soc_code_counts)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "cadec_filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(allSMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECallinSMM4H = cadec_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "\n",
    "# For SMM4H data\n",
    "df1 = allinSMM4H.copy()\n",
    "df1.loc[:, 'label'] = df1['soc_code'].map(label_dict)\n",
    "\n",
    "# For CADEC data\n",
    "df2 = CADECallinSMM4H.copy()\n",
    "df2.loc[:, 'label'] = df2['soc_code'].map(label_dict)\n",
    "\n",
    "print(\"SMM4H :\",df1)\n",
    "print(\"CADEC :\",df2)\n",
    "\n",
    "#cadec data\n",
    "df = df2\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 42, 2))\n",
    "batch_size = 16\n",
    "epochs = 4\n",
    "learningrate = 5e-5\n",
    "\n",
    "# Placeholder for accuracies\n",
    "all_accuracies = {label: [] for label in range(len(label_dict))}\n",
    "\n",
    "# Initialize dictionaries to hold metrics for each seed\n",
    "seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': []} for seed_val in seed_values}\n",
    "seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'confusion_matrix': []} for seed_val in seed_values}\n",
    "\n",
    "\n",
    "# Main loop over seed values\n",
    "for seed_val in seed_values:\n",
    "    # Set seeds\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Data preparation\n",
    "    # Apply the custom train-test split\n",
    "    X_train, X_val, y_train, y_val = custom_train_test_split(df, test_size=0.2, random_state=seed_val)\n",
    "    \n",
    "    # Add data_type column\n",
    "    df['data_type'] = 'not_set'\n",
    "    df.loc[X_train.index, 'data_type'] = 'train'\n",
    "    df.loc[X_val.index, 'data_type'] = 'val'\n",
    "\n",
    "    logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    print(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'train'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'val'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n",
    "\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict), output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learningrate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    logger.info(f\"Device used: {device}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "\n",
    "        # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n",
    "\n",
    "        logger.info(f'\\nEpoch {epoch}')\n",
    "        loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "        logger.info(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_vals.flatten(), np.argmax(predictions, axis=1).flatten(), average=None, labels=np.unique(true_vals.flatten()))\n",
    "\n",
    "    # issue with none label.\n",
    "    # for label in np.unique(true_vals):\n",
    "    for label in label_dict.values():\n",
    "        seed_metrics[seed_val]['precision'].append((label, precision[label]))\n",
    "        seed_metrics[seed_val]['recall'].append((label, recall[label]))\n",
    "        seed_metrics[seed_val]['f1'].append((label, f1[label]))\n",
    "        logger.info(f'Seed: {seed_val}, label: {label}, F1 : {f1[label]}')\n",
    "\n",
    "# Write the precision, recall, F1 scores, and seed values to a file\n",
    "with open('CADEC_all_20times_results_with_seeds_fix.txt', 'w') as f:\n",
    "    f.write('Seed\\tLabel\\tPrecision\\tRecall\\tF1\\n')\n",
    "    for seed_val in seed_values:\n",
    "        for label, precision_val in seed_metrics[seed_val]['precision']:\n",
    "            recall_val = next(val for lbl, val in seed_metrics[seed_val]['recall'] if lbl == label)\n",
    "            f1_val = next(val for lbl, val in seed_metrics[seed_val]['f1'] if lbl == label)\n",
    "            f.write(f'{seed_val}\\t{label}\\t{precision_val:.4f}\\t{recall_val:.4f}\\t{f1_val:.4f}\\n')\n",
    "\n",
    "# Initialize lists to hold precision, recall, and f1 values for each label\n",
    "precision_dict, recall_dict, f1_dict = {}, {}, {}\n",
    "\n",
    "# Collect metrics across seeds\n",
    "for seed in seed_metrics:\n",
    "    for label, value in seed_metrics[seed]['precision']:\n",
    "        precision_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['recall']:\n",
    "        recall_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['f1']:\n",
    "        f1_dict.setdefault(label, []).append(value)\n",
    "\n",
    "# Compute mean and std for precision, recall, and f1\n",
    "labels = sorted(precision_dict.keys())\n",
    "precision_mean = [np.mean(precision_dict[label]) for label in labels]\n",
    "precision_std = [np.std(precision_dict[label]) for label in labels]\n",
    "recall_mean = [np.mean(recall_dict[label]) for label in labels]\n",
    "recall_std = [np.std(recall_dict[label]) for label in labels]\n",
    "f1_mean = [np.mean(f1_dict[label]) for label in labels]\n",
    "f1_std = [np.std(f1_dict[label]) for label in labels]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(labels))  # label indices\n",
    "width = 0.25  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Bar plots with mean values\n",
    "bars_precision = ax.bar(x - width, precision_mean, width, label='Precision', color='b')\n",
    "bars_recall = ax.bar(x, recall_mean, width, label='Recall', color='g')\n",
    "bars_f1 = ax.bar(x + width, f1_mean, width, label='F1 Score', color='r')\n",
    "\n",
    "# # Annotate bars with mean and std values\n",
    "# Annotate bars with mean and std values, with smaller font size\n",
    "# for bars, means, stds in zip([bars_precision, bars_recall, bars_f1],\n",
    "#                              [precision_mean, recall_mean, f1_mean],\n",
    "#                              [precision_std, recall_std, f1_std]):\n",
    "#     for bar, mean, std in zip(bars, means, stds):\n",
    "#         height = bar.get_height()\n",
    "#         ax.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "#                 f'{mean:.2f}\\n±{std:.2f}', ha='center', va='bottom', fontsize=8)  # Smaller font size\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_title('Mean and Standard Deviation of Precision, Recall, and F1 Score by Label')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Set y-axis limit to [0, 1]\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Move legend outside the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the plot to fit the legend\n",
    "plt.savefig('CADEC_all_20times_results_plot_fix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fcbd3a-a563-43ad-b3b4-c27f1ecd1b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
