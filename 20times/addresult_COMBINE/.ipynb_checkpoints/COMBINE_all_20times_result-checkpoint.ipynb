{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3056cfc-5af3-4370-a0ee-badee5783431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "smm4h data: (1712, 2)\n",
      "smm4h data after filtering: (1710, 2)\n",
      "SOC count in SMM4H:  soc_code\n",
      "10037175    287\n",
      "10018065    235\n",
      "10029205    212\n",
      "10017947     63\n",
      "10028395     58\n",
      "10022891     54\n",
      "10027433     48\n",
      "10040785     28\n",
      "10038738     22\n",
      "10022117     16\n",
      "10015919     16\n",
      "10038604     10\n",
      "10047065     10\n",
      "10021428      8\n",
      "10041244      7\n",
      "10007541      7\n",
      "10038359      6\n",
      "10021881      5\n",
      "10013993      4\n",
      "10019805      2\n",
      "10042613      2\n",
      "10029104      2\n",
      "10077536      1\n",
      "10010331      1\n",
      "10014698      1\n",
      "Name: count, dtype: Int64\n",
      "SOC count in CADEC:  soc_code\n",
      "10028395    962\n",
      "10018065    654\n",
      "10037175    401\n",
      "10017947    300\n",
      "10029205    286\n",
      "10040785    184\n",
      "10007541     92\n",
      "10038738     91\n",
      "10022891     82\n",
      "10015919     67\n",
      "10038604     59\n",
      "10038359     50\n",
      "10022117     35\n",
      "10047065     25\n",
      "10013993     16\n",
      "10019805     15\n",
      "10041244      7\n",
      "10027433      6\n",
      "10021881      5\n",
      "10021428      4\n",
      "10014698      3\n",
      "10005329      3\n",
      "10029104      1\n",
      "Name: count, dtype: int64\n",
      "SMM4H :                             ade  soc_code  label\n",
      "1                     allergies  10021428     13\n",
      "2               HURT YOUR Liver  10019805     19\n",
      "3                            AD  10037175      0\n",
      "4                         focus  10029205      2\n",
      "5                          died  10018065      1\n",
      "...                         ...       ...    ...\n",
      "1703                 chest hurt  10018065      1\n",
      "1704   got ten minutes of sleep  10037175      0\n",
      "1706                  Nosebleed  10038738      8\n",
      "1708  never have another orgasm  10037175      0\n",
      "1710        gain so much weight  10022891      5\n",
      "\n",
      "[1105 rows x 3 columns]\n",
      "CADEC :                                    ade  soc_code  label\n",
      "3                      ankles swelling  10007541     15\n",
      "4             sever swelling of ankles  10007541     15\n",
      "5                      Edema of ankles  10007541     15\n",
      "6                    severe arrythmias  10007541     15\n",
      "7                            arrythmia  10007541     15\n",
      "...                                ...       ...    ...\n",
      "5955                      hypertension  10047065     12\n",
      "5956           Elevated blood pressure  10047065     12\n",
      "5959  LITTLE CIRCULATION IN MY FINGERS  10047065     12\n",
      "5960                  going into shock  10047065     12\n",
      "5961     vein in my one leg is bulging  10047065     12\n",
      "\n",
      "[3345 rows x 3 columns]\n",
      "df:                                     ade  soc_code  label data_type\n",
      "1                            allergies  10021428     13     train\n",
      "2                      HURT YOUR Liver  10019805     19     train\n",
      "3                                   AD  10037175      0     train\n",
      "4                                focus  10029205      2     train\n",
      "5                                 died  10018065      1     train\n",
      "...                                ...       ...    ...       ...\n",
      "5955                      hypertension  10047065     12     train\n",
      "5956           Elevated blood pressure  10047065     12     train\n",
      "5959  LITTLE CIRCULATION IN MY FINGERS  10047065     12       val\n",
      "5960                  going into shock  10047065     12     train\n",
      "5961     vein in my one leg is bulging  10047065     12     train\n",
      "\n",
      "[4450 rows x 4 columns]\n",
      "                          ade\n",
      "soc_code label data_type     \n",
      "10007541 15    train       79\n",
      "               val         20\n",
      "10010331 23    val          1\n",
      "10013993 18    train       16\n",
      "               val          4\n",
      "10014698 24    train        2\n",
      "               val          2\n",
      "10015919 10    train       67\n",
      "               val         16\n",
      "10017947 3     train      290\n",
      "               val         73\n",
      "10018065 1     train      711\n",
      "               val        178\n",
      "10019805 19    train       13\n",
      "               val          4\n",
      "10021428 13    train        9\n",
      "               val          3\n",
      "10021881 17    train        8\n",
      "               val          2\n",
      "10022117 9     train       41\n",
      "               val         10\n",
      "10022891 5     train      108\n",
      "               val         28\n",
      "10027433 6     train       43\n",
      "               val         11\n",
      "10028395 4     train      815\n",
      "               val        205\n",
      "10029104 21    train        1\n",
      "               val          2\n",
      "10029205 2     train      399\n",
      "               val         99\n",
      "10037175 0     train      550\n",
      "               val        138\n",
      "10038359 16    train       45\n",
      "               val         11\n",
      "10038604 11    train       55\n",
      "               val         14\n",
      "10038738 8     train       91\n",
      "               val         22\n",
      "10040785 7     train      169\n",
      "               val         43\n",
      "10041244 14    train       12\n",
      "               val          2\n",
      "10042613 20    train        1\n",
      "               val          1\n",
      "10047065 12    train       28\n",
      "               val          7\n",
      "10077536 22    val          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                                       | 0/223 [00:00<?, ?it/s]\u001b[AC:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "Epoch 1:   0%|                                                                                  | 0/223 [00:01<?, ?it/s, training_loss=1.048]\u001b[A\n",
      "Epoch 1:   0%|▎                                                                         | 1/223 [00:01<04:02,  1.09s/it, training_loss=1.048]\u001b[A\n",
      "Epoch 1:   0%|▎                                                                         | 1/223 [00:01<04:02,  1.09s/it, training_loss=1.070]\u001b[A\n",
      "Epoch 1:   1%|▋                                                                         | 2/223 [00:01<02:48,  1.32it/s, training_loss=1.070]\u001b[A\n",
      "Epoch 1:   1%|▋                                                                         | 2/223 [00:02<02:48,  1.32it/s, training_loss=1.049]\u001b[A\n",
      "Epoch 1:   1%|▉                                                                         | 3/223 [00:02<02:25,  1.51it/s, training_loss=1.049]\u001b[A\n",
      "Epoch 1:   1%|▉                                                                         | 3/223 [00:02<02:25,  1.51it/s, training_loss=1.015]\u001b[A\n",
      "Epoch 1:   2%|█▎                                                                        | 4/223 [00:02<02:07,  1.72it/s, training_loss=1.015]\u001b[A\n",
      "Epoch 1:   2%|█▎                                                                        | 4/223 [00:03<02:07,  1.72it/s, training_loss=1.017]\u001b[A\n",
      "Epoch 1:   2%|█▋                                                                        | 5/223 [00:03<02:00,  1.81it/s, training_loss=1.017]\u001b[A\n",
      "Epoch 1:   2%|█▋                                                                        | 5/223 [00:03<02:00,  1.81it/s, training_loss=0.971]\u001b[A\n",
      "Epoch 1:   3%|█▉                                                                        | 6/223 [00:03<01:57,  1.85it/s, training_loss=0.971]\u001b[A\n",
      "Epoch 1:   3%|█▉                                                                        | 6/223 [00:04<01:57,  1.85it/s, training_loss=0.968]\u001b[A\n",
      "Epoch 1:   3%|██▎                                                                       | 7/223 [00:04<01:54,  1.89it/s, training_loss=0.968]\u001b[A\n",
      "Epoch 1:   3%|██▎                                                                       | 7/223 [00:04<01:54,  1.89it/s, training_loss=0.961]\u001b[A\n",
      "Epoch 1:   4%|██▋                                                                       | 8/223 [00:04<01:52,  1.91it/s, training_loss=0.961]\u001b[A\n",
      "Epoch 1:   4%|██▋                                                                       | 8/223 [00:05<01:52,  1.91it/s, training_loss=0.983]\u001b[A\n",
      "Epoch 1:   4%|██▉                                                                       | 9/223 [00:05<01:49,  1.96it/s, training_loss=0.983]\u001b[A\n",
      "Epoch 1:   4%|██▉                                                                       | 9/223 [00:05<01:49,  1.96it/s, training_loss=0.879]\u001b[A\n",
      "Epoch 1:   4%|███▎                                                                     | 10/223 [00:05<01:47,  1.98it/s, training_loss=0.879]\u001b[A\n",
      "Epoch 1:   4%|███▎                                                                     | 10/223 [00:06<01:47,  1.98it/s, training_loss=0.905]\u001b[A\n",
      "Epoch 1:   5%|███▌                                                                     | 11/223 [00:06<01:45,  2.00it/s, training_loss=0.905]\u001b[A\n",
      "Epoch 1:   5%|███▌                                                                     | 11/223 [00:06<01:45,  2.00it/s, training_loss=0.941]\u001b[A\n",
      "Epoch 1:   5%|███▉                                                                     | 12/223 [00:06<01:45,  1.99it/s, training_loss=0.941]\u001b[A\n",
      "Epoch 1:   5%|███▉                                                                     | 12/223 [00:07<01:45,  1.99it/s, training_loss=0.861]\u001b[A\n",
      "Epoch 1:   6%|████▎                                                                    | 13/223 [00:07<01:46,  1.98it/s, training_loss=0.861]\u001b[A\n",
      "Epoch 1:   6%|████▎                                                                    | 13/223 [00:07<01:46,  1.98it/s, training_loss=0.803]\u001b[A\n",
      "Epoch 1:   6%|████▌                                                                    | 14/223 [00:07<01:45,  1.98it/s, training_loss=0.803]\u001b[A\n",
      "Epoch 1:   6%|████▌                                                                    | 14/223 [00:08<01:45,  1.98it/s, training_loss=0.735]\u001b[A\n",
      "Epoch 1:   7%|████▉                                                                    | 15/223 [00:08<01:44,  1.98it/s, training_loss=0.735]\u001b[A\n",
      "Epoch 1:   7%|████▉                                                                    | 15/223 [00:08<01:44,  1.98it/s, training_loss=0.626]\u001b[A\n",
      "Epoch 1:   7%|█████▏                                                                   | 16/223 [00:08<01:43,  1.99it/s, training_loss=0.626]\u001b[A\n",
      "Epoch 1:   7%|█████▏                                                                   | 16/223 [00:09<01:43,  1.99it/s, training_loss=0.906]\u001b[A\n",
      "Epoch 1:   8%|█████▌                                                                   | 17/223 [00:09<01:42,  2.01it/s, training_loss=0.906]\u001b[A\n",
      "Epoch 1:   8%|█████▌                                                                   | 17/223 [00:09<01:42,  2.01it/s, training_loss=0.868]\u001b[A\n",
      "Epoch 1:   8%|█████▉                                                                   | 18/223 [00:09<01:43,  1.98it/s, training_loss=0.868]\u001b[A\n",
      "Epoch 1:   8%|█████▉                                                                   | 18/223 [00:10<01:43,  1.98it/s, training_loss=0.920]\u001b[A\n",
      "Epoch 1:   9%|██████▏                                                                  | 19/223 [00:10<01:44,  1.95it/s, training_loss=0.920]\u001b[A\n",
      "Epoch 1:   9%|██████▏                                                                  | 19/223 [00:10<01:44,  1.95it/s, training_loss=0.957]\u001b[A\n",
      "Epoch 1:   9%|██████▌                                                                  | 20/223 [00:10<01:42,  1.97it/s, training_loss=0.957]\u001b[A\n",
      "Epoch 1:   9%|██████▌                                                                  | 20/223 [00:11<01:42,  1.97it/s, training_loss=0.818]\u001b[A\n",
      "Epoch 1:   9%|██████▊                                                                  | 21/223 [00:11<01:40,  2.01it/s, training_loss=0.818]\u001b[A\n",
      "Epoch 1:   9%|██████▊                                                                  | 21/223 [00:11<01:40,  2.01it/s, training_loss=0.811]\u001b[A\n",
      "Epoch 1:  10%|███████▏                                                                 | 22/223 [00:11<01:39,  2.01it/s, training_loss=0.811]\u001b[A\n",
      "Epoch 1:  10%|███████▏                                                                 | 22/223 [00:12<01:39,  2.01it/s, training_loss=0.846]\u001b[A\n",
      "Epoch 1:  10%|███████▌                                                                 | 23/223 [00:12<01:38,  2.03it/s, training_loss=0.846]\u001b[A\n",
      "Epoch 1:  10%|███████▌                                                                 | 23/223 [00:12<01:38,  2.03it/s, training_loss=0.815]\u001b[A\n",
      "Epoch 1:  11%|███████▊                                                                 | 24/223 [00:12<01:38,  2.02it/s, training_loss=0.815]\u001b[A\n",
      "Epoch 1:  11%|███████▊                                                                 | 24/223 [00:13<01:38,  2.02it/s, training_loss=0.752]\u001b[A\n",
      "Epoch 1:  11%|████████▏                                                                | 25/223 [00:13<01:37,  2.04it/s, training_loss=0.752]\u001b[A\n",
      "Epoch 1:  11%|████████▏                                                                | 25/223 [00:13<01:37,  2.04it/s, training_loss=0.647]\u001b[A\n",
      "Epoch 1:  12%|████████▌                                                                | 26/223 [00:13<01:36,  2.04it/s, training_loss=0.647]\u001b[A\n",
      "Epoch 1:  12%|████████▌                                                                | 26/223 [00:14<01:36,  2.04it/s, training_loss=0.893]\u001b[A\n",
      "Epoch 1:  12%|████████▊                                                                | 27/223 [00:14<01:35,  2.04it/s, training_loss=0.893]\u001b[A\n",
      "Epoch 1:  12%|████████▊                                                                | 27/223 [00:14<01:35,  2.04it/s, training_loss=0.779]\u001b[A\n",
      "Epoch 1:  13%|█████████▏                                                               | 28/223 [00:14<01:37,  2.00it/s, training_loss=0.779]\u001b[A\n",
      "Epoch 1:  13%|█████████▏                                                               | 28/223 [00:15<01:37,  2.00it/s, training_loss=0.680]\u001b[A\n",
      "Epoch 1:  13%|█████████▍                                                               | 29/223 [00:15<01:35,  2.02it/s, training_loss=0.680]\u001b[A\n",
      "Epoch 1:  13%|█████████▍                                                               | 29/223 [00:15<01:35,  2.02it/s, training_loss=0.657]\u001b[A\n",
      "Epoch 1:  13%|█████████▊                                                               | 30/223 [00:15<01:35,  2.03it/s, training_loss=0.657]\u001b[A\n",
      "Epoch 1:  13%|█████████▊                                                               | 30/223 [00:16<01:35,  2.03it/s, training_loss=0.599]\u001b[A\n",
      "Epoch 1:  14%|██████████▏                                                              | 31/223 [00:16<01:34,  2.03it/s, training_loss=0.599]\u001b[A\n",
      "Epoch 1:  14%|██████████▏                                                              | 31/223 [00:16<01:34,  2.03it/s, training_loss=0.760]\u001b[A\n",
      "Epoch 1:  14%|██████████▍                                                              | 32/223 [00:16<01:34,  2.02it/s, training_loss=0.760]\u001b[A\n",
      "Epoch 1:  14%|██████████▍                                                              | 32/223 [00:17<01:34,  2.02it/s, training_loss=0.882]\u001b[A\n",
      "Epoch 1:  15%|██████████▊                                                              | 33/223 [00:17<01:33,  2.03it/s, training_loss=0.882]\u001b[A\n",
      "Epoch 1:  15%|██████████▊                                                              | 33/223 [00:17<01:33,  2.03it/s, training_loss=0.746]\u001b[A\n",
      "Epoch 1:  15%|███████████▏                                                             | 34/223 [00:17<01:33,  2.01it/s, training_loss=0.746]\u001b[A\n",
      "Epoch 1:  15%|███████████▏                                                             | 34/223 [00:18<01:33,  2.01it/s, training_loss=0.726]\u001b[A\n",
      "Epoch 1:  16%|███████████▍                                                             | 35/223 [00:18<01:33,  2.02it/s, training_loss=0.726]\u001b[A\n",
      "Epoch 1:  16%|███████████▍                                                             | 35/223 [00:18<01:33,  2.02it/s, training_loss=0.747]\u001b[A\n",
      "Epoch 1:  16%|███████████▊                                                             | 36/223 [00:18<01:32,  2.02it/s, training_loss=0.747]\u001b[A\n",
      "Epoch 1:  16%|███████████▊                                                             | 36/223 [00:19<01:32,  2.02it/s, training_loss=0.740]\u001b[A\n",
      "Epoch 1:  17%|████████████                                                             | 37/223 [00:19<01:31,  2.03it/s, training_loss=0.740]\u001b[A\n",
      "Epoch 1:  17%|████████████                                                             | 37/223 [00:19<01:31,  2.03it/s, training_loss=0.812]\u001b[A\n",
      "Epoch 1:  17%|████████████▍                                                            | 38/223 [00:19<01:31,  2.03it/s, training_loss=0.812]\u001b[A\n",
      "Epoch 1:  17%|████████████▍                                                            | 38/223 [00:20<01:31,  2.03it/s, training_loss=0.717]\u001b[A\n",
      "Epoch 1:  17%|████████████▊                                                            | 39/223 [00:20<01:30,  2.04it/s, training_loss=0.717]\u001b[A\n",
      "Epoch 1:  17%|████████████▊                                                            | 39/223 [00:20<01:30,  2.04it/s, training_loss=0.718]\u001b[A\n",
      "Epoch 1:  18%|█████████████                                                            | 40/223 [00:20<01:29,  2.04it/s, training_loss=0.718]\u001b[A\n",
      "Epoch 1:  18%|█████████████                                                            | 40/223 [00:21<01:29,  2.04it/s, training_loss=0.520]\u001b[A\n",
      "Epoch 1:  18%|█████████████▍                                                           | 41/223 [00:21<01:29,  2.02it/s, training_loss=0.520]\u001b[A\n",
      "Epoch 1:  18%|█████████████▍                                                           | 41/223 [00:21<01:29,  2.02it/s, training_loss=0.736]\u001b[A\n",
      "Epoch 1:  19%|█████████████▋                                                           | 42/223 [00:21<01:29,  2.03it/s, training_loss=0.736]\u001b[A\n",
      "Epoch 1:  19%|█████████████▋                                                           | 42/223 [00:22<01:29,  2.03it/s, training_loss=0.672]\u001b[A\n",
      "Epoch 1:  19%|██████████████                                                           | 43/223 [00:22<01:29,  2.02it/s, training_loss=0.672]\u001b[A\n",
      "Epoch 1:  19%|██████████████                                                           | 43/223 [00:22<01:29,  2.02it/s, training_loss=0.845]\u001b[A\n",
      "Epoch 1:  20%|██████████████▍                                                          | 44/223 [00:22<01:27,  2.04it/s, training_loss=0.845]\u001b[A\n",
      "Epoch 1:  20%|██████████████▍                                                          | 44/223 [00:22<01:27,  2.04it/s, training_loss=0.726]\u001b[A\n",
      "Epoch 1:  20%|██████████████▋                                                          | 45/223 [00:22<01:27,  2.04it/s, training_loss=0.726]\u001b[A\n",
      "Epoch 1:  20%|██████████████▋                                                          | 45/223 [00:23<01:27,  2.04it/s, training_loss=0.678]\u001b[A\n",
      "Epoch 1:  21%|███████████████                                                          | 46/223 [00:23<01:26,  2.04it/s, training_loss=0.678]\u001b[A\n",
      "Epoch 1:  21%|███████████████                                                          | 46/223 [00:23<01:26,  2.04it/s, training_loss=0.631]\u001b[A\n",
      "Epoch 1:  21%|███████████████▍                                                         | 47/223 [00:23<01:26,  2.04it/s, training_loss=0.631]\u001b[A\n",
      "Epoch 1:  21%|███████████████▍                                                         | 47/223 [00:24<01:26,  2.04it/s, training_loss=0.784]\u001b[A\n",
      "Epoch 1:  22%|███████████████▋                                                         | 48/223 [00:24<01:26,  2.03it/s, training_loss=0.784]\u001b[A\n",
      "Epoch 1:  22%|███████████████▋                                                         | 48/223 [00:24<01:26,  2.03it/s, training_loss=0.558]\u001b[A\n",
      "Epoch 1:  22%|████████████████                                                         | 49/223 [00:24<01:24,  2.05it/s, training_loss=0.558]\u001b[A\n",
      "Epoch 1:  22%|████████████████                                                         | 49/223 [00:25<01:24,  2.05it/s, training_loss=0.729]\u001b[A\n",
      "Epoch 1:  22%|████████████████▎                                                        | 50/223 [00:25<01:25,  2.03it/s, training_loss=0.729]\u001b[A\n",
      "Epoch 1:  22%|████████████████▎                                                        | 50/223 [00:25<01:25,  2.03it/s, training_loss=0.831]\u001b[A\n",
      "Epoch 1:  23%|████████████████▋                                                        | 51/223 [00:25<01:23,  2.06it/s, training_loss=0.831]\u001b[A\n",
      "Epoch 1:  23%|████████████████▋                                                        | 51/223 [00:26<01:23,  2.06it/s, training_loss=0.587]\u001b[A\n",
      "Epoch 1:  23%|█████████████████                                                        | 52/223 [00:26<01:22,  2.06it/s, training_loss=0.587]\u001b[A\n",
      "Epoch 1:  23%|█████████████████                                                        | 52/223 [00:26<01:22,  2.06it/s, training_loss=0.643]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▎                                                       | 53/223 [00:26<01:22,  2.07it/s, training_loss=0.643]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▎                                                       | 53/223 [00:27<01:22,  2.07it/s, training_loss=0.702]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▋                                                       | 54/223 [00:27<01:22,  2.05it/s, training_loss=0.702]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▋                                                       | 54/223 [00:27<01:22,  2.05it/s, training_loss=0.752]\u001b[A\n",
      "Epoch 1:  25%|██████████████████                                                       | 55/223 [00:27<01:22,  2.04it/s, training_loss=0.752]\u001b[A\n",
      "Epoch 1:  25%|██████████████████                                                       | 55/223 [00:28<01:22,  2.04it/s, training_loss=0.790]\u001b[A\n",
      "Epoch 1:  25%|██████████████████▎                                                      | 56/223 [00:28<01:21,  2.05it/s, training_loss=0.790]\u001b[A\n",
      "Epoch 1:  25%|██████████████████▎                                                      | 56/223 [00:28<01:21,  2.05it/s, training_loss=0.737]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▋                                                      | 57/223 [00:28<01:21,  2.04it/s, training_loss=0.737]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▋                                                      | 57/223 [00:29<01:21,  2.04it/s, training_loss=0.626]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▉                                                      | 58/223 [00:29<01:20,  2.05it/s, training_loss=0.626]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▉                                                      | 58/223 [00:29<01:20,  2.05it/s, training_loss=0.453]\u001b[A\n",
      "Epoch 1:  26%|███████████████████▎                                                     | 59/223 [00:29<01:20,  2.03it/s, training_loss=0.453]\u001b[A\n",
      "Epoch 1:  26%|███████████████████▎                                                     | 59/223 [00:30<01:20,  2.03it/s, training_loss=0.670]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▋                                                     | 60/223 [00:30<01:20,  2.03it/s, training_loss=0.670]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▋                                                     | 60/223 [00:30<01:20,  2.03it/s, training_loss=0.650]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▉                                                     | 61/223 [00:30<01:20,  2.02it/s, training_loss=0.650]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▉                                                     | 61/223 [00:31<01:20,  2.02it/s, training_loss=0.697]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▎                                                    | 62/223 [00:31<01:18,  2.04it/s, training_loss=0.697]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▎                                                    | 62/223 [00:31<01:18,  2.04it/s, training_loss=0.690]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▌                                                    | 63/223 [00:31<01:18,  2.05it/s, training_loss=0.690]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▌                                                    | 63/223 [00:32<01:18,  2.05it/s, training_loss=0.690]\u001b[A\n",
      "Epoch 1:  29%|████████████████████▉                                                    | 64/223 [00:32<01:17,  2.05it/s, training_loss=0.690]\u001b[A\n",
      "Epoch 1:  29%|████████████████████▉                                                    | 64/223 [00:32<01:17,  2.05it/s, training_loss=0.731]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████▎                                                   | 65/223 [00:32<01:17,  2.03it/s, training_loss=0.731]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████▎                                                   | 65/223 [00:33<01:17,  2.03it/s, training_loss=0.447]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▌                                                   | 66/223 [00:33<01:17,  2.03it/s, training_loss=0.447]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▌                                                   | 66/223 [00:33<01:17,  2.03it/s, training_loss=0.632]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▉                                                   | 67/223 [00:33<01:16,  2.03it/s, training_loss=0.632]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▉                                                   | 67/223 [00:34<01:16,  2.03it/s, training_loss=0.659]\u001b[A\n",
      "Epoch 1:  30%|██████████████████████▎                                                  | 68/223 [00:34<01:16,  2.03it/s, training_loss=0.659]\u001b[A\n",
      "Epoch 1:  30%|██████████████████████▎                                                  | 68/223 [00:34<01:16,  2.03it/s, training_loss=0.695]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▌                                                  | 69/223 [00:34<01:15,  2.03it/s, training_loss=0.695]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▌                                                  | 69/223 [00:35<01:15,  2.03it/s, training_loss=0.559]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▉                                                  | 70/223 [00:35<01:15,  2.02it/s, training_loss=0.559]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▉                                                  | 70/223 [00:35<01:15,  2.02it/s, training_loss=0.523]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▏                                                 | 71/223 [00:35<01:15,  2.02it/s, training_loss=0.523]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▏                                                 | 71/223 [00:36<01:15,  2.02it/s, training_loss=0.653]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▌                                                 | 72/223 [00:36<01:15,  2.01it/s, training_loss=0.653]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▌                                                 | 72/223 [00:36<01:15,  2.01it/s, training_loss=0.618]\u001b[A\n",
      "Epoch 1:  33%|███████████████████████▉                                                 | 73/223 [00:36<01:14,  2.02it/s, training_loss=0.618]\u001b[A\n",
      "Epoch 1:  33%|███████████████████████▉                                                 | 73/223 [00:37<01:14,  2.02it/s, training_loss=0.498]\u001b[A\n",
      "Epoch 1:  33%|████████████████████████▏                                                | 74/223 [00:37<01:13,  2.02it/s, training_loss=0.498]\u001b[A\n",
      "Epoch 1:  33%|████████████████████████▏                                                | 74/223 [00:37<01:13,  2.02it/s, training_loss=0.590]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▌                                                | 75/223 [00:37<01:13,  2.02it/s, training_loss=0.590]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▌                                                | 75/223 [00:38<01:13,  2.02it/s, training_loss=0.630]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▉                                                | 76/223 [00:38<01:13,  2.01it/s, training_loss=0.630]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▉                                                | 76/223 [00:38<01:13,  2.01it/s, training_loss=0.619]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▏                                               | 77/223 [00:38<01:12,  2.02it/s, training_loss=0.619]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▏                                               | 77/223 [00:39<01:12,  2.02it/s, training_loss=0.606]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▌                                               | 78/223 [00:39<01:11,  2.02it/s, training_loss=0.606]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▌                                               | 78/223 [00:39<01:11,  2.02it/s, training_loss=0.551]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▊                                               | 79/223 [00:39<01:10,  2.05it/s, training_loss=0.551]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▊                                               | 79/223 [00:40<01:10,  2.05it/s, training_loss=0.504]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▏                                              | 80/223 [00:40<01:10,  2.04it/s, training_loss=0.504]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▏                                              | 80/223 [00:40<01:10,  2.04it/s, training_loss=0.577]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▌                                              | 81/223 [00:40<01:09,  2.04it/s, training_loss=0.577]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▌                                              | 81/223 [00:41<01:09,  2.04it/s, training_loss=0.697]\u001b[A\n",
      "Epoch 1:  37%|██████████████████████████▊                                              | 82/223 [00:41<01:09,  2.04it/s, training_loss=0.697]\u001b[A\n",
      "Epoch 1:  37%|██████████████████████████▊                                              | 82/223 [00:41<01:09,  2.04it/s, training_loss=0.578]\u001b[A\n",
      "Epoch 1:  37%|███████████████████████████▏                                             | 83/223 [00:41<01:08,  2.04it/s, training_loss=0.578]\u001b[A\n",
      "Epoch 1:  37%|███████████████████████████▏                                             | 83/223 [00:42<01:08,  2.04it/s, training_loss=0.513]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▍                                             | 84/223 [00:42<01:08,  2.04it/s, training_loss=0.513]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▍                                             | 84/223 [00:42<01:08,  2.04it/s, training_loss=0.489]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▊                                             | 85/223 [00:42<01:07,  2.03it/s, training_loss=0.489]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▊                                             | 85/223 [00:43<01:07,  2.03it/s, training_loss=0.538]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▏                                            | 86/223 [00:43<01:07,  2.04it/s, training_loss=0.538]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▏                                            | 86/223 [00:43<01:07,  2.04it/s, training_loss=0.524]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▍                                            | 87/223 [00:43<01:07,  2.03it/s, training_loss=0.524]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▍                                            | 87/223 [00:44<01:07,  2.03it/s, training_loss=0.532]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▊                                            | 88/223 [00:44<01:06,  2.03it/s, training_loss=0.532]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▊                                            | 88/223 [00:44<01:06,  2.03it/s, training_loss=0.677]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▏                                           | 89/223 [00:44<01:05,  2.06it/s, training_loss=0.677]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▏                                           | 89/223 [00:45<01:05,  2.06it/s, training_loss=0.583]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▍                                           | 90/223 [00:45<01:04,  2.06it/s, training_loss=0.583]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▍                                           | 90/223 [00:45<01:04,  2.06it/s, training_loss=0.616]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████████████▊                                           | 91/223 [00:45<01:03,  2.08it/s, training_loss=0.616]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████████████▊                                           | 91/223 [00:46<01:03,  2.08it/s, training_loss=0.753]\u001b[A\n",
      "Epoch 1:  41%|██████████████████████████████                                           | 92/223 [00:46<01:02,  2.08it/s, training_loss=0.753]\u001b[A\n",
      "Epoch 1:  41%|██████████████████████████████                                           | 92/223 [00:46<01:02,  2.08it/s, training_loss=0.676]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▍                                          | 93/223 [00:46<01:02,  2.07it/s, training_loss=0.676]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▍                                          | 93/223 [00:46<01:02,  2.07it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▊                                          | 94/223 [00:46<01:02,  2.06it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▊                                          | 94/223 [00:47<01:02,  2.06it/s, training_loss=0.590]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████                                          | 95/223 [00:47<01:02,  2.05it/s, training_loss=0.590]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████                                          | 95/223 [00:47<01:02,  2.05it/s, training_loss=0.687]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▍                                         | 96/223 [00:47<01:02,  2.05it/s, training_loss=0.687]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▍                                         | 96/223 [00:48<01:02,  2.05it/s, training_loss=0.586]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▊                                         | 97/223 [00:48<01:01,  2.03it/s, training_loss=0.586]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▊                                         | 97/223 [00:48<01:01,  2.03it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████                                         | 98/223 [00:48<01:01,  2.04it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████                                         | 98/223 [00:49<01:01,  2.04it/s, training_loss=0.613]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████▍                                        | 99/223 [00:49<01:00,  2.04it/s, training_loss=0.613]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████▍                                        | 99/223 [00:49<01:00,  2.04it/s, training_loss=0.607]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▎                                       | 100/223 [00:49<01:00,  2.05it/s, training_loss=0.607]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▎                                       | 100/223 [00:50<01:00,  2.05it/s, training_loss=0.544]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▌                                       | 101/223 [00:50<00:59,  2.04it/s, training_loss=0.544]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▌                                       | 101/223 [00:50<00:59,  2.04it/s, training_loss=0.585]\u001b[A\n",
      "Epoch 1:  46%|████████████████████████████████▉                                       | 102/223 [00:50<00:59,  2.04it/s, training_loss=0.585]\u001b[A\n",
      "Epoch 1:  46%|████████████████████████████████▉                                       | 102/223 [00:51<00:59,  2.04it/s, training_loss=0.628]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▎                                      | 103/223 [00:51<00:58,  2.04it/s, training_loss=0.628]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▎                                      | 103/223 [00:51<00:58,  2.04it/s, training_loss=0.593]\u001b[A\n",
      "Epoch 1:  47%|█████████████████████████████████▌                                      | 104/223 [00:51<00:58,  2.04it/s, training_loss=0.593]\u001b[A\n",
      "Epoch 1:  47%|█████████████████████████████████▌                                      | 104/223 [00:52<00:58,  2.04it/s, training_loss=0.623]\u001b[A\n",
      "Epoch 1:  47%|█████████████████████████████████▉                                      | 105/223 [00:52<00:58,  2.03it/s, training_loss=0.623]\u001b[A\n",
      "Epoch 1:  47%|█████████████████████████████████▉                                      | 105/223 [00:52<00:58,  2.03it/s, training_loss=0.597]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▏                                     | 106/223 [00:52<00:57,  2.02it/s, training_loss=0.597]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▏                                     | 106/223 [00:53<00:57,  2.02it/s, training_loss=0.608]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▌                                     | 107/223 [00:53<00:57,  2.03it/s, training_loss=0.608]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▌                                     | 107/223 [00:53<00:57,  2.03it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▊                                     | 108/223 [00:53<00:56,  2.04it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▊                                     | 108/223 [00:54<00:56,  2.04it/s, training_loss=0.524]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▏                                    | 109/223 [00:54<00:55,  2.04it/s, training_loss=0.524]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▏                                    | 109/223 [00:54<00:55,  2.04it/s, training_loss=0.592]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▌                                    | 110/223 [00:54<00:55,  2.03it/s, training_loss=0.592]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▌                                    | 110/223 [00:55<00:55,  2.03it/s, training_loss=0.574]\u001b[A\n",
      "Epoch 1:  50%|███████████████████████████████████▊                                    | 111/223 [00:55<00:55,  2.03it/s, training_loss=0.574]\u001b[A\n",
      "Epoch 1:  50%|███████████████████████████████████▊                                    | 111/223 [00:55<00:55,  2.03it/s, training_loss=0.645]\u001b[A\n",
      "Epoch 1:  50%|████████████████████████████████████▏                                   | 112/223 [00:55<00:54,  2.03it/s, training_loss=0.645]\u001b[A\n",
      "Epoch 1:  50%|████████████████████████████████████▏                                   | 112/223 [00:56<00:54,  2.03it/s, training_loss=0.466]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▍                                   | 113/223 [00:56<00:53,  2.04it/s, training_loss=0.466]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▍                                   | 113/223 [00:56<00:53,  2.04it/s, training_loss=0.492]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▊                                   | 114/223 [00:56<00:53,  2.02it/s, training_loss=0.492]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▊                                   | 114/223 [00:57<00:53,  2.02it/s, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▏                                  | 115/223 [00:57<00:53,  2.02it/s, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▏                                  | 115/223 [00:57<00:53,  2.02it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▍                                  | 116/223 [00:57<00:52,  2.05it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▍                                  | 116/223 [00:58<00:52,  2.05it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▊                                  | 117/223 [00:58<00:51,  2.04it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▊                                  | 117/223 [00:58<00:51,  2.04it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████                                  | 118/223 [00:58<00:51,  2.03it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████                                  | 118/223 [00:59<00:51,  2.03it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▍                                 | 119/223 [00:59<00:50,  2.04it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▍                                 | 119/223 [00:59<00:50,  2.04it/s, training_loss=0.572]\u001b[A\n",
      "Epoch 1:  54%|██████████████████████████████████████▋                                 | 120/223 [00:59<00:50,  2.03it/s, training_loss=0.572]\u001b[A\n",
      "Epoch 1:  54%|██████████████████████████████████████▋                                 | 120/223 [01:00<00:50,  2.03it/s, training_loss=0.560]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████                                 | 121/223 [01:00<00:49,  2.04it/s, training_loss=0.560]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████                                 | 121/223 [01:00<00:49,  2.04it/s, training_loss=0.409]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████████████████▍                                | 122/223 [01:00<00:49,  2.03it/s, training_loss=0.409]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████████████████▍                                | 122/223 [01:01<00:49,  2.03it/s, training_loss=0.587]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████████████████▋                                | 123/223 [01:01<00:49,  2.03it/s, training_loss=0.587]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████████████████▋                                | 123/223 [01:01<00:49,  2.03it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████                                | 124/223 [01:01<00:48,  2.03it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████                                | 124/223 [01:02<00:48,  2.03it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████▎                               | 125/223 [01:02<00:48,  2.04it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████▎                               | 125/223 [01:02<00:48,  2.04it/s, training_loss=0.682]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████████████▋                               | 126/223 [01:02<00:47,  2.03it/s, training_loss=0.682]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████████████▋                               | 126/223 [01:03<00:47,  2.03it/s, training_loss=0.499]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████                               | 127/223 [01:03<00:47,  2.04it/s, training_loss=0.499]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████                               | 127/223 [01:03<00:47,  2.04it/s, training_loss=0.658]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████▎                              | 128/223 [01:03<00:46,  2.05it/s, training_loss=0.658]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████▎                              | 128/223 [01:04<00:46,  2.05it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▋                              | 129/223 [01:04<00:46,  2.02it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▋                              | 129/223 [01:04<00:46,  2.02it/s, training_loss=0.462]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▉                              | 130/223 [01:04<00:45,  2.03it/s, training_loss=0.462]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▉                              | 130/223 [01:05<00:45,  2.03it/s, training_loss=0.652]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▎                             | 131/223 [01:05<00:45,  2.03it/s, training_loss=0.652]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▎                             | 131/223 [01:05<00:45,  2.03it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▌                             | 132/223 [01:05<00:44,  2.05it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▌                             | 132/223 [01:06<00:44,  2.05it/s, training_loss=0.417]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████████████████▉                             | 133/223 [01:06<00:44,  2.05it/s, training_loss=0.417]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████████████████▉                             | 133/223 [01:06<00:44,  2.05it/s, training_loss=0.589]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████████████████▎                            | 134/223 [01:06<00:43,  2.05it/s, training_loss=0.589]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████████████████▎                            | 134/223 [01:07<00:43,  2.05it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▌                            | 135/223 [01:07<00:43,  2.05it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▌                            | 135/223 [01:07<00:43,  2.05it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▉                            | 136/223 [01:07<00:42,  2.05it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▉                            | 136/223 [01:08<00:42,  2.05it/s, training_loss=0.458]\u001b[A\n",
      "Epoch 1:  61%|████████████████████████████████████████████▏                           | 137/223 [01:08<00:42,  2.04it/s, training_loss=0.458]\u001b[A\n",
      "Epoch 1:  61%|████████████████████████████████████████████▏                           | 137/223 [01:08<00:42,  2.04it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▌                           | 138/223 [01:08<00:41,  2.05it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▌                           | 138/223 [01:09<00:41,  2.05it/s, training_loss=0.512]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▉                           | 139/223 [01:09<00:41,  2.03it/s, training_loss=0.512]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▉                           | 139/223 [01:09<00:41,  2.03it/s, training_loss=0.511]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▏                          | 140/223 [01:09<00:40,  2.04it/s, training_loss=0.511]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▏                          | 140/223 [01:10<00:40,  2.04it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▌                          | 141/223 [01:10<00:40,  2.04it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▌                          | 141/223 [01:10<00:40,  2.04it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  64%|█████████████████████████████████████████████▊                          | 142/223 [01:10<00:39,  2.03it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:  64%|█████████████████████████████████████████████▊                          | 142/223 [01:11<00:39,  2.03it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████▏                         | 143/223 [01:11<00:39,  2.04it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████▏                         | 143/223 [01:11<00:39,  2.04it/s, training_loss=0.492]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▍                         | 144/223 [01:11<00:38,  2.05it/s, training_loss=0.492]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▍                         | 144/223 [01:12<00:38,  2.05it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▊                         | 145/223 [01:12<00:38,  2.05it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▊                         | 145/223 [01:12<00:38,  2.05it/s, training_loss=0.688]\u001b[A\n",
      "Epoch 1:  65%|███████████████████████████████████████████████▏                        | 146/223 [01:12<00:37,  2.03it/s, training_loss=0.688]\u001b[A\n",
      "Epoch 1:  65%|███████████████████████████████████████████████▏                        | 146/223 [01:13<00:37,  2.03it/s, training_loss=0.526]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▍                        | 147/223 [01:13<00:37,  2.03it/s, training_loss=0.526]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▍                        | 147/223 [01:13<00:37,  2.03it/s, training_loss=0.590]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▊                        | 148/223 [01:13<00:36,  2.03it/s, training_loss=0.590]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▊                        | 148/223 [01:14<00:36,  2.03it/s, training_loss=0.428]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████                        | 149/223 [01:14<00:36,  2.03it/s, training_loss=0.428]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████                        | 149/223 [01:14<00:36,  2.03it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████▍                       | 150/223 [01:14<00:35,  2.03it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████▍                       | 150/223 [01:14<00:35,  2.03it/s, training_loss=0.503]\u001b[A\n",
      "Epoch 1:  68%|████████████████████████████████████████████████▊                       | 151/223 [01:14<00:35,  2.04it/s, training_loss=0.503]\u001b[A\n",
      "Epoch 1:  68%|████████████████████████████████████████████████▊                       | 151/223 [01:15<00:35,  2.04it/s, training_loss=0.634]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████████████████████                       | 152/223 [01:15<00:34,  2.04it/s, training_loss=0.634]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████████████████████                       | 152/223 [01:15<00:34,  2.04it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▍                      | 153/223 [01:15<00:34,  2.05it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▍                      | 153/223 [01:16<00:34,  2.05it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▋                      | 154/223 [01:16<00:33,  2.05it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▋                      | 154/223 [01:16<00:33,  2.05it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████                      | 155/223 [01:16<00:33,  2.03it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████                      | 155/223 [01:17<00:33,  2.03it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▎                     | 156/223 [01:17<00:32,  2.04it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▎                     | 156/223 [01:17<00:32,  2.04it/s, training_loss=0.464]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▋                     | 157/223 [01:17<00:32,  2.05it/s, training_loss=0.464]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▋                     | 157/223 [01:18<00:32,  2.05it/s, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████                     | 158/223 [01:18<00:32,  2.02it/s, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████                     | 158/223 [01:18<00:32,  2.02it/s, training_loss=0.436]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████▎                    | 159/223 [01:18<00:31,  2.02it/s, training_loss=0.436]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████▎                    | 159/223 [01:19<00:31,  2.02it/s, training_loss=0.561]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▋                    | 160/223 [01:19<00:31,  2.03it/s, training_loss=0.561]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▋                    | 160/223 [01:19<00:31,  2.03it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▉                    | 161/223 [01:19<00:30,  2.02it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▉                    | 161/223 [01:20<00:30,  2.02it/s, training_loss=0.479]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▎                   | 162/223 [01:20<00:29,  2.03it/s, training_loss=0.479]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▎                   | 162/223 [01:20<00:29,  2.03it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▋                   | 163/223 [01:20<00:29,  2.02it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▋                   | 163/223 [01:21<00:29,  2.02it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  74%|████████████████████████████████████████████████████▉                   | 164/223 [01:21<00:28,  2.04it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  74%|████████████████████████████████████████████████████▉                   | 164/223 [01:21<00:28,  2.04it/s, training_loss=0.519]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▎                  | 165/223 [01:21<00:28,  2.05it/s, training_loss=0.519]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▎                  | 165/223 [01:22<00:28,  2.05it/s, training_loss=0.495]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▌                  | 166/223 [01:22<00:28,  2.02it/s, training_loss=0.495]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▌                  | 166/223 [01:22<00:28,  2.02it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████████████████████▉                  | 167/223 [01:22<00:27,  2.02it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  75%|█████████████████████████████████████████████████████▉                  | 167/223 [01:23<00:27,  2.02it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████████████████████▏                 | 168/223 [01:23<00:27,  2.01it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████████████████████▏                 | 168/223 [01:23<00:27,  2.01it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▌                 | 169/223 [01:23<00:26,  2.01it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▌                 | 169/223 [01:24<00:26,  2.01it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▉                 | 170/223 [01:24<00:26,  2.01it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▉                 | 170/223 [01:24<00:26,  2.01it/s, training_loss=0.411]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▏                | 171/223 [01:24<00:25,  2.04it/s, training_loss=0.411]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▏                | 171/223 [01:25<00:25,  2.04it/s, training_loss=0.607]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▌                | 172/223 [01:25<00:25,  2.02it/s, training_loss=0.607]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▌                | 172/223 [01:25<00:25,  2.02it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  78%|███████████████████████████████████████████████████████▊                | 173/223 [01:25<00:24,  2.02it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  78%|███████████████████████████████████████████████████████▊                | 173/223 [01:26<00:24,  2.02it/s, training_loss=0.522]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▏               | 174/223 [01:26<00:24,  2.01it/s, training_loss=0.522]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▏               | 174/223 [01:26<00:24,  2.01it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▌               | 175/223 [01:26<00:23,  2.03it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▌               | 175/223 [01:27<00:23,  2.03it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████████████████████▊               | 176/223 [01:27<00:23,  2.02it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████████████████████▊               | 176/223 [01:27<00:23,  2.02it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████████████████▏              | 177/223 [01:27<00:22,  2.02it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████████████████▏              | 177/223 [01:28<00:22,  2.02it/s, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▍              | 178/223 [01:28<00:22,  2.03it/s, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▍              | 178/223 [01:28<00:22,  2.03it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▊              | 179/223 [01:28<00:21,  2.03it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▊              | 179/223 [01:29<00:21,  2.03it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████              | 180/223 [01:29<00:21,  2.04it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████              | 180/223 [01:29<00:21,  2.04it/s, training_loss=0.463]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████▍             | 181/223 [01:29<00:20,  2.03it/s, training_loss=0.463]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████▍             | 181/223 [01:30<00:20,  2.03it/s, training_loss=0.300]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████████████████████▊             | 182/223 [01:30<00:20,  2.03it/s, training_loss=0.300]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████████████████████▊             | 182/223 [01:30<00:20,  2.03it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████████████████             | 183/223 [01:30<00:19,  2.05it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████████████████             | 183/223 [01:31<00:19,  2.05it/s, training_loss=0.529]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▍            | 184/223 [01:31<00:18,  2.05it/s, training_loss=0.529]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▍            | 184/223 [01:31<00:18,  2.05it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▋            | 185/223 [01:31<00:18,  2.06it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▋            | 185/223 [01:32<00:18,  2.06it/s, training_loss=0.431]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████████████████            | 186/223 [01:32<00:18,  2.05it/s, training_loss=0.431]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████████████████            | 186/223 [01:32<00:18,  2.05it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▍           | 187/223 [01:32<00:17,  2.06it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▍           | 187/223 [01:33<00:17,  2.06it/s, training_loss=0.548]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▋           | 188/223 [01:33<00:17,  2.05it/s, training_loss=0.548]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▋           | 188/223 [01:33<00:17,  2.05it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████           | 189/223 [01:33<00:16,  2.05it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████           | 189/223 [01:34<00:16,  2.05it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████▎          | 190/223 [01:34<00:16,  2.04it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████▎          | 190/223 [01:34<00:16,  2.04it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▋          | 191/223 [01:34<00:15,  2.04it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▋          | 191/223 [01:35<00:15,  2.04it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▉          | 192/223 [01:35<00:15,  2.04it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▉          | 192/223 [01:35<00:15,  2.04it/s, training_loss=0.429]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▎         | 193/223 [01:35<00:14,  2.04it/s, training_loss=0.429]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▎         | 193/223 [01:36<00:14,  2.04it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▋         | 194/223 [01:36<00:14,  2.03it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▋         | 194/223 [01:36<00:14,  2.03it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▉         | 195/223 [01:36<00:13,  2.02it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▉         | 195/223 [01:37<00:13,  2.02it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▎        | 196/223 [01:37<00:13,  2.03it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▎        | 196/223 [01:37<00:13,  2.03it/s, training_loss=0.507]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▌        | 197/223 [01:37<00:12,  2.05it/s, training_loss=0.507]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▌        | 197/223 [01:38<00:12,  2.05it/s, training_loss=0.571]\u001b[A\n",
      "Epoch 1:  89%|███████████████████████████████████████████████████████████████▉        | 198/223 [01:38<00:12,  2.07it/s, training_loss=0.571]\u001b[A\n",
      "Epoch 1:  89%|███████████████████████████████████████████████████████████████▉        | 198/223 [01:38<00:12,  2.07it/s, training_loss=0.554]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████▎       | 199/223 [01:38<00:11,  2.07it/s, training_loss=0.554]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████▎       | 199/223 [01:39<00:11,  2.07it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▌       | 200/223 [01:39<00:11,  2.06it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▌       | 200/223 [01:39<00:11,  2.06it/s, training_loss=0.499]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▉       | 201/223 [01:39<00:10,  2.06it/s, training_loss=0.499]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▉       | 201/223 [01:40<00:10,  2.06it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▏      | 202/223 [01:40<00:10,  2.04it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▏      | 202/223 [01:40<00:10,  2.04it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▌      | 203/223 [01:40<00:09,  2.05it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▌      | 203/223 [01:41<00:09,  2.05it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▊      | 204/223 [01:41<00:09,  2.05it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▊      | 204/223 [01:41<00:09,  2.05it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▏     | 205/223 [01:41<00:08,  2.02it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▏     | 205/223 [01:42<00:08,  2.02it/s, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▌     | 206/223 [01:42<00:08,  2.03it/s, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▌     | 206/223 [01:42<00:08,  2.03it/s, training_loss=0.501]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████████████████████████▊     | 207/223 [01:42<00:07,  2.03it/s, training_loss=0.501]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████████████████████████▊     | 207/223 [01:43<00:07,  2.03it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████████████████████████▏    | 208/223 [01:43<00:07,  2.03it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████████████████████████▏    | 208/223 [01:43<00:07,  2.03it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▍    | 209/223 [01:43<00:06,  2.04it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▍    | 209/223 [01:43<00:06,  2.04it/s, training_loss=0.607]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▊    | 210/223 [01:43<00:06,  2.04it/s, training_loss=0.607]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▊    | 210/223 [01:44<00:06,  2.04it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▏   | 211/223 [01:44<00:05,  2.03it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▏   | 211/223 [01:44<00:05,  2.03it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▍   | 212/223 [01:44<00:05,  2.04it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▍   | 212/223 [01:45<00:05,  2.04it/s, training_loss=0.465]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████████████████████▊   | 213/223 [01:45<00:04,  2.04it/s, training_loss=0.465]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████████████████████▊   | 213/223 [01:45<00:04,  2.04it/s, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████   | 214/223 [01:45<00:04,  2.03it/s, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████   | 214/223 [01:46<00:04,  2.03it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████▍  | 215/223 [01:46<00:04,  2.00it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████▍  | 215/223 [01:46<00:04,  2.00it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  97%|█████████████████████████████████████████████████████████████████████▋  | 216/223 [01:46<00:03,  2.03it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  97%|█████████████████████████████████████████████████████████████████████▋  | 216/223 [01:47<00:03,  2.03it/s, training_loss=0.514]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████████████████████████  | 217/223 [01:47<00:02,  2.02it/s, training_loss=0.514]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████████████████████████  | 217/223 [01:47<00:02,  2.02it/s, training_loss=0.419]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▍ | 218/223 [01:47<00:02,  2.03it/s, training_loss=0.419]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▍ | 218/223 [01:48<00:02,  2.03it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▋ | 219/223 [01:48<00:01,  2.02it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▋ | 219/223 [01:48<00:01,  2.02it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████ | 220/223 [01:48<00:01,  2.04it/s, training_loss=0.270]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████ | 220/223 [01:49<00:01,  2.04it/s, training_loss=0.678]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▎| 221/223 [01:49<00:00,  2.04it/s, training_loss=0.678]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▎| 221/223 [01:49<00:00,  2.04it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████▋| 222/223 [01:49<00:00,  2.03it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████▋| 222/223 [01:50<00:00,  2.03it/s, training_loss=0.030]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 223/223 [01:50<00:00,  2.28it/s, training_loss=0.030]\u001b[A\n",
      "Epoch Progress: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:50<00:00, 110.23s/it]\u001b[A\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " #install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def custom_train_test_split(df, test_size=0.2, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df[['ade', 'soc_code']]\n",
    "    y = df['label']\n",
    "    \n",
    "    # Identify classes and their counts\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    # Identify small classes\n",
    "    small_classes = classes[counts < 5]\n",
    "    \n",
    "    # Initialize lists for train and test sets\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Handle small classes separately\n",
    "    for cls in small_classes:\n",
    "        cls_mask = (y == cls)\n",
    "        cls_X = X[cls_mask]\n",
    "        cls_y = y[cls_mask]\n",
    "        cls_idx = df.index[cls_mask].tolist()\n",
    "        \n",
    "        if len(cls_X) == 1:\n",
    "            # If only one instance, put it in test set\n",
    "            test_indices.append(cls_idx[0])\n",
    "        else:\n",
    "            # Randomly choose one instance for testing\n",
    "            test_idx = np.random.choice(len(cls_X))\n",
    "            test_indices.append(cls_idx[test_idx])\n",
    "            \n",
    "            # Remaining instances go to training\n",
    "            train_indices.extend(np.delete(cls_idx, test_idx))\n",
    "    \n",
    "    # Combine the small class data into test and train sets\n",
    "    test_indices = np.array(test_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "    \n",
    "    X_test = df.loc[test_indices]\n",
    "    y_test = X_test['label']\n",
    "    \n",
    "    X_train = df.loc[train_indices]\n",
    "    y_train = X_train['label']\n",
    "    \n",
    "    # Handle large classes with stratified split\n",
    "    large_class_mask = ~np.isin(y, small_classes)\n",
    "    X_large = X[large_class_mask]\n",
    "    y_large = y[large_class_mask]\n",
    "    \n",
    "    X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "        X_large, y_large, test_size=test_size, random_state=random_state, stratify=y_large\n",
    "    )\n",
    "    \n",
    "    # Combine large class data with the small class data\n",
    "    X_train = pd.concat([X_train, X_train_large], axis=0)\n",
    "    y_train = pd.concat([y_train, y_train_large], axis=0)\n",
    "    \n",
    "    X_test = pd.concat([X_test, X_test_large], axis=0)\n",
    "    y_test = pd.concat([y_test, y_test_large], axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#evaluation\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "\n",
    "    return accuracy_dict, count_dict\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "\n",
    "# Function to calculate precision, recall, and F1 for each label\n",
    "def calculate_metrics(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score per label\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_flat, pred_flat, average=None, labels=np.unique(labels_flat))\n",
    "    \n",
    "    return precision, recall, f1\n",
    "    \n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='combine_all_training_40ep_16bs_5e-5lr_log_fix.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "cadec_csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "# read data from smm4h\n",
    "smm4h_csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n",
    "\n",
    "allSMM4H = [10037175, 10018065,10029205, 10017947, 10028395, 10022891, 10027433, 10040785, 10038738, 10022117, 10015919, 10038604, 10047065, \n",
    "            10021428,10041244, 10007541, 10038359, 10021881, 10013993, 10019805, 10042613, 10029104, 10077536, 10010331, 10014698]\n",
    "\n",
    "label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2,\n",
    "    10017947: 3,\n",
    "    10028395: 4,\n",
    "    10022891: 5,\n",
    "    10027433: 6,\n",
    "    10040785: 7,\n",
    "    10038738: 8,\n",
    "    10022117: 9,\n",
    "    10015919: 10,\n",
    "    10038604: 11,\n",
    "    10047065: 12,\n",
    "    10021428: 13,\n",
    "    10041244: 14,\n",
    "    10007541: 15,\n",
    "    10038359: 16,\n",
    "    10021881: 17,\n",
    "    10013993: 18,\n",
    "    10019805: 19,\n",
    "    10042613: 20,\n",
    "    10029104: 21,\n",
    "    10077536: 22,\n",
    "    10010331: 23,\n",
    "    10014698: 24\n",
    "}\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"ade\", \"soc_code\"]\n",
    "smm4h_all = pd.read_csv(smm4h_csv_url,names=column_names, sep = '\\t', header=None)\n",
    "print(\"smm4h data:\",smm4h_all.shape)\n",
    "\n",
    "smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n",
    "smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n",
    "\n",
    "smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n",
    "\n",
    "print(\"smm4h data after filtering:\",smm4h_all.shape)\n",
    "smm4h_soc_code_counts = smm4h_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "print(\"SOC count in SMM4H: \",smm4h_soc_code_counts)\n",
    "# Filter DataFrame\n",
    "smm4h_filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(allSMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "allinSMM4H = smm4h_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(cadec_csv_url,names=column_names, header=None)\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(\"clean cadec data:\",cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "cadec_soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "print(\"SOC count in CADEC: \",cadec_soc_code_counts)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "cadec_filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(allSMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECallinSMM4H = cadec_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "\n",
    "# For SMM4H data\n",
    "df1 = allinSMM4H.copy()\n",
    "df1.loc[:, 'label'] = df1['soc_code'].map(label_dict)\n",
    "\n",
    "# For CADEC data\n",
    "df2 = CADECallinSMM4H.copy()\n",
    "df2.loc[:, 'label'] = df2['soc_code'].map(label_dict)\n",
    "\n",
    "print(\"SMM4H :\",df1)\n",
    "print(\"CADEC :\",df2)\n",
    "\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 42, 2))\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "learningrate = 5e-5\n",
    "\n",
    "# Placeholder for accuracies\n",
    "all_accuracies = {label: [] for label in range(len(label_dict))}\n",
    "\n",
    "# Initialize dictionaries to hold metrics for each seed\n",
    "# seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': []} for seed_val in seed_values}\n",
    "seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'confusion_matrix': []} for seed_val in seed_values}\n",
    "\n",
    "\n",
    "# Main loop over seed values\n",
    "for seed_val in seed_values:\n",
    "    # Set seeds\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Data preparation\n",
    "     # Perform train-test split on df1\n",
    "    X_train_idx1, X_val_idx1, y_train1, y_val1 = custom_train_test_split(df1, test_size=0.2, random_state=seed_val)\n",
    "\n",
    "    # Perform train-test split on df2\n",
    "    X_train_idx2, X_val_idx2, y_train2, y_val2 = custom_train_test_split(df2, test_size=0.2, random_state=seed_val)\n",
    "\n",
    "    #  set the 'data_type' column for df1 and df2\n",
    "    df1['data_type'] = 'not_set'\n",
    "    df2['data_type'] = 'not_set'\n",
    "\n",
    "    df1.loc[df1.index.isin(X_train_idx1.index), 'data_type'] = 'train'\n",
    "    df1.loc[df1.index.isin(X_val_idx1.index), 'data_type'] = 'val'\n",
    "\n",
    "    df2.loc[df2.index.isin(X_train_idx2.index), 'data_type'] = 'train'\n",
    "    df2.loc[df2.index.isin(X_val_idx2.index), 'data_type'] = 'val'\n",
    "\n",
    "\n",
    "    # If you want to combine df1 and df2 into a single dataframe:\n",
    "    df = pd.concat([df1, df2])\n",
    "    print(\"df: \",df)\n",
    "    logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    print(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'train'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'val'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n",
    "\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict), output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learningrate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    logger.info(f\"Device used: {device}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "\n",
    "        # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n",
    "\n",
    "        logger.info(f'\\nEpoch {epoch}')\n",
    "        loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "        logger.info(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_vals.flatten(), np.argmax(predictions, axis=1).flatten(), average=None, labels=np.unique(true_vals.flatten()))\n",
    "\n",
    "    # Ensure that you use `true_vals` for the true labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1).flatten()\n",
    "    true_labels = true_vals.flatten()\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    seed_metrics[seed_val]['accuracy'] = accuracy\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=np.unique(true_labels))\n",
    "    seed_metrics[seed_val]['confusion_matrix'] = conf_matrix\n",
    "    \n",
    "    for label in label_dict.values():\n",
    "        seed_metrics[seed_val]['precision'].append((label, precision[label]))\n",
    "        seed_metrics[seed_val]['recall'].append((label, recall[label]))\n",
    "        seed_metrics[seed_val]['f1'].append((label, f1[label]))\n",
    "\n",
    "# Write the precision, recall, F1 scores, and seed values to a file\n",
    "with open('combine_all_20times_results_with_seeds.txt', 'w') as f:\n",
    "    f.write('Seed\\tLabel\\tPrecision\\tRecall\\tF1\\tAccuracy\\n')\n",
    "    for seed_val in seed_values:\n",
    "        for label, precision_val in seed_metrics[seed_val]['precision']:\n",
    "            recall_val = next(val for lbl, val in seed_metrics[seed_val]['recall'] if lbl == label)\n",
    "            f1_val = next(val for lbl, val in seed_metrics[seed_val]['f1'] if lbl == label)\n",
    "            accuracy = seed_metrics[seed_val]['accuracy']\n",
    "            f.write(f'{seed_val}\\t{label}\\t{precision_val:.4f}\\t{recall_val:.4f}\\t{f1_val:.4f}\\t{accuracy:.4f}\\n')\n",
    "\n",
    "        # Save the confusion matrix\n",
    "        f.write(f'\\nConfusion Matrix for Seed {seed_val}:\\n')\n",
    "        f.write(np.array2string(seed_metrics[seed_val]['confusion_matrix'], separator=', '))\n",
    "        f.write('\\n')\n",
    "\n",
    "    \n",
    "# Initialize lists to hold precision, recall, and f1 values for each label\n",
    "precision_dict, recall_dict, f1_dict = {}, {}, {}\n",
    "\n",
    "# Collect metrics across seeds\n",
    "for seed in seed_metrics:\n",
    "    for label, value in seed_metrics[seed]['precision']:\n",
    "        precision_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['recall']:\n",
    "        recall_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['f1']:\n",
    "        f1_dict.setdefault(label, []).append(value)\n",
    "\n",
    "# Compute mean and std for precision, recall, and f1\n",
    "labels = sorted(precision_dict.keys())\n",
    "precision_mean = [np.mean(precision_dict[label]) for label in labels]\n",
    "precision_std = [np.std(precision_dict[label]) for label in labels]\n",
    "recall_mean = [np.mean(recall_dict[label]) for label in labels]\n",
    "recall_std = [np.std(recall_dict[label]) for label in labels]\n",
    "f1_mean = [np.mean(f1_dict[label]) for label in labels]\n",
    "f1_std = [np.std(f1_dict[label]) for label in labels]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(labels))  # label indices\n",
    "width = 0.25  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Bar plots with mean values\n",
    "bars_precision = ax.bar(x - width, precision_mean, width, label='Precision', color='b')\n",
    "bars_recall = ax.bar(x, recall_mean, width, label='Recall', color='g')\n",
    "bars_f1 = ax.bar(x + width, f1_mean, width, label='F1 Score', color='r')\n",
    "\n",
    "# # Annotate bars with mean and std values\n",
    "# Annotate bars with mean and std values, with smaller font size\n",
    "# for bars, means, stds in zip([bars_precision, bars_recall, bars_f1],\n",
    "#                              [precision_mean, recall_mean, f1_mean],\n",
    "#                              [precision_std, recall_std, f1_std]):\n",
    "#     for bar, mean, std in zip(bars, means, stds):\n",
    "#         height = bar.get_height()\n",
    "#         ax.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "#                 f'{mean:.2f}\\n±{std:.2f}', ha='center', va='bottom', fontsize=8)  # Smaller font size\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_title('Mean and Standard Deviation of Precision, Recall, and F1 Score by Label')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Set y-axis limit to [0, 1]\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Move legend outside the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the plot to fit the legend\n",
    "plt.savefig('COMBINE_all_20times_results_plot_fix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71935ae8-8a25-4e2f-83d6-0f57e1001a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
