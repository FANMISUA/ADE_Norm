{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9de312-b759-4b54-a1b1-db9b9fe9b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMM4H top 3                             ade  soc_code  label\n",
      "3                            AD  10037175      0\n",
      "4                         focus  10029205      2\n",
      "5                          died  10018065      1\n",
      "8                        dreams  10037175      0\n",
      "10                   withdrawal  10018065      1\n",
      "...                         ...       ...    ...\n",
      "1698     can't go back to sleep  10037175      0\n",
      "1703                 chest hurt  10018065      1\n",
      "1704   got ten minutes of sleep  10037175      0\n",
      "1708  never have another orgasm  10037175      0\n",
      "1710        gain so much weight  10022891      5\n",
      "\n",
      "[909 rows x 3 columns]\n",
      "CADEC top 3                             ade  soc_code  label\n",
      "299      bowel/uterine cramping  10017947      3\n",
      "300            Abdominal cramps  10017947      3\n",
      "301          abdominal cramping  10017947      3\n",
      "302   abdominal cramps and pain  10017947      3\n",
      "303            abdominal cramps  10017947      3\n",
      "...                         ...       ...    ...\n",
      "5326  short term memory lacking  10037175      0\n",
      "5328      couldn't eat or drink  10037175      0\n",
      "5329              Could not eat  10037175      0\n",
      "5331           can't eat normal  10037175      0\n",
      "5332   Disturbed sleep patterns  10037175      0\n",
      "\n",
      "[2685 rows x 3 columns]\n",
      "df:                              ade  soc_code  label data_type\n",
      "3                            AD  10037175      0     train\n",
      "4                         focus  10029205      2     train\n",
      "5                          died  10018065      1     train\n",
      "8                        dreams  10037175      0     train\n",
      "10                   withdrawal  10018065      1     train\n",
      "...                         ...       ...    ...       ...\n",
      "5326  short term memory lacking  10037175      0     train\n",
      "5328      couldn't eat or drink  10037175      0     train\n",
      "5329              Could not eat  10037175      0     train\n",
      "5331           can't eat normal  10037175      0       val\n",
      "5332   Disturbed sleep patterns  10037175      0     train\n",
      "\n",
      "[3594 rows x 4 columns]\n",
      "                          ade\n",
      "soc_code label data_type     \n",
      "10017947 3     train      290\n",
      "               val         73\n",
      "10018065 1     train      711\n",
      "               val        178\n",
      "10022891 5     train      108\n",
      "               val         28\n",
      "10028395 4     train      816\n",
      "               val        204\n",
      "10029205 2     train      399\n",
      "               val         99\n",
      "10037175 0     train      551\n",
      "               val        137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                                       | 0/180 [00:00<?, ?it/s]\u001b[AC:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "Epoch 1:   0%|                                                                                  | 0/180 [00:00<?, ?it/s, training_loss=0.623]\u001b[A\n",
      "Epoch 1:   1%|▍                                                                         | 1/180 [00:00<02:22,  1.25it/s, training_loss=0.623]\u001b[A\n",
      "Epoch 1:   1%|▍                                                                         | 1/180 [00:01<02:22,  1.25it/s, training_loss=0.585]\u001b[A\n",
      "Epoch 1:   1%|▊                                                                         | 2/180 [00:01<01:34,  1.88it/s, training_loss=0.585]\u001b[A\n",
      "Epoch 1:   1%|▊                                                                         | 2/180 [00:01<01:34,  1.88it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                                        | 3/180 [00:01<01:16,  2.31it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                                        | 3/180 [00:01<01:16,  2.31it/s, training_loss=0.554]\u001b[A\n",
      "Epoch 1:   2%|█▋                                                                        | 4/180 [00:01<01:08,  2.59it/s, training_loss=0.554]\u001b[A\n",
      "Epoch 1:   2%|█▋                                                                        | 4/180 [00:02<01:08,  2.59it/s, training_loss=0.604]\u001b[A\n",
      "Epoch 1:   3%|██                                                                        | 5/180 [00:02<01:03,  2.75it/s, training_loss=0.604]\u001b[A\n",
      "Epoch 1:   3%|██                                                                        | 5/180 [00:02<01:03,  2.75it/s, training_loss=0.568]\u001b[A\n",
      "Epoch 1:   3%|██▍                                                                       | 6/180 [00:02<01:00,  2.87it/s, training_loss=0.568]\u001b[A\n",
      "Epoch 1:   3%|██▍                                                                       | 6/180 [00:02<01:00,  2.87it/s, training_loss=0.568]\u001b[A\n",
      "Epoch 1:   4%|██▉                                                                       | 7/180 [00:02<00:58,  2.96it/s, training_loss=0.568]\u001b[A\n",
      "Epoch 1:   4%|██▉                                                                       | 7/180 [00:03<00:58,  2.96it/s, training_loss=0.599]\u001b[A\n",
      "Epoch 1:   4%|███▎                                                                      | 8/180 [00:03<00:56,  3.04it/s, training_loss=0.599]\u001b[A\n",
      "Epoch 1:   4%|███▎                                                                      | 8/180 [00:03<00:56,  3.04it/s, training_loss=0.511]\u001b[A\n",
      "Epoch 1:   5%|███▋                                                                      | 9/180 [00:03<00:56,  3.03it/s, training_loss=0.511]\u001b[A\n",
      "Epoch 1:   5%|███▋                                                                      | 9/180 [00:03<00:56,  3.03it/s, training_loss=0.508]\u001b[A\n",
      "Epoch 1:   6%|████                                                                     | 10/180 [00:03<00:55,  3.07it/s, training_loss=0.508]\u001b[A\n",
      "Epoch 1:   6%|████                                                                     | 10/180 [00:04<00:55,  3.07it/s, training_loss=0.483]\u001b[A\n",
      "Epoch 1:   6%|████▍                                                                    | 11/180 [00:04<00:54,  3.12it/s, training_loss=0.483]\u001b[A\n",
      "Epoch 1:   6%|████▍                                                                    | 11/180 [00:04<00:54,  3.12it/s, training_loss=0.587]\u001b[A\n",
      "Epoch 1:   7%|████▊                                                                    | 12/180 [00:04<00:55,  3.05it/s, training_loss=0.587]\u001b[A\n",
      "Epoch 1:   7%|████▊                                                                    | 12/180 [00:04<00:55,  3.05it/s, training_loss=0.534]\u001b[A\n",
      "Epoch 1:   7%|█████▎                                                                   | 13/180 [00:04<00:54,  3.04it/s, training_loss=0.534]\u001b[A\n",
      "Epoch 1:   7%|█████▎                                                                   | 13/180 [00:05<00:54,  3.04it/s, training_loss=0.597]\u001b[A\n",
      "Epoch 1:   8%|█████▋                                                                   | 14/180 [00:05<00:54,  3.04it/s, training_loss=0.597]\u001b[A\n",
      "Epoch 1:   8%|█████▋                                                                   | 14/180 [00:05<00:54,  3.04it/s, training_loss=0.549]\u001b[A\n",
      "Epoch 1:   8%|██████                                                                   | 15/180 [00:05<00:53,  3.06it/s, training_loss=0.549]\u001b[A\n",
      "Epoch 1:   8%|██████                                                                   | 15/180 [00:05<00:53,  3.06it/s, training_loss=0.534]\u001b[A\n",
      "Epoch 1:   9%|██████▍                                                                  | 16/180 [00:05<00:53,  3.08it/s, training_loss=0.534]\u001b[A\n",
      "Epoch 1:   9%|██████▍                                                                  | 16/180 [00:05<00:53,  3.08it/s, training_loss=0.457]\u001b[A\n",
      "Epoch 1:   9%|██████▉                                                                  | 17/180 [00:05<00:52,  3.09it/s, training_loss=0.457]\u001b[A\n",
      "Epoch 1:   9%|██████▉                                                                  | 17/180 [00:06<00:52,  3.09it/s, training_loss=0.556]\u001b[A\n",
      "Epoch 1:  10%|███████▎                                                                 | 18/180 [00:06<00:52,  3.11it/s, training_loss=0.556]\u001b[A\n",
      "Epoch 1:  10%|███████▎                                                                 | 18/180 [00:06<00:52,  3.11it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  11%|███████▋                                                                 | 19/180 [00:06<00:52,  3.09it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:  11%|███████▋                                                                 | 19/180 [00:06<00:52,  3.09it/s, training_loss=0.520]\u001b[A\n",
      "Epoch 1:  11%|████████                                                                 | 20/180 [00:06<00:51,  3.13it/s, training_loss=0.520]\u001b[A\n",
      "Epoch 1:  11%|████████                                                                 | 20/180 [00:07<00:51,  3.13it/s, training_loss=0.514]\u001b[A\n",
      "Epoch 1:  12%|████████▌                                                                | 21/180 [00:07<00:51,  3.11it/s, training_loss=0.514]\u001b[A\n",
      "Epoch 1:  12%|████████▌                                                                | 21/180 [00:07<00:51,  3.11it/s, training_loss=0.488]\u001b[A\n",
      "Epoch 1:  12%|████████▉                                                                | 22/180 [00:07<00:50,  3.15it/s, training_loss=0.488]\u001b[A\n",
      "Epoch 1:  12%|████████▉                                                                | 22/180 [00:07<00:50,  3.15it/s, training_loss=0.523]\u001b[A\n",
      "Epoch 1:  13%|█████████▎                                                               | 23/180 [00:07<00:49,  3.15it/s, training_loss=0.523]\u001b[A\n",
      "Epoch 1:  13%|█████████▎                                                               | 23/180 [00:08<00:49,  3.15it/s, training_loss=0.420]\u001b[A\n",
      "Epoch 1:  13%|█████████▋                                                               | 24/180 [00:08<00:50,  3.10it/s, training_loss=0.420]\u001b[A\n",
      "Epoch 1:  13%|█████████▋                                                               | 24/180 [00:08<00:50,  3.10it/s, training_loss=0.433]\u001b[A\n",
      "Epoch 1:  14%|██████████▏                                                              | 25/180 [00:08<00:49,  3.13it/s, training_loss=0.433]\u001b[A\n",
      "Epoch 1:  14%|██████████▏                                                              | 25/180 [00:08<00:49,  3.13it/s, training_loss=0.443]\u001b[A\n",
      "Epoch 1:  14%|██████████▌                                                              | 26/180 [00:08<00:49,  3.12it/s, training_loss=0.443]\u001b[A\n",
      "Epoch 1:  14%|██████████▌                                                              | 26/180 [00:09<00:49,  3.12it/s, training_loss=0.522]\u001b[A\n",
      "Epoch 1:  15%|██████████▉                                                              | 27/180 [00:09<00:49,  3.10it/s, training_loss=0.522]\u001b[A\n",
      "Epoch 1:  15%|██████████▉                                                              | 27/180 [00:09<00:49,  3.10it/s, training_loss=0.488]\u001b[A\n",
      "Epoch 1:  16%|███████████▎                                                             | 28/180 [00:09<00:49,  3.10it/s, training_loss=0.488]\u001b[A\n",
      "Epoch 1:  16%|███████████▎                                                             | 28/180 [00:09<00:49,  3.10it/s, training_loss=0.571]\u001b[A\n",
      "Epoch 1:  16%|███████████▊                                                             | 29/180 [00:09<00:48,  3.11it/s, training_loss=0.571]\u001b[A\n",
      "Epoch 1:  16%|███████████▊                                                             | 29/180 [00:10<00:48,  3.11it/s, training_loss=0.439]\u001b[A\n",
      "Epoch 1:  17%|████████████▏                                                            | 30/180 [00:10<00:48,  3.11it/s, training_loss=0.439]\u001b[A\n",
      "Epoch 1:  17%|████████████▏                                                            | 30/180 [00:10<00:48,  3.11it/s, training_loss=0.421]\u001b[A\n",
      "Epoch 1:  17%|████████████▌                                                            | 31/180 [00:10<00:47,  3.14it/s, training_loss=0.421]\u001b[A\n",
      "Epoch 1:  17%|████████████▌                                                            | 31/180 [00:10<00:47,  3.14it/s, training_loss=0.548]\u001b[A\n",
      "Epoch 1:  18%|████████████▉                                                            | 32/180 [00:10<00:47,  3.11it/s, training_loss=0.548]\u001b[A\n",
      "Epoch 1:  18%|████████████▉                                                            | 32/180 [00:11<00:47,  3.11it/s, training_loss=0.457]\u001b[A\n",
      "Epoch 1:  18%|█████████████▍                                                           | 33/180 [00:11<00:47,  3.12it/s, training_loss=0.457]\u001b[A\n",
      "Epoch 1:  18%|█████████████▍                                                           | 33/180 [00:11<00:47,  3.12it/s, training_loss=0.453]\u001b[A\n",
      "Epoch 1:  19%|█████████████▊                                                           | 34/180 [00:11<00:47,  3.08it/s, training_loss=0.453]\u001b[A\n",
      "Epoch 1:  19%|█████████████▊                                                           | 34/180 [00:11<00:47,  3.08it/s, training_loss=0.585]\u001b[A\n",
      "Epoch 1:  19%|██████████████▏                                                          | 35/180 [00:11<00:46,  3.09it/s, training_loss=0.585]\u001b[A\n",
      "Epoch 1:  19%|██████████████▏                                                          | 35/180 [00:12<00:46,  3.09it/s, training_loss=0.511]\u001b[A\n",
      "Epoch 1:  20%|██████████████▌                                                          | 36/180 [00:12<00:46,  3.11it/s, training_loss=0.511]\u001b[A\n",
      "Epoch 1:  20%|██████████████▌                                                          | 36/180 [00:12<00:46,  3.11it/s, training_loss=0.532]\u001b[A\n",
      "Epoch 1:  21%|███████████████                                                          | 37/180 [00:12<00:45,  3.12it/s, training_loss=0.532]\u001b[A\n",
      "Epoch 1:  21%|███████████████                                                          | 37/180 [00:12<00:45,  3.12it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  21%|███████████████▍                                                         | 38/180 [00:12<00:45,  3.14it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  21%|███████████████▍                                                         | 38/180 [00:13<00:45,  3.14it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  22%|███████████████▊                                                         | 39/180 [00:13<00:45,  3.12it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  22%|███████████████▊                                                         | 39/180 [00:13<00:45,  3.12it/s, training_loss=0.460]\u001b[A\n",
      "Epoch 1:  22%|████████████████▏                                                        | 40/180 [00:13<00:44,  3.12it/s, training_loss=0.460]\u001b[A\n",
      "Epoch 1:  22%|████████████████▏                                                        | 40/180 [00:13<00:44,  3.12it/s, training_loss=0.437]\u001b[A\n",
      "Epoch 1:  23%|████████████████▋                                                        | 41/180 [00:13<00:43,  3.17it/s, training_loss=0.437]\u001b[A\n",
      "Epoch 1:  23%|████████████████▋                                                        | 41/180 [00:13<00:43,  3.17it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  23%|█████████████████                                                        | 42/180 [00:13<00:43,  3.16it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  23%|█████████████████                                                        | 42/180 [00:14<00:43,  3.16it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▍                                                       | 43/180 [00:14<00:44,  3.11it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▍                                                       | 43/180 [00:14<00:44,  3.11it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▊                                                       | 44/180 [00:14<00:43,  3.10it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▊                                                       | 44/180 [00:14<00:43,  3.10it/s, training_loss=0.435]\u001b[A\n",
      "Epoch 1:  25%|██████████████████▎                                                      | 45/180 [00:14<00:43,  3.13it/s, training_loss=0.435]\u001b[A\n",
      "Epoch 1:  25%|██████████████████▎                                                      | 45/180 [00:15<00:43,  3.13it/s, training_loss=0.433]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▋                                                      | 46/180 [00:15<00:42,  3.18it/s, training_loss=0.433]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▋                                                      | 46/180 [00:15<00:42,  3.18it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  26%|███████████████████                                                      | 47/180 [00:15<00:42,  3.17it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  26%|███████████████████                                                      | 47/180 [00:15<00:42,  3.17it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▍                                                     | 48/180 [00:15<00:41,  3.16it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▍                                                     | 48/180 [00:16<00:41,  3.16it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▊                                                     | 49/180 [00:16<00:41,  3.15it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▊                                                     | 49/180 [00:16<00:41,  3.15it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▎                                                    | 50/180 [00:16<00:41,  3.15it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▎                                                    | 50/180 [00:16<00:41,  3.15it/s, training_loss=0.443]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▋                                                    | 51/180 [00:16<00:41,  3.13it/s, training_loss=0.443]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▋                                                    | 51/180 [00:17<00:41,  3.13it/s, training_loss=0.474]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████                                                    | 52/180 [00:17<00:40,  3.17it/s, training_loss=0.474]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████                                                    | 52/180 [00:17<00:40,  3.17it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████▍                                                   | 53/180 [00:17<00:39,  3.19it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████▍                                                   | 53/180 [00:17<00:39,  3.19it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▉                                                   | 54/180 [00:17<00:40,  3.15it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▉                                                   | 54/180 [00:18<00:40,  3.15it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▎                                                  | 55/180 [00:18<00:39,  3.14it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▎                                                  | 55/180 [00:18<00:39,  3.14it/s, training_loss=0.495]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▋                                                  | 56/180 [00:18<00:39,  3.13it/s, training_loss=0.495]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▋                                                  | 56/180 [00:18<00:39,  3.13it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████                                                  | 57/180 [00:18<00:39,  3.14it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████                                                  | 57/180 [00:19<00:39,  3.14it/s, training_loss=0.447]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▌                                                 | 58/180 [00:19<00:39,  3.13it/s, training_loss=0.447]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▌                                                 | 58/180 [00:19<00:39,  3.13it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  33%|███████████████████████▉                                                 | 59/180 [00:19<00:38,  3.12it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  33%|███████████████████████▉                                                 | 59/180 [00:19<00:38,  3.12it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  33%|████████████████████████▎                                                | 60/180 [00:19<00:38,  3.12it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  33%|████████████████████████▎                                                | 60/180 [00:20<00:38,  3.12it/s, training_loss=0.444]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▋                                                | 61/180 [00:20<00:38,  3.11it/s, training_loss=0.444]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▋                                                | 61/180 [00:20<00:38,  3.11it/s, training_loss=0.486]\u001b[A\n",
      "Epoch 1:  34%|█████████████████████████▏                                               | 62/180 [00:20<00:37,  3.12it/s, training_loss=0.486]\u001b[A\n",
      "Epoch 1:  34%|█████████████████████████▏                                               | 62/180 [00:20<00:37,  3.12it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▌                                               | 63/180 [00:20<00:37,  3.15it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▌                                               | 63/180 [00:20<00:37,  3.15it/s, training_loss=0.484]\u001b[A\n",
      "Epoch 1:  36%|█████████████████████████▉                                               | 64/180 [00:20<00:36,  3.15it/s, training_loss=0.484]\u001b[A\n",
      "Epoch 1:  36%|█████████████████████████▉                                               | 64/180 [00:21<00:36,  3.15it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▎                                              | 65/180 [00:21<00:36,  3.15it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▎                                              | 65/180 [00:21<00:36,  3.15it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  37%|██████████████████████████▊                                              | 66/180 [00:21<00:36,  3.10it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  37%|██████████████████████████▊                                              | 66/180 [00:21<00:36,  3.10it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  37%|███████████████████████████▏                                             | 67/180 [00:21<00:36,  3.10it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  37%|███████████████████████████▏                                             | 67/180 [00:22<00:36,  3.10it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▌                                             | 68/180 [00:22<00:36,  3.11it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▌                                             | 68/180 [00:22<00:36,  3.11it/s, training_loss=0.279]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▉                                             | 69/180 [00:22<00:35,  3.11it/s, training_loss=0.279]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▉                                             | 69/180 [00:22<00:35,  3.11it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▍                                            | 70/180 [00:22<00:34,  3.16it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▍                                            | 70/180 [00:23<00:34,  3.16it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▊                                            | 71/180 [00:23<00:34,  3.15it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▊                                            | 71/180 [00:23<00:34,  3.15it/s, training_loss=0.436]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▏                                           | 72/180 [00:23<00:34,  3.13it/s, training_loss=0.436]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▏                                           | 72/180 [00:23<00:34,  3.13it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████████████▌                                           | 73/180 [00:23<00:34,  3.12it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████████████▌                                           | 73/180 [00:24<00:34,  3.12it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  41%|██████████████████████████████                                           | 74/180 [00:24<00:34,  3.06it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  41%|██████████████████████████████                                           | 74/180 [00:24<00:34,  3.06it/s, training_loss=0.440]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▍                                          | 75/180 [00:24<00:34,  3.08it/s, training_loss=0.440]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▍                                          | 75/180 [00:24<00:34,  3.08it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▊                                          | 76/180 [00:24<00:33,  3.09it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▊                                          | 76/180 [00:25<00:33,  3.09it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▏                                         | 77/180 [00:25<00:33,  3.08it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▏                                         | 77/180 [00:25<00:33,  3.08it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▋                                         | 78/180 [00:25<00:32,  3.11it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▋                                         | 78/180 [00:25<00:32,  3.11it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████                                         | 79/180 [00:25<00:31,  3.18it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████                                         | 79/180 [00:26<00:31,  3.18it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████▍                                        | 80/180 [00:26<00:31,  3.16it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████▍                                        | 80/180 [00:26<00:31,  3.16it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▊                                        | 81/180 [00:26<00:31,  3.15it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▊                                        | 81/180 [00:26<00:31,  3.15it/s, training_loss=0.482]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▎                                       | 82/180 [00:26<00:31,  3.13it/s, training_loss=0.482]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▎                                       | 82/180 [00:27<00:31,  3.13it/s, training_loss=0.437]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▋                                       | 83/180 [00:27<00:31,  3.12it/s, training_loss=0.437]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▋                                       | 83/180 [00:27<00:31,  3.12it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████                                       | 84/180 [00:27<00:30,  3.13it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████                                       | 84/180 [00:27<00:30,  3.13it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████▍                                      | 85/180 [00:27<00:30,  3.12it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████▍                                      | 85/180 [00:28<00:30,  3.12it/s, training_loss=0.215]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▉                                      | 86/180 [00:28<00:29,  3.14it/s, training_loss=0.215]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▉                                      | 86/180 [00:28<00:29,  3.14it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 1:  48%|███████████████████████████████████▎                                     | 87/180 [00:28<00:29,  3.17it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 1:  48%|███████████████████████████████████▎                                     | 87/180 [00:28<00:29,  3.17it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▋                                     | 88/180 [00:28<00:29,  3.12it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▋                                     | 88/180 [00:28<00:29,  3.12it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████████████████                                     | 89/180 [00:28<00:29,  3.09it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████████████████                                     | 89/180 [00:29<00:29,  3.09it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  50%|████████████████████████████████████▌                                    | 90/180 [00:29<00:29,  3.08it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  50%|████████████████████████████████████▌                                    | 90/180 [00:29<00:29,  3.08it/s, training_loss=0.203]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▉                                    | 91/180 [00:29<00:28,  3.12it/s, training_loss=0.203]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▉                                    | 91/180 [00:29<00:28,  3.12it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████████████████▎                                   | 92/180 [00:29<00:28,  3.08it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████████████████▎                                   | 92/180 [00:30<00:28,  3.08it/s, training_loss=0.279]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▋                                   | 93/180 [00:30<00:28,  3.10it/s, training_loss=0.279]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▋                                   | 93/180 [00:30<00:28,  3.10it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████████████████                                   | 94/180 [00:30<00:26,  3.19it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████████████████                                   | 94/180 [00:30<00:26,  3.19it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▌                                  | 95/180 [00:30<00:27,  3.13it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▌                                  | 95/180 [00:31<00:27,  3.13it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▉                                  | 96/180 [00:31<00:26,  3.16it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▉                                  | 96/180 [00:31<00:26,  3.16it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▎                                 | 97/180 [00:31<00:26,  3.12it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▎                                 | 97/180 [00:31<00:26,  3.12it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▋                                 | 98/180 [00:31<00:26,  3.13it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▋                                 | 98/180 [00:32<00:26,  3.13it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████████████████▏                                | 99/180 [00:32<00:26,  3.09it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████████████████▏                                | 99/180 [00:32<00:26,  3.09it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████                                | 100/180 [00:32<00:26,  3.00it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████                                | 100/180 [00:32<00:26,  3.00it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████▍                               | 101/180 [00:32<00:26,  3.00it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████▍                               | 101/180 [00:33<00:26,  3.00it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████████████▊                               | 102/180 [00:33<00:25,  3.03it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████████████▊                               | 102/180 [00:33<00:25,  3.03it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████▏                              | 103/180 [00:33<00:25,  3.05it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████▏                              | 103/180 [00:33<00:25,  3.05it/s, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▌                              | 104/180 [00:33<00:25,  3.03it/s, training_loss=0.402]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▌                              | 104/180 [00:34<00:25,  3.03it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 1:  58%|██████████████████████████████████████████                              | 105/180 [00:34<00:24,  3.05it/s, training_loss=0.288]\u001b[A\n",
      "Epoch 1:  58%|██████████████████████████████████████████                              | 105/180 [00:34<00:24,  3.05it/s, training_loss=0.224]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▍                             | 106/180 [00:34<00:24,  3.06it/s, training_loss=0.224]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▍                             | 106/180 [00:34<00:24,  3.06it/s, training_loss=0.161]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▊                             | 107/180 [00:34<00:23,  3.14it/s, training_loss=0.161]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▊                             | 107/180 [00:35<00:23,  3.14it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████████████████▏                            | 108/180 [00:35<00:23,  3.05it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████████████████▏                            | 108/180 [00:35<00:23,  3.05it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▌                            | 109/180 [00:35<00:23,  3.02it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▌                            | 109/180 [00:35<00:23,  3.02it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 1:  61%|████████████████████████████████████████████                            | 110/180 [00:35<00:22,  3.05it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 1:  61%|████████████████████████████████████████████                            | 110/180 [00:36<00:22,  3.05it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▍                           | 111/180 [00:36<00:22,  3.03it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▍                           | 111/180 [00:36<00:22,  3.03it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▊                           | 112/180 [00:36<00:22,  3.05it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▊                           | 112/180 [00:36<00:22,  3.05it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▏                          | 113/180 [00:36<00:22,  2.99it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▏                          | 113/180 [00:37<00:22,  2.99it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▌                          | 114/180 [00:37<00:22,  2.99it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▌                          | 114/180 [00:37<00:22,  2.99it/s, training_loss=0.250]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████                          | 115/180 [00:37<00:21,  2.97it/s, training_loss=0.250]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████                          | 115/180 [00:37<00:21,  2.97it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████▍                         | 116/180 [00:37<00:21,  3.03it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████▍                         | 116/180 [00:38<00:21,  3.03it/s, training_loss=0.282]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▊                         | 117/180 [00:38<00:20,  3.02it/s, training_loss=0.282]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▊                         | 117/180 [00:38<00:20,  3.02it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▏                        | 118/180 [00:38<00:20,  3.06it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▏                        | 118/180 [00:38<00:20,  3.06it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▌                        | 119/180 [00:38<00:20,  3.04it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▌                        | 119/180 [00:39<00:20,  3.04it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████                        | 120/180 [00:39<00:19,  3.00it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████                        | 120/180 [00:39<00:19,  3.00it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████▍                       | 121/180 [00:39<00:19,  3.00it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████▍                       | 121/180 [00:39<00:19,  3.00it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 1:  68%|████████████████████████████████████████████████▊                       | 122/180 [00:39<00:19,  3.02it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 1:  68%|████████████████████████████████████████████████▊                       | 122/180 [00:40<00:19,  3.02it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████████████████████▏                      | 123/180 [00:40<00:19,  2.96it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████████████████████▏                      | 123/180 [00:40<00:19,  2.96it/s, training_loss=0.198]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▌                      | 124/180 [00:40<00:18,  3.02it/s, training_loss=0.198]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▌                      | 124/180 [00:40<00:18,  3.02it/s, training_loss=0.143]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████████████████████                      | 125/180 [00:40<00:18,  2.98it/s, training_loss=0.143]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████████████████████                      | 125/180 [00:41<00:18,  2.98it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▍                     | 126/180 [00:41<00:17,  3.06it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▍                     | 126/180 [00:41<00:17,  3.06it/s, training_loss=0.203]\u001b[A\n",
      "Epoch 1:  71%|██████████████████████████████████████████████████▊                     | 127/180 [00:41<00:17,  3.01it/s, training_loss=0.203]\u001b[A\n",
      "Epoch 1:  71%|██████████████████████████████████████████████████▊                     | 127/180 [00:41<00:17,  3.01it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████▏                    | 128/180 [00:41<00:16,  3.07it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████▏                    | 128/180 [00:42<00:16,  3.07it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▌                    | 129/180 [00:42<00:16,  3.07it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▌                    | 129/180 [00:42<00:16,  3.07it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████████████████████                    | 130/180 [00:42<00:16,  3.06it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████████████████████                    | 130/180 [00:42<00:16,  3.06it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▍                   | 131/180 [00:42<00:15,  3.10it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▍                   | 131/180 [00:43<00:15,  3.10it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▊                   | 132/180 [00:43<00:15,  3.07it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▊                   | 132/180 [00:43<00:15,  3.07it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▏                  | 133/180 [00:43<00:15,  3.08it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▏                  | 133/180 [00:43<00:15,  3.08it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▌                  | 134/180 [00:43<00:14,  3.12it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▌                  | 134/180 [00:44<00:14,  3.12it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████████████████████                  | 135/180 [00:44<00:14,  3.08it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████████████████████                  | 135/180 [00:44<00:14,  3.08it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▍                 | 136/180 [00:44<00:14,  3.00it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▍                 | 136/180 [00:44<00:14,  3.00it/s, training_loss=0.174]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▊                 | 137/180 [00:44<00:14,  3.06it/s, training_loss=0.174]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▊                 | 137/180 [00:45<00:14,  3.06it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▏                | 138/180 [00:45<00:13,  3.05it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▏                | 138/180 [00:45<00:13,  3.05it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▌                | 139/180 [00:45<00:13,  3.05it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▌                | 139/180 [00:45<00:13,  3.05it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████                | 140/180 [00:45<00:12,  3.10it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████                | 140/180 [00:46<00:12,  3.10it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▍               | 141/180 [00:46<00:12,  3.04it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▍               | 141/180 [00:46<00:12,  3.04it/s, training_loss=0.203]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████████████████████▊               | 142/180 [00:46<00:12,  3.04it/s, training_loss=0.203]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████████████████████▊               | 142/180 [00:46<00:12,  3.04it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████████████████▏              | 143/180 [00:46<00:12,  3.04it/s, training_loss=0.255]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████████████████▏              | 143/180 [00:47<00:12,  3.04it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▌              | 144/180 [00:47<00:11,  3.03it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▌              | 144/180 [00:47<00:11,  3.03it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████              | 145/180 [00:47<00:11,  3.07it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████              | 145/180 [00:47<00:11,  3.07it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████▍             | 146/180 [00:47<00:11,  3.02it/s, training_loss=0.268]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████▍             | 146/180 [00:47<00:11,  3.02it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████████████████████▊             | 147/180 [00:47<00:10,  3.06it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████████████████████▊             | 147/180 [00:48<00:10,  3.06it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████████████████▏            | 148/180 [00:48<00:10,  3.06it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████████████████▏            | 148/180 [00:48<00:10,  3.06it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▌            | 149/180 [00:48<00:10,  3.05it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▌            | 149/180 [00:48<00:10,  3.05it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████████████████            | 150/180 [00:48<00:09,  3.05it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████████████████            | 150/180 [00:49<00:09,  3.05it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▍           | 151/180 [00:49<00:09,  3.05it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▍           | 151/180 [00:49<00:09,  3.05it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▊           | 152/180 [00:49<00:09,  3.05it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▊           | 152/180 [00:49<00:09,  3.05it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████▏          | 153/180 [00:49<00:08,  3.05it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████▏          | 153/180 [00:50<00:08,  3.05it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▌          | 154/180 [00:50<00:08,  3.09it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▌          | 154/180 [00:50<00:08,  3.09it/s, training_loss=0.221]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████████████████████████          | 155/180 [00:50<00:08,  3.05it/s, training_loss=0.221]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████████████████████████          | 155/180 [00:50<00:08,  3.05it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▍         | 156/180 [00:50<00:07,  3.04it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▍         | 156/180 [00:51<00:07,  3.04it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▊         | 157/180 [00:51<00:07,  3.09it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▊         | 157/180 [00:51<00:07,  3.09it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▏        | 158/180 [00:51<00:07,  3.08it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▏        | 158/180 [00:51<00:07,  3.08it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▌        | 159/180 [00:51<00:06,  3.07it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▌        | 159/180 [00:52<00:06,  3.07it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████        | 160/180 [00:52<00:06,  3.06it/s, training_loss=0.292]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████        | 160/180 [00:52<00:06,  3.06it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████▍       | 161/180 [00:52<00:06,  3.06it/s, training_loss=0.299]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████▍       | 161/180 [00:52<00:06,  3.06it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▊       | 162/180 [00:52<00:05,  3.05it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▊       | 162/180 [00:53<00:05,  3.05it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▏      | 163/180 [00:53<00:05,  3.05it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▏      | 163/180 [00:53<00:05,  3.05it/s, training_loss=0.259]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▌      | 164/180 [00:53<00:05,  3.05it/s, training_loss=0.259]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▌      | 164/180 [00:53<00:05,  3.05it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████      | 165/180 [00:53<00:04,  3.01it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████      | 165/180 [00:54<00:04,  3.01it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▍     | 166/180 [00:54<00:04,  2.98it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▍     | 166/180 [00:54<00:04,  2.98it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████████████████████████▊     | 167/180 [00:54<00:04,  3.04it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████████████████████████▊     | 167/180 [00:54<00:04,  3.04it/s, training_loss=0.300]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████████████████████████▏    | 168/180 [00:54<00:03,  3.04it/s, training_loss=0.300]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████████████████████████▏    | 168/180 [00:55<00:03,  3.04it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▌    | 169/180 [00:55<00:03,  3.04it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▌    | 169/180 [00:55<00:03,  3.04it/s, training_loss=0.172]\u001b[A\n",
      "Epoch 1:  94%|████████████████████████████████████████████████████████████████████    | 170/180 [00:55<00:03,  3.05it/s, training_loss=0.172]\u001b[A\n",
      "Epoch 1:  94%|████████████████████████████████████████████████████████████████████    | 170/180 [00:55<00:03,  3.05it/s, training_loss=0.200]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▍   | 171/180 [00:55<00:02,  3.09it/s, training_loss=0.200]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▍   | 171/180 [00:56<00:02,  3.09it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████████████████████▊   | 172/180 [00:56<00:02,  3.08it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████████████████████▊   | 172/180 [00:56<00:02,  3.08it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████▏  | 173/180 [00:56<00:02,  3.07it/s, training_loss=0.267]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████▏  | 173/180 [00:56<00:02,  3.07it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  97%|█████████████████████████████████████████████████████████████████████▌  | 174/180 [00:56<00:01,  3.06it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  97%|█████████████████████████████████████████████████████████████████████▌  | 174/180 [00:57<00:01,  3.06it/s, training_loss=0.207]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████████████████████████  | 175/180 [00:57<00:01,  3.10it/s, training_loss=0.207]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████████████████████████  | 175/180 [00:57<00:01,  3.10it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▍ | 176/180 [00:57<00:01,  3.09it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▍ | 176/180 [00:57<00:01,  3.09it/s, training_loss=0.205]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▊ | 177/180 [00:57<00:00,  3.03it/s, training_loss=0.205]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▊ | 177/180 [00:58<00:00,  3.03it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▏| 178/180 [00:58<00:00,  3.08it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▏| 178/180 [00:58<00:00,  3.08it/s, training_loss=0.190]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▌| 179/180 [00:58<00:00,  3.07it/s, training_loss=0.190]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▌| 179/180 [00:58<00:00,  3.07it/s, training_loss=0.300]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 180/180 [00:58<00:00,  3.25it/s, training_loss=0.300]\u001b[A\n",
      "Epoch Progress: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:58<00:00, 58.73s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:                              ade  soc_code  label data_type\n",
      "3                            AD  10037175      0     train\n",
      "4                         focus  10029205      2     train\n",
      "5                          died  10018065      1     train\n",
      "8                        dreams  10037175      0     train\n",
      "10                   withdrawal  10018065      1     train\n",
      "...                         ...       ...    ...       ...\n",
      "5326  short term memory lacking  10037175      0     train\n",
      "5328      couldn't eat or drink  10037175      0     train\n",
      "5329              Could not eat  10037175      0     train\n",
      "5331           can't eat normal  10037175      0       val\n",
      "5332   Disturbed sleep patterns  10037175      0     train\n",
      "\n",
      "[3594 rows x 4 columns]\n",
      "                          ade\n",
      "soc_code label data_type     \n",
      "10017947 3     train      290\n",
      "               val         73\n",
      "10018065 1     train      711\n",
      "               val        178\n",
      "10022891 5     train      108\n",
      "               val         28\n",
      "10028395 4     train      816\n",
      "               val        204\n",
      "10029205 2     train      399\n",
      "               val         99\n",
      "10037175 0     train      551\n",
      "               val        137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                                       | 0/180 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                                                  | 0/180 [00:00<?, ?it/s, training_loss=0.549]\u001b[A\n",
      "Epoch 1:   1%|▍                                                                         | 1/180 [00:00<01:01,  2.91it/s, training_loss=0.549]\u001b[A\n",
      "Epoch 1:   1%|▍                                                                         | 1/180 [00:00<01:01,  2.91it/s, training_loss=0.550]\u001b[A\n",
      "Epoch 1:   1%|▊                                                                         | 2/180 [00:00<00:59,  2.99it/s, training_loss=0.550]\u001b[A\n",
      "Epoch 1:   1%|▊                                                                         | 2/180 [00:00<00:59,  2.99it/s, training_loss=0.520]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                                        | 3/180 [00:00<00:58,  3.02it/s, training_loss=0.520]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                                        | 3/180 [00:01<00:58,  3.02it/s, training_loss=0.593]\u001b[A\n",
      "Epoch 1:   2%|█▋                                                                        | 4/180 [00:01<00:58,  3.03it/s, training_loss=0.593]\u001b[A\n",
      "Epoch 1:   2%|█▋                                                                        | 4/180 [00:01<00:58,  3.03it/s, training_loss=0.607]\u001b[A\n",
      "Epoch 1:   3%|██                                                                        | 5/180 [00:01<00:57,  3.06it/s, training_loss=0.607]\u001b[A\n",
      "Epoch 1:   3%|██                                                                        | 5/180 [00:01<00:57,  3.06it/s, training_loss=0.582]\u001b[A\n",
      "Epoch 1:   3%|██▍                                                                       | 6/180 [00:01<00:56,  3.05it/s, training_loss=0.582]\u001b[A\n",
      "Epoch 1:   3%|██▍                                                                       | 6/180 [00:02<00:56,  3.05it/s, training_loss=0.535]\u001b[A\n",
      "Epoch 1:   4%|██▉                                                                       | 7/180 [00:02<00:56,  3.05it/s, training_loss=0.535]\u001b[A\n",
      "Epoch 1:   4%|██▉                                                                       | 7/180 [00:02<00:56,  3.05it/s, training_loss=0.573]\u001b[A\n",
      "Epoch 1:   4%|███▎                                                                      | 8/180 [00:02<00:56,  3.05it/s, training_loss=0.573]\u001b[A\n",
      "Epoch 1:   4%|███▎                                                                      | 8/180 [00:02<00:56,  3.05it/s, training_loss=0.561]\u001b[A\n",
      "Epoch 1:   5%|███▋                                                                      | 9/180 [00:02<00:55,  3.10it/s, training_loss=0.561]\u001b[A\n",
      "Epoch 1:   5%|███▋                                                                      | 9/180 [00:03<00:55,  3.10it/s, training_loss=0.574]\u001b[A\n",
      "Epoch 1:   6%|████                                                                     | 10/180 [00:03<00:55,  3.08it/s, training_loss=0.574]\u001b[A\n",
      "Epoch 1:   6%|████                                                                     | 10/180 [00:03<00:55,  3.08it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:   6%|████▍                                                                    | 11/180 [00:03<00:55,  3.07it/s, training_loss=0.530]\u001b[A\n",
      "Epoch 1:   6%|████▍                                                                    | 11/180 [00:03<00:55,  3.07it/s, training_loss=0.551]\u001b[A\n",
      "Epoch 1:   7%|████▊                                                                    | 12/180 [00:03<00:54,  3.06it/s, training_loss=0.551]\u001b[A\n",
      "Epoch 1:   7%|████▊                                                                    | 12/180 [00:04<00:54,  3.06it/s, training_loss=0.580]\u001b[A\n",
      "Epoch 1:   7%|█████▎                                                                   | 13/180 [00:04<00:53,  3.10it/s, training_loss=0.580]\u001b[A\n",
      "Epoch 1:   7%|█████▎                                                                   | 13/180 [00:04<00:53,  3.10it/s, training_loss=0.568]\u001b[A\n",
      "Epoch 1:   8%|█████▋                                                                   | 14/180 [00:04<00:54,  3.04it/s, training_loss=0.568]\u001b[A\n",
      "Epoch 1:   8%|█████▋                                                                   | 14/180 [00:04<00:54,  3.04it/s, training_loss=0.516]\u001b[A\n",
      "Epoch 1:   8%|██████                                                                   | 15/180 [00:04<00:54,  3.04it/s, training_loss=0.516]\u001b[A\n",
      "Epoch 1:   8%|██████                                                                   | 15/180 [00:05<00:54,  3.04it/s, training_loss=0.519]\u001b[A\n",
      "Epoch 1:   9%|██████▍                                                                  | 16/180 [00:05<00:53,  3.06it/s, training_loss=0.519]\u001b[A\n",
      "Epoch 1:   9%|██████▍                                                                  | 16/180 [00:05<00:53,  3.06it/s, training_loss=0.506]\u001b[A\n",
      "Epoch 1:   9%|██████▉                                                                  | 17/180 [00:05<00:52,  3.10it/s, training_loss=0.506]\u001b[A\n",
      "Epoch 1:   9%|██████▉                                                                  | 17/180 [00:05<00:52,  3.10it/s, training_loss=0.510]\u001b[A\n",
      "Epoch 1:  10%|███████▎                                                                 | 18/180 [00:05<00:52,  3.09it/s, training_loss=0.510]\u001b[A\n",
      "Epoch 1:  10%|███████▎                                                                 | 18/180 [00:06<00:52,  3.09it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  11%|███████▋                                                                 | 19/180 [00:06<00:53,  3.03it/s, training_loss=0.542]\u001b[A\n",
      "Epoch 1:  11%|███████▋                                                                 | 19/180 [00:06<00:53,  3.03it/s, training_loss=0.519]\u001b[A\n",
      "Epoch 1:  11%|████████                                                                 | 20/180 [00:06<00:51,  3.08it/s, training_loss=0.519]\u001b[A\n",
      "Epoch 1:  11%|████████                                                                 | 20/180 [00:06<00:51,  3.08it/s, training_loss=0.414]\u001b[A\n",
      "Epoch 1:  12%|████████▌                                                                | 21/180 [00:06<00:51,  3.07it/s, training_loss=0.414]\u001b[A\n",
      "Epoch 1:  12%|████████▌                                                                | 21/180 [00:07<00:51,  3.07it/s, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  12%|████████▉                                                                | 22/180 [00:07<00:51,  3.06it/s, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  12%|████████▉                                                                | 22/180 [00:07<00:51,  3.06it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  13%|█████████▎                                                               | 23/180 [00:07<00:51,  3.06it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  13%|█████████▎                                                               | 23/180 [00:07<00:51,  3.06it/s, training_loss=0.610]\u001b[A\n",
      "Epoch 1:  13%|█████████▋                                                               | 24/180 [00:07<00:51,  3.06it/s, training_loss=0.610]\u001b[A\n",
      "Epoch 1:  13%|█████████▋                                                               | 24/180 [00:08<00:51,  3.06it/s, training_loss=0.486]\u001b[A\n",
      "Epoch 1:  14%|██████████▏                                                              | 25/180 [00:08<00:50,  3.05it/s, training_loss=0.486]\u001b[A\n",
      "Epoch 1:  14%|██████████▏                                                              | 25/180 [00:08<00:50,  3.05it/s, training_loss=0.446]\u001b[A\n",
      "Epoch 1:  14%|██████████▌                                                              | 26/180 [00:08<00:49,  3.10it/s, training_loss=0.446]\u001b[A\n",
      "Epoch 1:  14%|██████████▌                                                              | 26/180 [00:08<00:49,  3.10it/s, training_loss=0.472]\u001b[A\n",
      "Epoch 1:  15%|██████████▉                                                              | 27/180 [00:08<00:49,  3.09it/s, training_loss=0.472]\u001b[A\n",
      "Epoch 1:  15%|██████████▉                                                              | 27/180 [00:09<00:49,  3.09it/s, training_loss=0.473]\u001b[A\n",
      "Epoch 1:  16%|███████████▎                                                             | 28/180 [00:09<00:50,  3.03it/s, training_loss=0.473]\u001b[A\n",
      "Epoch 1:  16%|███████████▎                                                             | 28/180 [00:09<00:50,  3.03it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  16%|███████████▊                                                             | 29/180 [00:09<00:49,  3.05it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  16%|███████████▊                                                             | 29/180 [00:09<00:49,  3.05it/s, training_loss=0.502]\u001b[A\n",
      "Epoch 1:  17%|████████████▏                                                            | 30/180 [00:09<00:49,  3.04it/s, training_loss=0.502]\u001b[A\n",
      "Epoch 1:  17%|████████████▏                                                            | 30/180 [00:10<00:49,  3.04it/s, training_loss=0.462]\u001b[A\n",
      "Epoch 1:  17%|████████████▌                                                            | 31/180 [00:10<00:48,  3.08it/s, training_loss=0.462]\u001b[A\n",
      "Epoch 1:  17%|████████████▌                                                            | 31/180 [00:10<00:48,  3.08it/s, training_loss=0.474]\u001b[A\n",
      "Epoch 1:  18%|████████████▉                                                            | 32/180 [00:10<00:48,  3.07it/s, training_loss=0.474]\u001b[A\n",
      "Epoch 1:  18%|████████████▉                                                            | 32/180 [00:10<00:48,  3.07it/s, training_loss=0.417]\u001b[A\n",
      "Epoch 1:  18%|█████████████▍                                                           | 33/180 [00:10<00:47,  3.07it/s, training_loss=0.417]\u001b[A\n",
      "Epoch 1:  18%|█████████████▍                                                           | 33/180 [00:11<00:47,  3.07it/s, training_loss=0.489]\u001b[A\n",
      "Epoch 1:  19%|█████████████▊                                                           | 34/180 [00:11<00:47,  3.06it/s, training_loss=0.489]\u001b[A\n",
      "Epoch 1:  19%|█████████████▊                                                           | 34/180 [00:11<00:47,  3.06it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  19%|██████████████▏                                                          | 35/180 [00:11<00:46,  3.10it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  19%|██████████████▏                                                          | 35/180 [00:11<00:46,  3.10it/s, training_loss=0.491]\u001b[A\n",
      "Epoch 1:  20%|██████████████▌                                                          | 36/180 [00:11<00:46,  3.09it/s, training_loss=0.491]\u001b[A\n",
      "Epoch 1:  20%|██████████████▌                                                          | 36/180 [00:12<00:46,  3.09it/s, training_loss=0.490]\u001b[A\n",
      "Epoch 1:  21%|███████████████                                                          | 37/180 [00:12<00:46,  3.07it/s, training_loss=0.490]\u001b[A\n",
      "Epoch 1:  21%|███████████████                                                          | 37/180 [00:12<00:46,  3.07it/s, training_loss=0.492]\u001b[A\n",
      "Epoch 1:  21%|███████████████▍                                                         | 38/180 [00:12<00:46,  3.02it/s, training_loss=0.492]\u001b[A\n",
      "Epoch 1:  21%|███████████████▍                                                         | 38/180 [00:12<00:46,  3.02it/s, training_loss=0.406]\u001b[A\n",
      "Epoch 1:  22%|███████████████▊                                                         | 39/180 [00:12<00:45,  3.07it/s, training_loss=0.406]\u001b[A\n",
      "Epoch 1:  22%|███████████████▊                                                         | 39/180 [00:13<00:45,  3.07it/s, training_loss=0.495]\u001b[A\n",
      "Epoch 1:  22%|████████████████▏                                                        | 40/180 [00:13<00:45,  3.07it/s, training_loss=0.495]\u001b[A\n",
      "Epoch 1:  22%|████████████████▏                                                        | 40/180 [00:13<00:45,  3.07it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 1:  23%|████████████████▋                                                        | 41/180 [00:13<00:46,  3.02it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 1:  23%|████████████████▋                                                        | 41/180 [00:13<00:46,  3.02it/s, training_loss=0.412]\u001b[A\n",
      "Epoch 1:  23%|█████████████████                                                        | 42/180 [00:13<00:45,  3.03it/s, training_loss=0.412]\u001b[A\n",
      "Epoch 1:  23%|█████████████████                                                        | 42/180 [00:14<00:45,  3.03it/s, training_loss=0.460]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▍                                                       | 43/180 [00:14<00:45,  3.03it/s, training_loss=0.460]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▍                                                       | 43/180 [00:14<00:45,  3.03it/s, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▊                                                       | 44/180 [00:14<00:44,  3.04it/s, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▊                                                       | 44/180 [00:14<00:44,  3.04it/s, training_loss=0.442]\u001b[A\n",
      "Epoch 1:  25%|██████████████████▎                                                      | 45/180 [00:14<00:43,  3.08it/s, training_loss=0.442]\u001b[A\n",
      "Epoch 1:  25%|██████████████████▎                                                      | 45/180 [00:15<00:43,  3.08it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▋                                                      | 46/180 [00:15<00:44,  3.03it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▋                                                      | 46/180 [00:15<00:44,  3.03it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  26%|███████████████████                                                      | 47/180 [00:15<00:43,  3.04it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  26%|███████████████████                                                      | 47/180 [00:15<00:43,  3.04it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▍                                                     | 48/180 [00:15<00:42,  3.08it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▍                                                     | 48/180 [00:16<00:42,  3.08it/s, training_loss=0.421]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▊                                                     | 49/180 [00:16<00:42,  3.09it/s, training_loss=0.421]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▊                                                     | 49/180 [00:16<00:42,  3.09it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▎                                                    | 50/180 [00:16<00:42,  3.03it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▎                                                    | 50/180 [00:16<00:42,  3.03it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▋                                                    | 51/180 [00:16<00:42,  3.04it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▋                                                    | 51/180 [00:17<00:42,  3.04it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████                                                    | 52/180 [00:17<00:42,  3.04it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████                                                    | 52/180 [00:17<00:42,  3.04it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████▍                                                   | 53/180 [00:17<00:41,  3.04it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████▍                                                   | 53/180 [00:17<00:41,  3.04it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▉                                                   | 54/180 [00:17<00:40,  3.09it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▉                                                   | 54/180 [00:17<00:40,  3.09it/s, training_loss=0.411]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▎                                                  | 55/180 [00:17<00:41,  3.03it/s, training_loss=0.411]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▎                                                  | 55/180 [00:18<00:41,  3.03it/s, training_loss=0.456]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▋                                                  | 56/180 [00:18<00:40,  3.08it/s, training_loss=0.456]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▋                                                  | 56/180 [00:18<00:40,  3.08it/s, training_loss=0.517]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████                                                  | 57/180 [00:18<00:40,  3.03it/s, training_loss=0.517]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████                                                  | 57/180 [00:18<00:40,  3.03it/s, training_loss=0.422]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▌                                                 | 58/180 [00:18<00:39,  3.08it/s, training_loss=0.422]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▌                                                 | 58/180 [00:19<00:39,  3.08it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  33%|███████████████████████▉                                                 | 59/180 [00:19<00:39,  3.07it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  33%|███████████████████████▉                                                 | 59/180 [00:19<00:39,  3.07it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  33%|████████████████████████▎                                                | 60/180 [00:19<00:39,  3.05it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  33%|████████████████████████▎                                                | 60/180 [00:19<00:39,  3.05it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▋                                                | 61/180 [00:19<00:38,  3.05it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▋                                                | 61/180 [00:20<00:38,  3.05it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 1:  34%|█████████████████████████▏                                               | 62/180 [00:20<00:38,  3.05it/s, training_loss=0.294]\u001b[A\n",
      "Epoch 1:  34%|█████████████████████████▏                                               | 62/180 [00:20<00:38,  3.05it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▌                                               | 63/180 [00:20<00:38,  3.05it/s, training_loss=0.425]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▌                                               | 63/180 [00:20<00:38,  3.05it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  36%|█████████████████████████▉                                               | 64/180 [00:20<00:38,  3.05it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  36%|█████████████████████████▉                                               | 64/180 [00:21<00:38,  3.05it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▎                                              | 65/180 [00:21<00:37,  3.05it/s, training_loss=0.468]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▎                                              | 65/180 [00:21<00:37,  3.05it/s, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  37%|██████████████████████████▊                                              | 66/180 [00:21<00:37,  3.05it/s, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  37%|██████████████████████████▊                                              | 66/180 [00:21<00:37,  3.05it/s, training_loss=0.502]\u001b[A\n",
      "Epoch 1:  37%|███████████████████████████▏                                             | 67/180 [00:21<00:37,  3.05it/s, training_loss=0.502]\u001b[A\n",
      "Epoch 1:  37%|███████████████████████████▏                                             | 67/180 [00:22<00:37,  3.05it/s, training_loss=0.409]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▌                                             | 68/180 [00:22<00:36,  3.09it/s, training_loss=0.409]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▌                                             | 68/180 [00:22<00:36,  3.09it/s, training_loss=0.455]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▉                                             | 69/180 [00:22<00:36,  3.04it/s, training_loss=0.455]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▉                                             | 69/180 [00:22<00:36,  3.04it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▍                                            | 70/180 [00:22<00:36,  3.04it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▍                                            | 70/180 [00:23<00:36,  3.04it/s, training_loss=0.446]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▊                                            | 71/180 [00:23<00:35,  3.04it/s, training_loss=0.446]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▊                                            | 71/180 [00:23<00:35,  3.04it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▏                                           | 72/180 [00:23<00:35,  3.04it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▏                                           | 72/180 [00:23<00:35,  3.04it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████████████▌                                           | 73/180 [00:23<00:35,  3.05it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████████████▌                                           | 73/180 [00:24<00:35,  3.05it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  41%|██████████████████████████████                                           | 74/180 [00:24<00:34,  3.05it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  41%|██████████████████████████████                                           | 74/180 [00:24<00:34,  3.05it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▍                                          | 75/180 [00:24<00:33,  3.09it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▍                                          | 75/180 [00:24<00:33,  3.09it/s, training_loss=0.455]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▊                                          | 76/180 [00:24<00:34,  3.03it/s, training_loss=0.455]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▊                                          | 76/180 [00:25<00:34,  3.03it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▏                                         | 77/180 [00:25<00:33,  3.04it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▏                                         | 77/180 [00:25<00:33,  3.04it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▋                                         | 78/180 [00:25<00:33,  3.04it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▋                                         | 78/180 [00:25<00:33,  3.04it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████                                         | 79/180 [00:25<00:33,  3.04it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████                                         | 79/180 [00:26<00:33,  3.04it/s, training_loss=0.437]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████▍                                        | 80/180 [00:26<00:32,  3.08it/s, training_loss=0.437]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████▍                                        | 80/180 [00:26<00:32,  3.08it/s, training_loss=0.475]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▊                                        | 81/180 [00:26<00:32,  3.07it/s, training_loss=0.475]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▊                                        | 81/180 [00:26<00:32,  3.07it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▎                                       | 82/180 [00:26<00:32,  3.02it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▎                                       | 82/180 [00:27<00:32,  3.02it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▋                                       | 83/180 [00:27<00:31,  3.07it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▋                                       | 83/180 [00:27<00:31,  3.07it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████                                       | 84/180 [00:27<00:30,  3.11it/s, training_loss=0.271]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████                                       | 84/180 [00:27<00:30,  3.11it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████▍                                      | 85/180 [00:27<00:30,  3.09it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████▍                                      | 85/180 [00:28<00:30,  3.09it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▉                                      | 86/180 [00:28<00:30,  3.08it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▉                                      | 86/180 [00:28<00:30,  3.08it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  48%|███████████████████████████████████▎                                     | 87/180 [00:28<00:30,  3.07it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  48%|███████████████████████████████████▎                                     | 87/180 [00:28<00:30,  3.07it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▋                                     | 88/180 [00:28<00:30,  3.06it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▋                                     | 88/180 [00:29<00:30,  3.06it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████████████████                                     | 89/180 [00:29<00:29,  3.06it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████████████████                                     | 89/180 [00:29<00:29,  3.06it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  50%|████████████████████████████████████▌                                    | 90/180 [00:29<00:29,  3.06it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  50%|████████████████████████████████████▌                                    | 90/180 [00:29<00:29,  3.06it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▉                                    | 91/180 [00:29<00:29,  3.05it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▉                                    | 91/180 [00:30<00:29,  3.05it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████████████████▎                                   | 92/180 [00:30<00:28,  3.05it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████████████████▎                                   | 92/180 [00:30<00:28,  3.05it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▋                                   | 93/180 [00:30<00:28,  3.01it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▋                                   | 93/180 [00:30<00:28,  3.01it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████████████████                                   | 94/180 [00:30<00:28,  3.02it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████████████████                                   | 94/180 [00:31<00:28,  3.02it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▌                                  | 95/180 [00:31<00:28,  3.03it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▌                                  | 95/180 [00:31<00:28,  3.03it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▉                                  | 96/180 [00:31<00:27,  3.03it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▉                                  | 96/180 [00:31<00:27,  3.03it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▎                                 | 97/180 [00:31<00:27,  3.04it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▎                                 | 97/180 [00:32<00:27,  3.04it/s, training_loss=0.500]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▋                                 | 98/180 [00:32<00:26,  3.04it/s, training_loss=0.500]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▋                                 | 98/180 [00:32<00:26,  3.04it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████████████████▏                                | 99/180 [00:32<00:26,  3.04it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████████████████▏                                | 99/180 [00:32<00:26,  3.04it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████                                | 100/180 [00:32<00:26,  3.04it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████                                | 100/180 [00:33<00:26,  3.04it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████▍                               | 101/180 [00:33<00:26,  3.00it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████▍                               | 101/180 [00:33<00:26,  3.00it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████████████▊                               | 102/180 [00:33<00:25,  3.06it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████████████▊                               | 102/180 [00:33<00:25,  3.06it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████▏                              | 103/180 [00:33<00:25,  3.06it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████▏                              | 103/180 [00:34<00:25,  3.06it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▌                              | 104/180 [00:34<00:24,  3.05it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▌                              | 104/180 [00:34<00:24,  3.05it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  58%|██████████████████████████████████████████                              | 105/180 [00:34<00:24,  3.01it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  58%|██████████████████████████████████████████                              | 105/180 [00:34<00:24,  3.01it/s, training_loss=0.432]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▍                             | 106/180 [00:34<00:24,  3.02it/s, training_loss=0.432]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▍                             | 106/180 [00:35<00:24,  3.02it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▊                             | 107/180 [00:35<00:24,  3.03it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▊                             | 107/180 [00:35<00:24,  3.03it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████████████████▏                            | 108/180 [00:35<00:24,  2.99it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████████████████▏                            | 108/180 [00:35<00:24,  2.99it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▌                            | 109/180 [00:35<00:23,  3.05it/s, training_loss=0.276]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▌                            | 109/180 [00:36<00:23,  3.05it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 1:  61%|████████████████████████████████████████████                            | 110/180 [00:36<00:23,  3.03it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 1:  61%|████████████████████████████████████████████                            | 110/180 [00:36<00:23,  3.03it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▍                           | 111/180 [00:36<00:22,  3.05it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▍                           | 111/180 [00:36<00:22,  3.05it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▊                           | 112/180 [00:36<00:22,  3.05it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▊                           | 112/180 [00:36<00:22,  3.05it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▏                          | 113/180 [00:37<00:21,  3.05it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▏                          | 113/180 [00:37<00:21,  3.05it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▌                          | 114/180 [00:37<00:21,  3.05it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▌                          | 114/180 [00:37<00:21,  3.05it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████                          | 115/180 [00:37<00:21,  3.05it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████                          | 115/180 [00:37<00:21,  3.05it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████▍                         | 116/180 [00:37<00:20,  3.05it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████▍                         | 116/180 [00:38<00:20,  3.05it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▊                         | 117/180 [00:38<00:20,  3.01it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▊                         | 117/180 [00:38<00:20,  3.01it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▏                        | 118/180 [00:38<00:20,  3.02it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▏                        | 118/180 [00:38<00:20,  3.02it/s, training_loss=0.227]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▌                        | 119/180 [00:38<00:20,  3.03it/s, training_loss=0.227]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▌                        | 119/180 [00:39<00:20,  3.03it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████                        | 120/180 [00:39<00:19,  3.03it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████                        | 120/180 [00:39<00:19,  3.03it/s, training_loss=0.440]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████▍                       | 121/180 [00:39<00:19,  3.04it/s, training_loss=0.440]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████▍                       | 121/180 [00:39<00:19,  3.04it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  68%|████████████████████████████████████████████████▊                       | 122/180 [00:39<00:19,  3.04it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  68%|████████████████████████████████████████████████▊                       | 122/180 [00:40<00:19,  3.04it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████████████████████▏                      | 123/180 [00:40<00:18,  3.04it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████████████████████▏                      | 123/180 [00:40<00:18,  3.04it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▌                      | 124/180 [00:40<00:18,  3.00it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▌                      | 124/180 [00:40<00:18,  3.00it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████████████████████                      | 125/180 [00:40<00:18,  2.97it/s, training_loss=0.226]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████████████████████                      | 125/180 [00:41<00:18,  2.97it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▍                     | 126/180 [00:41<00:18,  3.00it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▍                     | 126/180 [00:41<00:18,  3.00it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  71%|██████████████████████████████████████████████████▊                     | 127/180 [00:41<00:17,  3.01it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  71%|██████████████████████████████████████████████████▊                     | 127/180 [00:41<00:17,  3.01it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████▏                    | 128/180 [00:41<00:17,  3.02it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████▏                    | 128/180 [00:42<00:17,  3.02it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▌                    | 129/180 [00:42<00:16,  3.03it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▌                    | 129/180 [00:42<00:16,  3.03it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████████████████████                    | 130/180 [00:42<00:16,  3.04it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████████████████████                    | 130/180 [00:42<00:16,  3.04it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▍                   | 131/180 [00:42<00:16,  3.04it/s, training_loss=0.262]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▍                   | 131/180 [00:43<00:16,  3.04it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▊                   | 132/180 [00:43<00:15,  3.09it/s, training_loss=0.296]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▊                   | 132/180 [00:43<00:15,  3.09it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▏                  | 133/180 [00:43<00:15,  3.03it/s, training_loss=0.275]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▏                  | 133/180 [00:43<00:15,  3.03it/s, training_loss=0.179]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▌                  | 134/180 [00:43<00:14,  3.08it/s, training_loss=0.179]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▌                  | 134/180 [00:44<00:14,  3.08it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████████████████████                  | 135/180 [00:44<00:14,  3.03it/s, training_loss=0.297]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████████████████████                  | 135/180 [00:44<00:14,  3.03it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▍                 | 136/180 [00:44<00:14,  3.03it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▍                 | 136/180 [00:44<00:14,  3.03it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▊                 | 137/180 [00:44<00:14,  3.04it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▊                 | 137/180 [00:45<00:14,  3.04it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▏                | 138/180 [00:45<00:13,  3.04it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▏                | 138/180 [00:45<00:13,  3.04it/s, training_loss=0.259]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▌                | 139/180 [00:45<00:13,  3.04it/s, training_loss=0.259]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▌                | 139/180 [00:45<00:13,  3.04it/s, training_loss=0.113]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████                | 140/180 [00:45<00:13,  3.04it/s, training_loss=0.113]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████                | 140/180 [00:46<00:13,  3.04it/s, training_loss=0.208]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▍               | 141/180 [00:46<00:12,  3.03it/s, training_loss=0.208]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▍               | 141/180 [00:46<00:12,  3.03it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████████████████████▊               | 142/180 [00:46<00:12,  3.03it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████████████████████▊               | 142/180 [00:46<00:12,  3.03it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████████████████▏              | 143/180 [00:46<00:12,  3.04it/s, training_loss=0.242]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████████████████▏              | 143/180 [00:47<00:12,  3.04it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▌              | 144/180 [00:47<00:11,  3.09it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▌              | 144/180 [00:47<00:11,  3.09it/s, training_loss=0.227]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████              | 145/180 [00:47<00:11,  3.07it/s, training_loss=0.227]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████              | 145/180 [00:47<00:11,  3.07it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████▍             | 146/180 [00:47<00:11,  3.07it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████▍             | 146/180 [00:48<00:11,  3.07it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████████████████████▊             | 147/180 [00:48<00:10,  3.06it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████████████████████▊             | 147/180 [00:48<00:10,  3.06it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████████████████▏            | 148/180 [00:48<00:10,  3.06it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████████████████▏            | 148/180 [00:48<00:10,  3.06it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▌            | 149/180 [00:48<00:10,  3.05it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▌            | 149/180 [00:49<00:10,  3.05it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████████████████            | 150/180 [00:49<00:09,  3.05it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████████████████            | 150/180 [00:49<00:09,  3.05it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▍           | 151/180 [00:49<00:09,  3.05it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▍           | 151/180 [00:49<00:09,  3.05it/s, training_loss=0.206]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▊           | 152/180 [00:49<00:09,  3.05it/s, training_loss=0.206]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▊           | 152/180 [00:50<00:09,  3.05it/s, training_loss=0.214]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████▏          | 153/180 [00:50<00:08,  3.05it/s, training_loss=0.214]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████▏          | 153/180 [00:50<00:08,  3.05it/s, training_loss=0.205]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▌          | 154/180 [00:50<00:08,  3.05it/s, training_loss=0.205]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▌          | 154/180 [00:50<00:08,  3.05it/s, training_loss=0.173]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████████████████████████          | 155/180 [00:50<00:08,  3.05it/s, training_loss=0.173]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████████████████████████          | 155/180 [00:51<00:08,  3.05it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▍         | 156/180 [00:51<00:07,  3.01it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▍         | 156/180 [00:51<00:07,  3.01it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▊         | 157/180 [00:51<00:07,  2.98it/s, training_loss=0.187]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▊         | 157/180 [00:51<00:07,  2.98it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▏        | 158/180 [00:51<00:07,  3.04it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▏        | 158/180 [00:52<00:07,  3.04it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▌        | 159/180 [00:52<00:07,  3.00it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▌        | 159/180 [00:52<00:07,  3.00it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████        | 160/180 [00:52<00:06,  3.01it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████        | 160/180 [00:52<00:06,  3.01it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████▍       | 161/180 [00:52<00:06,  3.02it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████▍       | 161/180 [00:53<00:06,  3.02it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▊       | 162/180 [00:53<00:05,  3.08it/s, training_loss=0.217]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▊       | 162/180 [00:53<00:05,  3.08it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▏      | 163/180 [00:53<00:05,  3.11it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▏      | 163/180 [00:53<00:05,  3.11it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▌      | 164/180 [00:53<00:05,  3.09it/s, training_loss=0.185]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▌      | 164/180 [00:54<00:05,  3.09it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████      | 165/180 [00:54<00:04,  3.08it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████      | 165/180 [00:54<00:04,  3.08it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▍     | 166/180 [00:54<00:04,  3.07it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▍     | 166/180 [00:54<00:04,  3.07it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████████████████████████▊     | 167/180 [00:54<00:04,  3.06it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████████████████████████▊     | 167/180 [00:55<00:04,  3.06it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████████████████████████▏    | 168/180 [00:55<00:03,  3.06it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████████████████████████▏    | 168/180 [00:55<00:03,  3.06it/s, training_loss=0.243]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▌    | 169/180 [00:55<00:03,  3.06it/s, training_loss=0.243]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▌    | 169/180 [00:55<00:03,  3.06it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  94%|████████████████████████████████████████████████████████████████████    | 170/180 [00:55<00:03,  3.05it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  94%|████████████████████████████████████████████████████████████████████    | 170/180 [00:56<00:03,  3.05it/s, training_loss=0.178]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▍   | 171/180 [00:56<00:02,  3.05it/s, training_loss=0.178]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▍   | 171/180 [00:56<00:02,  3.05it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████████████████████▊   | 172/180 [00:56<00:02,  3.05it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████████████████████▊   | 172/180 [00:56<00:02,  3.05it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████▏  | 173/180 [00:56<00:02,  3.01it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████▏  | 173/180 [00:57<00:02,  3.01it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  97%|█████████████████████████████████████████████████████████████████████▌  | 174/180 [00:57<00:01,  3.02it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  97%|█████████████████████████████████████████████████████████████████████▌  | 174/180 [00:57<00:01,  3.02it/s, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████████████████████████  | 175/180 [00:57<00:01,  3.03it/s, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████████████████████████  | 175/180 [00:57<00:01,  3.03it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▍ | 176/180 [00:57<00:01,  3.03it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▍ | 176/180 [00:58<00:01,  3.03it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▊ | 177/180 [00:58<00:00,  3.04it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▊ | 177/180 [00:58<00:00,  3.04it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▏| 178/180 [00:58<00:00,  3.04it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▏| 178/180 [00:58<00:00,  3.04it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▌| 179/180 [00:58<00:00,  3.04it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▌| 179/180 [00:58<00:00,  3.04it/s, training_loss=0.233]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 180/180 [00:58<00:00,  3.28it/s, training_loss=0.233]\u001b[A\n",
      "Epoch Progress: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:58<00:00, 58.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:                              ade  soc_code  label data_type\n",
      "3                            AD  10037175      0     train\n",
      "4                         focus  10029205      2     train\n",
      "5                          died  10018065      1     train\n",
      "8                        dreams  10037175      0     train\n",
      "10                   withdrawal  10018065      1     train\n",
      "...                         ...       ...    ...       ...\n",
      "5326  short term memory lacking  10037175      0     train\n",
      "5328      couldn't eat or drink  10037175      0     train\n",
      "5329              Could not eat  10037175      0     train\n",
      "5331           can't eat normal  10037175      0       val\n",
      "5332   Disturbed sleep patterns  10037175      0     train\n",
      "\n",
      "[3594 rows x 4 columns]\n",
      "                          ade\n",
      "soc_code label data_type     \n",
      "10017947 3     train      290\n",
      "               val         73\n",
      "10018065 1     train      711\n",
      "               val        178\n",
      "10022891 5     train      108\n",
      "               val         28\n",
      "10028395 4     train      816\n",
      "               val        204\n",
      "10029205 2     train      399\n",
      "               val         99\n",
      "10037175 0     train      551\n",
      "               val        137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                                       | 0/180 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                                                  | 0/180 [00:00<?, ?it/s, training_loss=0.569]\u001b[A\n",
      "Epoch 1:   1%|▍                                                                         | 1/180 [00:00<01:02,  2.88it/s, training_loss=0.569]\u001b[A\n",
      "Epoch 1:   1%|▍                                                                         | 1/180 [00:00<01:02,  2.88it/s, training_loss=0.526]\u001b[A\n",
      "Epoch 1:   1%|▊                                                                         | 2/180 [00:00<01:01,  2.90it/s, training_loss=0.526]\u001b[A\n",
      "Epoch 1:   1%|▊                                                                         | 2/180 [00:01<01:01,  2.90it/s, training_loss=0.511]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                                        | 3/180 [00:01<01:00,  2.94it/s, training_loss=0.511]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                                        | 3/180 [00:01<01:00,  2.94it/s, training_loss=0.506]\u001b[A\n",
      "Epoch 1:   2%|█▋                                                                        | 4/180 [00:01<00:58,  3.01it/s, training_loss=0.506]\u001b[A\n",
      "Epoch 1:   2%|█▋                                                                        | 4/180 [00:01<00:58,  3.01it/s, training_loss=0.694]\u001b[A\n",
      "Epoch 1:   3%|██                                                                        | 5/180 [00:01<00:58,  3.00it/s, training_loss=0.694]\u001b[A\n",
      "Epoch 1:   3%|██                                                                        | 5/180 [00:02<00:58,  3.00it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:   3%|██▍                                                                       | 6/180 [00:02<00:58,  3.00it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:   3%|██▍                                                                       | 6/180 [00:02<00:58,  3.00it/s, training_loss=0.579]\u001b[A\n",
      "Epoch 1:   4%|██▉                                                                       | 7/180 [00:02<00:58,  2.98it/s, training_loss=0.579]\u001b[A\n",
      "Epoch 1:   4%|██▉                                                                       | 7/180 [00:02<00:58,  2.98it/s, training_loss=0.541]\u001b[A\n",
      "Epoch 1:   4%|███▎                                                                      | 8/180 [00:02<00:56,  3.03it/s, training_loss=0.541]\u001b[A\n",
      "Epoch 1:   4%|███▎                                                                      | 8/180 [00:03<00:56,  3.03it/s, training_loss=0.571]\u001b[A\n",
      "Epoch 1:   5%|███▋                                                                      | 9/180 [00:03<00:56,  3.00it/s, training_loss=0.571]\u001b[A\n",
      "Epoch 1:   5%|███▋                                                                      | 9/180 [00:03<00:56,  3.00it/s, training_loss=0.582]\u001b[A\n",
      "Epoch 1:   6%|████                                                                     | 10/180 [00:03<00:55,  3.04it/s, training_loss=0.582]\u001b[A\n",
      "Epoch 1:   6%|████                                                                     | 10/180 [00:03<00:55,  3.04it/s, training_loss=0.484]\u001b[A\n",
      "Epoch 1:   6%|████▍                                                                    | 11/180 [00:03<00:56,  3.01it/s, training_loss=0.484]\u001b[A\n",
      "Epoch 1:   6%|████▍                                                                    | 11/180 [00:03<00:56,  3.01it/s, training_loss=0.501]\u001b[A\n",
      "Epoch 1:   7%|████▊                                                                    | 12/180 [00:03<00:55,  3.05it/s, training_loss=0.501]\u001b[A\n",
      "Epoch 1:   7%|████▊                                                                    | 12/180 [00:04<00:55,  3.05it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:   7%|█████▎                                                                   | 13/180 [00:04<00:54,  3.04it/s, training_loss=0.416]\u001b[A\n",
      "Epoch 1:   7%|█████▎                                                                   | 13/180 [00:04<00:54,  3.04it/s, training_loss=0.570]\u001b[A\n",
      "Epoch 1:   8%|█████▋                                                                   | 14/180 [00:04<00:54,  3.04it/s, training_loss=0.570]\u001b[A\n",
      "Epoch 1:   8%|█████▋                                                                   | 14/180 [00:04<00:54,  3.04it/s, training_loss=0.479]\u001b[A\n",
      "Epoch 1:   8%|██████                                                                   | 15/180 [00:04<00:55,  2.99it/s, training_loss=0.479]\u001b[A\n",
      "Epoch 1:   8%|██████                                                                   | 15/180 [00:05<00:55,  2.99it/s, training_loss=0.426]\u001b[A\n",
      "Epoch 1:   9%|██████▍                                                                  | 16/180 [00:05<00:54,  2.99it/s, training_loss=0.426]\u001b[A\n",
      "Epoch 1:   9%|██████▍                                                                  | 16/180 [00:05<00:54,  2.99it/s, training_loss=0.512]\u001b[A\n",
      "Epoch 1:   9%|██████▉                                                                  | 17/180 [00:05<00:54,  3.00it/s, training_loss=0.512]\u001b[A\n",
      "Epoch 1:   9%|██████▉                                                                  | 17/180 [00:05<00:54,  3.00it/s, training_loss=0.553]\u001b[A\n",
      "Epoch 1:  10%|███████▎                                                                 | 18/180 [00:05<00:53,  3.01it/s, training_loss=0.553]\u001b[A\n",
      "Epoch 1:  10%|███████▎                                                                 | 18/180 [00:06<00:53,  3.01it/s, training_loss=0.458]\u001b[A\n",
      "Epoch 1:  11%|███████▋                                                                 | 19/180 [00:06<00:53,  3.00it/s, training_loss=0.458]\u001b[A\n",
      "Epoch 1:  11%|███████▋                                                                 | 19/180 [00:06<00:53,  3.00it/s, training_loss=0.565]\u001b[A\n",
      "Epoch 1:  11%|████████                                                                 | 20/180 [00:06<00:52,  3.03it/s, training_loss=0.565]\u001b[A\n",
      "Epoch 1:  11%|████████                                                                 | 20/180 [00:06<00:52,  3.03it/s, training_loss=0.577]\u001b[A\n",
      "Epoch 1:  12%|████████▌                                                                | 21/180 [00:06<00:52,  3.04it/s, training_loss=0.577]\u001b[A\n",
      "Epoch 1:  12%|████████▌                                                                | 21/180 [00:07<00:52,  3.04it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  12%|████████▉                                                                | 22/180 [00:07<00:52,  3.04it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  12%|████████▉                                                                | 22/180 [00:07<00:52,  3.04it/s, training_loss=0.526]\u001b[A\n",
      "Epoch 1:  13%|█████████▎                                                               | 23/180 [00:07<00:51,  3.06it/s, training_loss=0.526]\u001b[A\n",
      "Epoch 1:  13%|█████████▎                                                               | 23/180 [00:07<00:51,  3.06it/s, training_loss=0.536]\u001b[A\n",
      "Epoch 1:  13%|█████████▋                                                               | 24/180 [00:07<00:51,  3.06it/s, training_loss=0.536]\u001b[A\n",
      "Epoch 1:  13%|█████████▋                                                               | 24/180 [00:08<00:51,  3.06it/s, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  14%|██████████▏                                                              | 25/180 [00:08<00:50,  3.05it/s, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  14%|██████████▏                                                              | 25/180 [00:08<00:50,  3.05it/s, training_loss=0.541]\u001b[A\n",
      "Epoch 1:  14%|██████████▌                                                              | 26/180 [00:08<00:50,  3.05it/s, training_loss=0.541]\u001b[A\n",
      "Epoch 1:  14%|██████████▌                                                              | 26/180 [00:08<00:50,  3.05it/s, training_loss=0.621]\u001b[A\n",
      "Epoch 1:  15%|██████████▉                                                              | 27/180 [00:08<00:50,  3.05it/s, training_loss=0.621]\u001b[A\n",
      "Epoch 1:  15%|██████████▉                                                              | 27/180 [00:09<00:50,  3.05it/s, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  16%|███████████▎                                                             | 28/180 [00:09<00:50,  3.02it/s, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  16%|███████████▎                                                             | 28/180 [00:09<00:50,  3.02it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:  16%|███████████▊                                                             | 29/180 [00:09<00:49,  3.03it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:  16%|███████████▊                                                             | 29/180 [00:09<00:49,  3.03it/s, training_loss=0.456]\u001b[A\n",
      "Epoch 1:  17%|████████████▏                                                            | 30/180 [00:09<00:49,  3.03it/s, training_loss=0.456]\u001b[A\n",
      "Epoch 1:  17%|████████████▏                                                            | 30/180 [00:10<00:49,  3.03it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 1:  17%|████████████▌                                                            | 31/180 [00:10<00:49,  3.03it/s, training_loss=0.410]\u001b[A\n",
      "Epoch 1:  17%|████████████▌                                                            | 31/180 [00:10<00:49,  3.03it/s, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  18%|████████████▉                                                            | 32/180 [00:10<00:49,  3.01it/s, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  18%|████████████▉                                                            | 32/180 [00:10<00:49,  3.01it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  18%|█████████████▍                                                           | 33/180 [00:10<00:48,  3.02it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  18%|█████████████▍                                                           | 33/180 [00:11<00:48,  3.02it/s, training_loss=0.610]\u001b[A\n",
      "Epoch 1:  19%|█████████████▊                                                           | 34/180 [00:11<00:48,  3.02it/s, training_loss=0.610]\u001b[A\n",
      "Epoch 1:  19%|█████████████▊                                                           | 34/180 [00:11<00:48,  3.02it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:  19%|██████████████▏                                                          | 35/180 [00:11<00:48,  3.01it/s, training_loss=0.566]\u001b[A\n",
      "Epoch 1:  19%|██████████████▏                                                          | 35/180 [00:11<00:48,  3.01it/s, training_loss=0.460]\u001b[A\n",
      "Epoch 1:  20%|██████████████▌                                                          | 36/180 [00:11<00:47,  3.01it/s, training_loss=0.460]\u001b[A\n",
      "Epoch 1:  20%|██████████████▌                                                          | 36/180 [00:12<00:47,  3.01it/s, training_loss=0.574]\u001b[A\n",
      "Epoch 1:  21%|███████████████                                                          | 37/180 [00:12<00:47,  3.04it/s, training_loss=0.574]\u001b[A\n",
      "Epoch 1:  21%|███████████████                                                          | 37/180 [00:12<00:47,  3.04it/s, training_loss=0.420]\u001b[A\n",
      "Epoch 1:  21%|███████████████▍                                                         | 38/180 [00:12<00:46,  3.04it/s, training_loss=0.420]\u001b[A\n",
      "Epoch 1:  21%|███████████████▍                                                         | 38/180 [00:12<00:46,  3.04it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  22%|███████████████▊                                                         | 39/180 [00:12<00:46,  3.04it/s, training_loss=0.505]\u001b[A\n",
      "Epoch 1:  22%|███████████████▊                                                         | 39/180 [00:13<00:46,  3.04it/s, training_loss=0.535]\u001b[A\n",
      "Epoch 1:  22%|████████████████▏                                                        | 40/180 [00:13<00:46,  3.04it/s, training_loss=0.535]\u001b[A\n",
      "Epoch 1:  22%|████████████████▏                                                        | 40/180 [00:13<00:46,  3.04it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  23%|████████████████▋                                                        | 41/180 [00:13<00:46,  3.02it/s, training_loss=0.496]\u001b[A\n",
      "Epoch 1:  23%|████████████████▋                                                        | 41/180 [00:13<00:46,  3.02it/s, training_loss=0.544]\u001b[A\n",
      "Epoch 1:  23%|█████████████████                                                        | 42/180 [00:13<00:45,  3.02it/s, training_loss=0.544]\u001b[A\n",
      "Epoch 1:  23%|█████████████████                                                        | 42/180 [00:14<00:45,  3.02it/s, training_loss=0.413]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▍                                                       | 43/180 [00:14<00:45,  3.03it/s, training_loss=0.413]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▍                                                       | 43/180 [00:14<00:45,  3.03it/s, training_loss=0.434]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▊                                                       | 44/180 [00:14<00:45,  3.01it/s, training_loss=0.434]\u001b[A\n",
      "Epoch 1:  24%|█████████████████▊                                                       | 44/180 [00:14<00:45,  3.01it/s, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  25%|██████████████████▎                                                      | 45/180 [00:14<00:45,  2.99it/s, training_loss=0.480]\u001b[A\n",
      "Epoch 1:  25%|██████████████████▎                                                      | 45/180 [00:15<00:45,  2.99it/s, training_loss=0.409]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▋                                                      | 46/180 [00:15<00:44,  2.98it/s, training_loss=0.409]\u001b[A\n",
      "Epoch 1:  26%|██████████████████▋                                                      | 46/180 [00:15<00:44,  2.98it/s, training_loss=0.519]\u001b[A\n",
      "Epoch 1:  26%|███████████████████                                                      | 47/180 [00:15<00:44,  3.00it/s, training_loss=0.519]\u001b[A\n",
      "Epoch 1:  26%|███████████████████                                                      | 47/180 [00:15<00:44,  3.00it/s, training_loss=0.422]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▍                                                     | 48/180 [00:15<00:43,  3.02it/s, training_loss=0.422]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▍                                                     | 48/180 [00:16<00:43,  3.02it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▊                                                     | 49/180 [00:16<00:43,  3.02it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  27%|███████████████████▊                                                     | 49/180 [00:16<00:43,  3.02it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▎                                                    | 50/180 [00:16<00:42,  3.03it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▎                                                    | 50/180 [00:16<00:42,  3.03it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▋                                                    | 51/180 [00:16<00:42,  3.03it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  28%|████████████████████▋                                                    | 51/180 [00:17<00:42,  3.03it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████                                                    | 52/180 [00:17<00:42,  3.03it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████                                                    | 52/180 [00:17<00:42,  3.03it/s, training_loss=0.563]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████▍                                                   | 53/180 [00:17<00:41,  3.06it/s, training_loss=0.563]\u001b[A\n",
      "Epoch 1:  29%|█████████████████████▍                                                   | 53/180 [00:17<00:41,  3.06it/s, training_loss=0.457]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▉                                                   | 54/180 [00:17<00:41,  3.02it/s, training_loss=0.457]\u001b[A\n",
      "Epoch 1:  30%|█████████████████████▉                                                   | 54/180 [00:18<00:41,  3.02it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▎                                                  | 55/180 [00:18<00:41,  3.04it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▎                                                  | 55/180 [00:18<00:41,  3.04it/s, training_loss=0.463]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▋                                                  | 56/180 [00:18<00:40,  3.04it/s, training_loss=0.463]\u001b[A\n",
      "Epoch 1:  31%|██████████████████████▋                                                  | 56/180 [00:18<00:40,  3.04it/s, training_loss=0.482]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████                                                  | 57/180 [00:18<00:40,  3.02it/s, training_loss=0.482]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████                                                  | 57/180 [00:19<00:40,  3.02it/s, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▌                                                 | 58/180 [00:19<00:41,  2.98it/s, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  32%|███████████████████████▌                                                 | 58/180 [00:19<00:41,  2.98it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  33%|███████████████████████▉                                                 | 59/180 [00:19<00:40,  2.99it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  33%|███████████████████████▉                                                 | 59/180 [00:19<00:40,  2.99it/s, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  33%|████████████████████████▎                                                | 60/180 [00:19<00:39,  3.00it/s, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  33%|████████████████████████▎                                                | 60/180 [00:20<00:39,  3.00it/s, training_loss=0.466]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▋                                                | 61/180 [00:20<00:39,  3.01it/s, training_loss=0.466]\u001b[A\n",
      "Epoch 1:  34%|████████████████████████▋                                                | 61/180 [00:20<00:39,  3.01it/s, training_loss=0.529]\u001b[A\n",
      "Epoch 1:  34%|█████████████████████████▏                                               | 62/180 [00:20<00:39,  3.02it/s, training_loss=0.529]\u001b[A\n",
      "Epoch 1:  34%|█████████████████████████▏                                               | 62/180 [00:20<00:39,  3.02it/s, training_loss=0.476]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▌                                               | 63/180 [00:20<00:38,  3.02it/s, training_loss=0.476]\u001b[A\n",
      "Epoch 1:  35%|█████████████████████████▌                                               | 63/180 [00:21<00:38,  3.02it/s, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  36%|█████████████████████████▉                                               | 64/180 [00:21<00:38,  3.02it/s, training_loss=0.485]\u001b[A\n",
      "Epoch 1:  36%|█████████████████████████▉                                               | 64/180 [00:21<00:38,  3.02it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▎                                              | 65/180 [00:21<00:37,  3.03it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  36%|██████████████████████████▎                                              | 65/180 [00:21<00:37,  3.03it/s, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  37%|██████████████████████████▊                                              | 66/180 [00:21<00:37,  3.03it/s, training_loss=0.438]\u001b[A\n",
      "Epoch 1:  37%|██████████████████████████▊                                              | 66/180 [00:22<00:37,  3.03it/s, training_loss=0.431]\u001b[A\n",
      "Epoch 1:  37%|███████████████████████████▏                                             | 67/180 [00:22<00:37,  3.03it/s, training_loss=0.431]\u001b[A\n",
      "Epoch 1:  37%|███████████████████████████▏                                             | 67/180 [00:22<00:37,  3.03it/s, training_loss=0.445]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▌                                             | 68/180 [00:22<00:37,  3.03it/s, training_loss=0.445]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▌                                             | 68/180 [00:22<00:37,  3.03it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▉                                             | 69/180 [00:22<00:36,  3.02it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  38%|███████████████████████████▉                                             | 69/180 [00:23<00:36,  3.02it/s, training_loss=0.561]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▍                                            | 70/180 [00:23<00:36,  3.03it/s, training_loss=0.561]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▍                                            | 70/180 [00:23<00:36,  3.03it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▊                                            | 71/180 [00:23<00:35,  3.03it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  39%|████████████████████████████▊                                            | 71/180 [00:23<00:35,  3.03it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▏                                           | 72/180 [00:23<00:35,  3.03it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  40%|█████████████████████████████▏                                           | 72/180 [00:24<00:35,  3.03it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████████████▌                                           | 73/180 [00:24<00:35,  3.03it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████████████▌                                           | 73/180 [00:24<00:35,  3.03it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 1:  41%|██████████████████████████████                                           | 74/180 [00:24<00:35,  3.03it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 1:  41%|██████████████████████████████                                           | 74/180 [00:24<00:35,  3.03it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▍                                          | 75/180 [00:24<00:34,  3.03it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▍                                          | 75/180 [00:25<00:34,  3.03it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▊                                          | 76/180 [00:25<00:34,  3.03it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  42%|██████████████████████████████▊                                          | 76/180 [00:25<00:34,  3.03it/s, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▏                                         | 77/180 [00:25<00:34,  3.03it/s, training_loss=0.470]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▏                                         | 77/180 [00:25<00:34,  3.03it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▋                                         | 78/180 [00:25<00:33,  3.07it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  43%|███████████████████████████████▋                                         | 78/180 [00:26<00:33,  3.07it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████                                         | 79/180 [00:26<00:32,  3.14it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████                                         | 79/180 [00:26<00:32,  3.14it/s, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████▍                                        | 80/180 [00:26<00:32,  3.10it/s, training_loss=0.400]\u001b[A\n",
      "Epoch 1:  44%|████████████████████████████████▍                                        | 80/180 [00:26<00:32,  3.10it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▊                                        | 81/180 [00:26<00:32,  3.08it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  45%|████████████████████████████████▊                                        | 81/180 [00:27<00:32,  3.08it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▎                                       | 82/180 [00:27<00:31,  3.06it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▎                                       | 82/180 [00:27<00:31,  3.06it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▋                                       | 83/180 [00:27<00:31,  3.08it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  46%|█████████████████████████████████▋                                       | 83/180 [00:27<00:31,  3.08it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████                                       | 84/180 [00:27<00:31,  3.04it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████                                       | 84/180 [00:28<00:31,  3.04it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████▍                                      | 85/180 [00:28<00:31,  3.04it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:  47%|██████████████████████████████████▍                                      | 85/180 [00:28<00:31,  3.04it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▉                                      | 86/180 [00:28<00:31,  3.03it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  48%|██████████████████████████████████▉                                      | 86/180 [00:28<00:31,  3.03it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  48%|███████████████████████████████████▎                                     | 87/180 [00:28<00:30,  3.03it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  48%|███████████████████████████████████▎                                     | 87/180 [00:29<00:30,  3.03it/s, training_loss=0.445]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▋                                     | 88/180 [00:29<00:30,  3.04it/s, training_loss=0.445]\u001b[A\n",
      "Epoch 1:  49%|███████████████████████████████████▋                                     | 88/180 [00:29<00:30,  3.04it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████████████████                                     | 89/180 [00:29<00:29,  3.03it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████████████████                                     | 89/180 [00:29<00:29,  3.03it/s, training_loss=0.481]\u001b[A\n",
      "Epoch 1:  50%|████████████████████████████████████▌                                    | 90/180 [00:29<00:29,  3.03it/s, training_loss=0.481]\u001b[A\n",
      "Epoch 1:  50%|████████████████████████████████████▌                                    | 90/180 [00:30<00:29,  3.03it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▉                                    | 91/180 [00:30<00:29,  3.03it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  51%|████████████████████████████████████▉                                    | 91/180 [00:30<00:29,  3.03it/s, training_loss=0.457]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████████████████▎                                   | 92/180 [00:30<00:29,  3.03it/s, training_loss=0.457]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████████████████▎                                   | 92/180 [00:30<00:29,  3.03it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▋                                   | 93/180 [00:30<00:28,  3.03it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████████████▋                                   | 93/180 [00:31<00:28,  3.03it/s, training_loss=0.475]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████████████████                                   | 94/180 [00:31<00:28,  3.03it/s, training_loss=0.475]\u001b[A\n",
      "Epoch 1:  52%|██████████████████████████████████████                                   | 94/180 [00:31<00:28,  3.03it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▌                                  | 95/180 [00:31<00:28,  3.03it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▌                                  | 95/180 [00:31<00:28,  3.03it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▉                                  | 96/180 [00:31<00:27,  3.03it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████████████▉                                  | 96/180 [00:32<00:27,  3.03it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▎                                 | 97/180 [00:32<00:27,  3.03it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▎                                 | 97/180 [00:32<00:27,  3.03it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▋                                 | 98/180 [00:32<00:27,  3.02it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████████████▋                                 | 98/180 [00:32<00:27,  3.02it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████████████████▏                                | 99/180 [00:32<00:26,  3.03it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████████████████▏                                | 99/180 [00:33<00:26,  3.03it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████                                | 100/180 [00:33<00:26,  3.03it/s, training_loss=0.244]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████                                | 100/180 [00:33<00:26,  3.03it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████▍                               | 101/180 [00:33<00:26,  3.03it/s, training_loss=0.454]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████████████▍                               | 101/180 [00:33<00:26,  3.03it/s, training_loss=0.503]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████████████▊                               | 102/180 [00:33<00:25,  3.03it/s, training_loss=0.503]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████████████▊                               | 102/180 [00:34<00:25,  3.03it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████▏                              | 103/180 [00:34<00:25,  3.03it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████████████████▏                              | 103/180 [00:34<00:25,  3.03it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▌                              | 104/180 [00:34<00:25,  3.03it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████████████▌                              | 104/180 [00:34<00:25,  3.03it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 1:  58%|██████████████████████████████████████████                              | 105/180 [00:34<00:24,  3.03it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 1:  58%|██████████████████████████████████████████                              | 105/180 [00:35<00:24,  3.03it/s, training_loss=0.469]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▍                             | 106/180 [00:35<00:24,  3.03it/s, training_loss=0.469]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▍                             | 106/180 [00:35<00:24,  3.03it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▊                             | 107/180 [00:35<00:24,  3.03it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████████████████▊                             | 107/180 [00:35<00:24,  3.03it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████████████████▏                            | 108/180 [00:35<00:23,  3.03it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████████████████▏                            | 108/180 [00:36<00:23,  3.03it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▌                            | 109/180 [00:36<00:23,  3.03it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████████████▌                            | 109/180 [00:36<00:23,  3.03it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  61%|████████████████████████████████████████████                            | 110/180 [00:36<00:23,  3.03it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  61%|████████████████████████████████████████████                            | 110/180 [00:36<00:23,  3.03it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▍                           | 111/180 [00:36<00:22,  3.03it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▍                           | 111/180 [00:37<00:22,  3.03it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▊                           | 112/180 [00:37<00:22,  3.03it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████████████████▊                           | 112/180 [00:37<00:22,  3.03it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▏                          | 113/180 [00:37<00:22,  3.03it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▏                          | 113/180 [00:37<00:22,  3.03it/s, training_loss=0.427]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▌                          | 114/180 [00:37<00:21,  3.04it/s, training_loss=0.427]\u001b[A\n",
      "Epoch 1:  63%|█████████████████████████████████████████████▌                          | 114/180 [00:37<00:21,  3.04it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████                          | 115/180 [00:37<00:21,  3.03it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████                          | 115/180 [00:38<00:21,  3.03it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████▍                         | 116/180 [00:38<00:21,  3.03it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  64%|██████████████████████████████████████████████▍                         | 116/180 [00:38<00:21,  3.03it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▊                         | 117/180 [00:38<00:20,  3.01it/s, training_loss=0.216]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████████████████▊                         | 117/180 [00:38<00:20,  3.01it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▏                        | 118/180 [00:38<00:20,  3.04it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▏                        | 118/180 [00:39<00:20,  3.04it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▌                        | 119/180 [00:39<00:20,  3.03it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 1:  66%|███████████████████████████████████████████████▌                        | 119/180 [00:39<00:20,  3.03it/s, training_loss=0.209]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████                        | 120/180 [00:39<00:19,  3.04it/s, training_loss=0.209]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████                        | 120/180 [00:39<00:19,  3.04it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████▍                       | 121/180 [00:39<00:19,  3.04it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  67%|████████████████████████████████████████████████▍                       | 121/180 [00:40<00:19,  3.04it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  68%|████████████████████████████████████████████████▊                       | 122/180 [00:40<00:19,  3.03it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  68%|████████████████████████████████████████████████▊                       | 122/180 [00:40<00:19,  3.03it/s, training_loss=0.414]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████████████████████▏                      | 123/180 [00:40<00:18,  3.01it/s, training_loss=0.414]\u001b[A\n",
      "Epoch 1:  68%|█████████████████████████████████████████████████▏                      | 123/180 [00:40<00:18,  3.01it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▌                      | 124/180 [00:40<00:18,  3.04it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  69%|█████████████████████████████████████████████████▌                      | 124/180 [00:41<00:18,  3.04it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████████████████████                      | 125/180 [00:41<00:18,  3.03it/s, training_loss=0.459]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████████████████████████████████                      | 125/180 [00:41<00:18,  3.03it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▍                     | 126/180 [00:41<00:17,  3.03it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  70%|██████████████████████████████████████████████████▍                     | 126/180 [00:41<00:17,  3.03it/s, training_loss=0.481]\u001b[A\n",
      "Epoch 1:  71%|██████████████████████████████████████████████████▊                     | 127/180 [00:41<00:17,  3.03it/s, training_loss=0.481]\u001b[A\n",
      "Epoch 1:  71%|██████████████████████████████████████████████████▊                     | 127/180 [00:42<00:17,  3.03it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████▏                    | 128/180 [00:42<00:17,  3.03it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████████████████████████████████▏                    | 128/180 [00:42<00:17,  3.03it/s, training_loss=0.253]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▌                    | 129/180 [00:42<00:16,  3.03it/s, training_loss=0.253]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████████████████████████████████▌                    | 129/180 [00:42<00:16,  3.03it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████████████████████                    | 130/180 [00:42<00:16,  3.03it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  72%|████████████████████████████████████████████████████                    | 130/180 [00:43<00:16,  3.03it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▍                   | 131/180 [00:43<00:16,  3.03it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▍                   | 131/180 [00:43<00:16,  3.03it/s, training_loss=0.436]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▊                   | 132/180 [00:43<00:15,  3.03it/s, training_loss=0.436]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████████████████████████████████▊                   | 132/180 [00:43<00:15,  3.03it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▏                  | 133/180 [00:43<00:15,  3.03it/s, training_loss=0.269]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▏                  | 133/180 [00:44<00:15,  3.03it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▌                  | 134/180 [00:44<00:15,  3.02it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  74%|█████████████████████████████████████████████████████▌                  | 134/180 [00:44<00:15,  3.02it/s, training_loss=0.434]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████████████████████                  | 135/180 [00:44<00:14,  3.03it/s, training_loss=0.434]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████████████████████                  | 135/180 [00:44<00:14,  3.03it/s, training_loss=0.241]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▍                 | 136/180 [00:44<00:14,  3.03it/s, training_loss=0.241]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▍                 | 136/180 [00:45<00:14,  3.03it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▊                 | 137/180 [00:45<00:14,  3.03it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  76%|██████████████████████████████████████████████████████▊                 | 137/180 [00:45<00:14,  3.03it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▏                | 138/180 [00:45<00:13,  3.03it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▏                | 138/180 [00:45<00:13,  3.03it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▌                | 139/180 [00:45<00:13,  3.03it/s, training_loss=0.286]\u001b[A\n",
      "Epoch 1:  77%|███████████████████████████████████████████████████████▌                | 139/180 [00:46<00:13,  3.03it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████                | 140/180 [00:46<00:13,  3.03it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████                | 140/180 [00:46<00:13,  3.03it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▍               | 141/180 [00:46<00:12,  3.03it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████████████████████▍               | 141/180 [00:46<00:12,  3.03it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████████████████████▊               | 142/180 [00:46<00:12,  3.03it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  79%|████████████████████████████████████████████████████████▊               | 142/180 [00:47<00:12,  3.03it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████████████████▏              | 143/180 [00:47<00:12,  3.03it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████████████████▏              | 143/180 [00:47<00:12,  3.03it/s, training_loss=0.214]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▌              | 144/180 [00:47<00:11,  3.03it/s, training_loss=0.214]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████████████████████▌              | 144/180 [00:47<00:11,  3.03it/s, training_loss=0.252]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████              | 145/180 [00:47<00:11,  3.03it/s, training_loss=0.252]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████              | 145/180 [00:48<00:11,  3.03it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████▍             | 146/180 [00:48<00:11,  3.03it/s, training_loss=0.257]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████████████████▍             | 146/180 [00:48<00:11,  3.03it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████████████████████▊             | 147/180 [00:48<00:10,  3.03it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████████████████████▊             | 147/180 [00:48<00:10,  3.03it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████████████████▏            | 148/180 [00:48<00:10,  3.03it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████████████████▏            | 148/180 [00:49<00:10,  3.03it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▌            | 149/180 [00:49<00:10,  3.02it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████████████████████████████████████▌            | 149/180 [00:49<00:10,  3.02it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████████████████            | 150/180 [00:49<00:09,  3.04it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████████████████            | 150/180 [00:49<00:09,  3.04it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▍           | 151/180 [00:49<00:09,  3.02it/s, training_loss=0.285]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▍           | 151/180 [00:50<00:09,  3.02it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▊           | 152/180 [00:50<00:09,  3.03it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████████████████▊           | 152/180 [00:50<00:09,  3.03it/s, training_loss=0.451]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████▏          | 153/180 [00:50<00:08,  3.03it/s, training_loss=0.451]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████████████████▏          | 153/180 [00:50<00:08,  3.03it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▌          | 154/180 [00:50<00:08,  3.01it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████████████████▌          | 154/180 [00:51<00:08,  3.01it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████████████████████████          | 155/180 [00:51<00:08,  3.06it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  86%|██████████████████████████████████████████████████████████████          | 155/180 [00:51<00:08,  3.06it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▍         | 156/180 [00:51<00:07,  3.06it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▍         | 156/180 [00:51<00:07,  3.06it/s, training_loss=0.254]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▊         | 157/180 [00:51<00:07,  3.07it/s, training_loss=0.254]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████████████████▊         | 157/180 [00:52<00:07,  3.07it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▏        | 158/180 [00:52<00:07,  3.05it/s, training_loss=0.293]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▏        | 158/180 [00:52<00:07,  3.05it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▌        | 159/180 [00:52<00:06,  3.05it/s, training_loss=0.239]\u001b[A\n",
      "Epoch 1:  88%|███████████████████████████████████████████████████████████████▌        | 159/180 [00:52<00:06,  3.05it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████        | 160/180 [00:52<00:06,  3.05it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████        | 160/180 [00:53<00:06,  3.05it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████▍       | 161/180 [00:53<00:06,  3.04it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  89%|████████████████████████████████████████████████████████████████▍       | 161/180 [00:53<00:06,  3.04it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▊       | 162/180 [00:53<00:05,  3.04it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████████████████▊       | 162/180 [00:53<00:05,  3.04it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▏      | 163/180 [00:53<00:05,  3.06it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▏      | 163/180 [00:54<00:05,  3.06it/s, training_loss=0.273]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▌      | 164/180 [00:54<00:05,  3.08it/s, training_loss=0.273]\u001b[A\n",
      "Epoch 1:  91%|█████████████████████████████████████████████████████████████████▌      | 164/180 [00:54<00:05,  3.08it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████      | 165/180 [00:54<00:04,  3.06it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████      | 165/180 [00:54<00:04,  3.06it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▍     | 166/180 [00:54<00:04,  3.05it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████████████████████▍     | 166/180 [00:55<00:04,  3.05it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████████████████████████▊     | 167/180 [00:55<00:04,  3.05it/s, training_loss=0.291]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████████████████████████████████████████▊     | 167/180 [00:55<00:04,  3.05it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████████████████████████▏    | 168/180 [00:55<00:03,  3.04it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 1:  93%|███████████████████████████████████████████████████████████████████▏    | 168/180 [00:55<00:03,  3.04it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▌    | 169/180 [00:55<00:03,  3.04it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████████████████████▌    | 169/180 [00:56<00:03,  3.04it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  94%|████████████████████████████████████████████████████████████████████    | 170/180 [00:56<00:03,  3.04it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  94%|████████████████████████████████████████████████████████████████████    | 170/180 [00:56<00:03,  3.04it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▍   | 171/180 [00:56<00:02,  3.04it/s, training_loss=0.283]\u001b[A\n",
      "Epoch 1:  95%|████████████████████████████████████████████████████████████████████▍   | 171/180 [00:56<00:02,  3.04it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████████████████████▊   | 172/180 [00:56<00:02,  3.01it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████████████████████▊   | 172/180 [00:57<00:02,  3.01it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████▏  | 173/180 [00:57<00:02,  2.98it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  96%|█████████████████████████████████████████████████████████████████████▏  | 173/180 [00:57<00:02,  2.98it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  97%|█████████████████████████████████████████████████████████████████████▌  | 174/180 [00:57<00:02,  2.89it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  97%|█████████████████████████████████████████████████████████████████████▌  | 174/180 [00:57<00:02,  2.89it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████████████████████████  | 175/180 [00:57<00:01,  2.99it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████████████████████████  | 175/180 [00:58<00:01,  2.99it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▍ | 176/180 [00:58<00:01,  3.07it/s, training_loss=0.266]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▍ | 176/180 [00:58<00:01,  3.07it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▊ | 177/180 [00:58<00:00,  3.06it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  98%|██████████████████████████████████████████████████████████████████████▊ | 177/180 [00:58<00:00,  3.06it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▏| 178/180 [00:58<00:00,  3.05it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▏| 178/180 [00:59<00:00,  3.05it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▌| 179/180 [00:59<00:00,  3.07it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████████████████████████████████████████████▌| 179/180 [00:59<00:00,  3.07it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████████████████| 180/180 [00:59<00:00,  3.31it/s, training_loss=0.277]\u001b[A\n",
      "Epoch Progress: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:59<00:00, 59.34s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " #install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def custom_train_test_split(df, test_size=0.2, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df[['ade', 'soc_code']]\n",
    "    y = df['label']\n",
    "    \n",
    "    # Identify classes and their counts\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    # Identify small classes\n",
    "    small_classes = classes[counts < 5]\n",
    "    \n",
    "    # Initialize lists for train and test sets\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Handle small classes separately\n",
    "    for cls in small_classes:\n",
    "        cls_mask = (y == cls)\n",
    "        cls_X = X[cls_mask]\n",
    "        cls_y = y[cls_mask]\n",
    "        cls_idx = df.index[cls_mask].tolist()\n",
    "        \n",
    "        if len(cls_X) == 1:\n",
    "            # If only one instance, put it in test set\n",
    "            test_indices.append(cls_idx[0])\n",
    "        else:\n",
    "            # Randomly choose one instance for testing\n",
    "            test_idx = np.random.choice(len(cls_X))\n",
    "            test_indices.append(cls_idx[test_idx])\n",
    "            \n",
    "            # Remaining instances go to training\n",
    "            train_indices.extend(np.delete(cls_idx, test_idx))\n",
    "    \n",
    "    # Combine the small class data into test and train sets\n",
    "    test_indices = np.array(test_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "    \n",
    "    X_test = df.loc[test_indices]\n",
    "    y_test = X_test['label']\n",
    "    \n",
    "    X_train = df.loc[train_indices]\n",
    "    y_train = X_train['label']\n",
    "    \n",
    "    # Handle large classes with stratified split\n",
    "    large_class_mask = ~np.isin(y, small_classes)\n",
    "    X_large = X[large_class_mask]\n",
    "    y_large = y[large_class_mask]\n",
    "    \n",
    "    X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "        X_large, y_large, test_size=test_size, random_state=random_state, stratify=y_large\n",
    "    )\n",
    "    \n",
    "    # Combine large class data with the small class data\n",
    "    X_train = pd.concat([X_train, X_train_large], axis=0)\n",
    "    y_train = pd.concat([y_train, y_train_large], axis=0)\n",
    "    \n",
    "    X_test = pd.concat([X_test, X_test_large], axis=0)\n",
    "    y_test = pd.concat([y_test, y_test_large], axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#evaluation\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "\n",
    "    return accuracy_dict, count_dict\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "\n",
    "# Function to calculate precision, recall, and F1 for each label\n",
    "def calculate_metrics(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score per label\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_flat, pred_flat, average=None, labels=np.unique(labels_flat))\n",
    "    \n",
    "    return precision, recall, f1\n",
    "    \n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='combine_top6_training_40ep_16bs_5e-5lr_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "cadec_csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "# read data from smm4h\n",
    "smm4h_csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n",
    "\n",
    "top6SMM4H = [10037175, 10018065,10029205, 10017947, 10028395, 10022891]\n",
    "top6label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2,\n",
    "    10017947: 3,\n",
    "    10028395: 4,\n",
    "    10022891: 5\n",
    "}\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"ade\", \"soc_code\"]\n",
    "smm4h_all = pd.read_csv(smm4h_csv_url,names=column_names, sep = '\\t', header=None)\n",
    "\n",
    "smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n",
    "smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n",
    "smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# print(\"smm4h data:\",smm4h_all.shape)\n",
    "smm4h_soc_code_counts = smm4h_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",smm4h_soc_code_counts)\n",
    "# Filter DataFrame\n",
    "smm4h_filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(top6SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "top3inSMM4H = smm4h_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "top3inSMM4H.loc[:, 'label'] = top3inSMM4H['soc_code'].map(top6label_dict)\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(cadec_csv_url,names=column_names, header=None)\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(\"clean cadec data:\",cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "cadec_soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",cadec_soc_code_counts)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "cadec_filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECtop3inSMM4H = cadec_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "\n",
    "# print(\"CADEC top3 in SMM4H:\",CADECtop3inSMM4H)\n",
    "\n",
    "# For SMM4H data\n",
    "df1 = top3inSMM4H.copy()\n",
    "df1.loc[:, 'label'] = df1['soc_code'].map(top6label_dict)\n",
    "\n",
    "# For CADEC data\n",
    "df2 = CADECtop3inSMM4H.copy()\n",
    "df2.loc[:, 'label'] = df2['soc_code'].map(top6label_dict)\n",
    "\n",
    "print(\"SMM4H top 3\",df1)\n",
    "print(\"CADEC top 3\",df2)\n",
    "\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 42, 2))\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "learningrate = 5e-5\n",
    "\n",
    "# Placeholder for accuracies\n",
    "all_accuracies = {label: [] for label in range(len(top6label_dict))}\n",
    "\n",
    "# Initialize dictionaries to hold metrics for each seed\n",
    "# seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': []} for seed_val in seed_values}\n",
    "seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'confusion_matrix': []} for seed_val in seed_values}\n",
    "\n",
    "\n",
    "# Main loop over seed values\n",
    "for seed_val in seed_values:\n",
    "    # Set seeds\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Data preparation\n",
    "    # Apply the custom train-test split\n",
    "    # X_train, X_val, y_train, y_val = custom_train_test_split(df, test_size=0.2, random_state=seed_val)\n",
    "    \n",
    "    # # Add data_type column\n",
    "    # df['data_type'] = 'not_set'\n",
    "    # df.loc[X_train.index, 'data_type'] = 'train'\n",
    "    # df.loc[X_val.index, 'data_type'] = 'val'\n",
    "\n",
    "    # logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    # print(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    # Perform train-test split on df1\n",
    "    X_train_idx1, X_val_idx1, y_train1, y_val1 = custom_train_test_split(df1, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Perform train-test split on df2\n",
    "    X_train_idx2, X_val_idx2, y_train2, y_val2 = custom_train_test_split(df2, test_size=0.2, random_state=42)\n",
    "\n",
    "    #  set the 'data_type' column for df1 and df2\n",
    "    df1['data_type'] = 'not_set'\n",
    "    df2['data_type'] = 'not_set'\n",
    "\n",
    "    df1.loc[df1.index.isin(X_train_idx1.index), 'data_type'] = 'train'\n",
    "    df1.loc[df1.index.isin(X_val_idx1.index), 'data_type'] = 'val'\n",
    "\n",
    "    df2.loc[df2.index.isin(X_train_idx2.index), 'data_type'] = 'train'\n",
    "    df2.loc[df2.index.isin(X_val_idx2.index), 'data_type'] = 'val'\n",
    "\n",
    "\n",
    "    # If you want to combine df1 and df2 into a single dataframe:\n",
    "    df = pd.concat([df1, df2])\n",
    "    print(\"df: \",df)\n",
    "    logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    print(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "\n",
    "    \n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'train'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'val'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n",
    "\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top6label_dict), output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learningrate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    logger.info(f\"Device used: {device}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "\n",
    "        # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n",
    "\n",
    "        logger.info(f'\\nEpoch {epoch}')\n",
    "        loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "        logger.info(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_vals.flatten(), np.argmax(predictions, axis=1).flatten(), average=None, labels=np.unique(true_vals.flatten()))\n",
    "\n",
    "     # Ensure that you use `true_vals` for the true labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1).flatten()\n",
    "    true_labels = true_vals.flatten()\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    seed_metrics[seed_val]['accuracy'] = accuracy\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=np.unique(true_labels))\n",
    "    seed_metrics[seed_val]['confusion_matrix'] = conf_matrix\n",
    "    \n",
    "    for label in np.unique(true_vals):\n",
    "        seed_metrics[seed_val]['precision'].append((label, precision[label]))\n",
    "        seed_metrics[seed_val]['recall'].append((label, recall[label]))\n",
    "        seed_metrics[seed_val]['f1'].append((label, f1[label]))\n",
    "\n",
    "# Write the precision, recall, F1 scores, and seed values to a file\n",
    "with open('combine_to6_20times_results_with_seeds.txt', 'w') as f:\n",
    "    f.write('Seed\\tLabel\\tPrecision\\tRecall\\tF1\\tAccuracy\\n')\n",
    "    for seed_val in seed_values:\n",
    "        for label, precision_val in seed_metrics[seed_val]['precision']:\n",
    "            recall_val = next(val for lbl, val in seed_metrics[seed_val]['recall'] if lbl == label)\n",
    "            f1_val = next(val for lbl, val in seed_metrics[seed_val]['f1'] if lbl == label)\n",
    "            accuracy = seed_metrics[seed_val]['accuracy']\n",
    "            f.write(f'{seed_val}\\t{label}\\t{precision_val:.4f}\\t{recall_val:.4f}\\t{f1_val:.4f}\\t{accuracy:.4f}\\n')\n",
    "\n",
    "        # Save the confusion matrix\n",
    "        f.write(f'\\nConfusion Matrix for Seed {seed_val}:\\n')\n",
    "        f.write(np.array2string(seed_metrics[seed_val]['confusion_matrix'], separator=', '))\n",
    "        f.write('\\n')\n",
    "    \n",
    "# Initialize lists to hold precision, recall, and f1 values for each label\n",
    "precision_dict, recall_dict, f1_dict = {}, {}, {}\n",
    "\n",
    "# Collect metrics across seeds\n",
    "for seed in seed_metrics:\n",
    "    for label, value in seed_metrics[seed]['precision']:\n",
    "        precision_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['recall']:\n",
    "        recall_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['f1']:\n",
    "        f1_dict.setdefault(label, []).append(value)\n",
    "\n",
    "# Compute mean and std for precision, recall, and f1\n",
    "labels = sorted(precision_dict.keys())\n",
    "precision_mean = [np.mean(precision_dict[label]) for label in labels]\n",
    "precision_std = [np.std(precision_dict[label]) for label in labels]\n",
    "recall_mean = [np.mean(recall_dict[label]) for label in labels]\n",
    "recall_std = [np.std(recall_dict[label]) for label in labels]\n",
    "f1_mean = [np.mean(f1_dict[label]) for label in labels]\n",
    "f1_std = [np.std(f1_dict[label]) for label in labels]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(labels))  # label indices\n",
    "width = 0.25  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Bar plots with mean values\n",
    "bars_precision = ax.bar(x - width, precision_mean, width, label='Precision', color='b')\n",
    "bars_recall = ax.bar(x, recall_mean, width, label='Recall', color='g')\n",
    "bars_f1 = ax.bar(x + width, f1_mean, width, label='F1 Score', color='r')\n",
    "\n",
    "# Annotate bars with mean and std values\n",
    "# Annotate bars with mean and std values, with smaller font size\n",
    "for bars, means, stds in zip([bars_precision, bars_recall, bars_f1],\n",
    "                             [precision_mean, recall_mean, f1_mean],\n",
    "                             [precision_std, recall_std, f1_std]):\n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "                f'{mean:.2f}\\n±{std:.2f}', ha='center', va='bottom', fontsize=8)  # Smaller font size\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_title('Mean and Standard Deviation of Precision, Recall, and F1 Score by Label')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Set y-axis limit to [0, 1]\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Move legend outside the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the plot to fit the legend\n",
    "plt.savefig('combine_top6_20times_results_plot.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd69b5e-c47f-46d5-aa85-dc75901e5e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
