{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa5fa24-8462-46cb-8a01-e927df5e319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "SMM4H top 3                             ade  soc_code  label\n",
      "3                            AD  10037175      0\n",
      "4                         focus  10029205      2\n",
      "5                          died  10018065      1\n",
      "8                        dreams  10037175      0\n",
      "10                   withdrawal  10018065      1\n",
      "...                         ...       ...    ...\n",
      "1695       talk a mile a minute  10037175      0\n",
      "1698     can't go back to sleep  10037175      0\n",
      "1703                 chest hurt  10018065      1\n",
      "1704   got ten minutes of sleep  10037175      0\n",
      "1708  never have another orgasm  10037175      0\n",
      "\n",
      "[734 rows x 3 columns]\n",
      "CADEC top 3                             ade  soc_code  label\n",
      "926            voracious hunger  10018065      1\n",
      "927            loss of appetite  10018065      1\n",
      "929            lack of appetite  10018065      1\n",
      "931                    anorexia  10018065      1\n",
      "932                    anorexic  10018065      1\n",
      "...                         ...       ...    ...\n",
      "5326  short term memory lacking  10037175      0\n",
      "5328      couldn't eat or drink  10037175      0\n",
      "5329              Could not eat  10037175      0\n",
      "5331           can't eat normal  10037175      0\n",
      "5332   Disturbed sleep patterns  10037175      0\n",
      "\n",
      "[1341 rows x 3 columns]\n",
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[AC:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:19,  1.82it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:19,  1.82it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  2.99it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  2.99it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:00<00:08,  3.88it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:08,  3.88it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:07,  4.45it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:07,  4.45it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:06,  4.96it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:06,  4.96it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:06,  5.10it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:06,  5.10it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:01<00:05,  5.44it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:01<00:05,  5.44it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:01<00:05,  5.54it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:01<00:05,  5.54it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:01<00:05,  5.51it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:05,  5.51it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:04,  5.67it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:04,  5.67it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:02<00:04,  5.75it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:02<00:04,  5.75it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:02<00:04,  5.84it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:02<00:04,  5.84it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:02<00:03,  6.05it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:02<00:03,  6.05it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:02<00:03,  6.05it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:02<00:03,  6.05it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:02<00:03,  5.98it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:03<00:03,  5.98it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:03<00:03,  6.02it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:03<00:03,  6.02it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:03<00:03,  6.02it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:03<00:03,  6.02it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:03<00:03,  5.93it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:03<00:03,  5.93it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:03<00:02,  6.06it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:03<00:02,  6.06it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:03<00:02,  5.99it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:03<00:02,  5.99it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:03<00:02,  6.03it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:04<00:02,  6.03it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:04<00:02,  5.96it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:04<00:02,  5.96it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:04<00:02,  6.09it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:04<00:02,  6.09it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:04<00:02,  5.98it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:04<00:02,  5.98it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:04<00:01,  6.01it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:04<00:01,  6.01it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:04<00:01,  6.13it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:04<00:01,  6.13it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:04<00:01,  6.03it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:05<00:01,  6.03it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:05<00:01,  5.93it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:05<00:01,  5.93it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:05<00:01,  5.91it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:05<00:01,  5.91it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:05<00:01,  5.96it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:05<00:01,  5.96it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:05<00:01,  5.81it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:05<00:01,  5.81it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:05<00:00,  5.95it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:05<00:00,  5.95it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:05<00:00,  5.91it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:06<00:00,  5.91it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:06<00:00,  6.05it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:06<00:00,  6.05it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:06<00:00,  5.98it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:06<00:00,  5.98it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:06<00:00,  5.95it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:06<00:00,  5.95it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:06<00:00,  6.57it/s, training_loss=0.390]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.57s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.27it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.27it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.15it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.15it/s, training_loss=0.427]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:00<00:10,  3.14it/s, training_loss=0.427]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:10,  3.14it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:09,  3.37it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:09,  3.37it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:08,  3.90it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:08,  3.90it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:07,  4.22it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:07,  4.22it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:01<00:06,  4.53it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:01<00:06,  4.53it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:01<00:06,  4.81it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:06,  4.81it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:05,  4.92it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:05,  4.92it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:05,  5.05it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:05,  5.05it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:02<00:04,  5.25it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:02<00:04,  5.25it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:02<00:04,  5.32it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:02<00:04,  5.32it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:02<00:04,  4.99it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:03<00:04,  4.99it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:03<00:05,  4.48it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:03<00:05,  4.48it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:03<00:04,  4.76it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:03<00:04,  4.76it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:03<00:04,  4.22it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:04<00:04,  4.22it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:04<00:05,  3.77it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:04<00:05,  3.77it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:04<00:05,  3.55it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:04<00:05,  3.55it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:04<00:05,  3.46it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:04<00:05,  3.46it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:04<00:05,  3.35it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:05<00:05,  3.35it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:05<00:04,  3.32it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:05<00:04,  3.32it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:05<00:04,  3.31it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:05<00:04,  3.31it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:05<00:04,  3.28it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:06<00:04,  3.28it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:04,  3.24it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:04,  3.24it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:06<00:03,  3.16it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:06<00:03,  3.16it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:06<00:03,  3.17it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:07<00:03,  3.17it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:07<00:03,  3.15it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:07<00:03,  3.15it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:07<00:02,  3.06it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:07<00:02,  3.06it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:07<00:02,  3.07it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:08<00:02,  3.07it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:08<00:02,  3.07it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:08<00:02,  3.07it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:08<00:01,  3.07it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:08<00:01,  3.07it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:08<00:01,  3.12it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:01,  3.12it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:01,  3.08it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:01,  3.08it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:09<00:00,  3.15it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:09<00:00,  3.15it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:09<00:00,  3.12it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:10<00:00,  3.12it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  3.12it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  3.12it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:10<00:00,  3.36it/s, training_loss=0.362]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.34s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:12,  2.77it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:12,  2.77it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.00it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:11,  3.00it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  3.00it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  3.00it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:10,  3.01it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:10,  3.01it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:10,  3.05it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:10,  3.05it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:10,  3.03it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:10,  3.03it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:09,  3.02it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:09,  3.02it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:09,  3.01it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:09,  3.01it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  2.97it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  2.97it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:08,  3.06it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:08,  3.06it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  3.14it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  3.14it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:03<00:08,  3.10it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:08,  3.10it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.08it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.08it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  3.08it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  3.08it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:04<00:06,  3.61it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:04<00:06,  3.61it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:04<00:05,  4.01it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:05,  4.01it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:04,  4.45it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:04,  4.45it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:05<00:04,  4.63it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:05<00:04,  4.63it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:05<00:03,  4.73it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:05<00:03,  4.73it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:05<00:03,  4.92it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:05<00:03,  4.92it/s, training_loss=0.422]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:05<00:03,  5.08it/s, training_loss=0.422]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:06<00:03,  5.08it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:06<00:02,  5.11it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:06<00:02,  5.11it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:06<00:02,  5.11it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:06<00:02,  5.11it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:02,  5.20it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:02,  5.20it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:06<00:02,  4.59it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:07<00:02,  4.59it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:07<00:02,  4.05it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:07<00:02,  4.05it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:07<00:02,  3.79it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:07<00:02,  3.79it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:07<00:02,  3.46it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:08<00:02,  3.46it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:08<00:02,  3.35it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:08<00:02,  3.35it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:08<00:02,  3.24it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:08<00:02,  3.24it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:08<00:01,  3.16it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:09<00:01,  3.16it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:01,  3.14it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:01,  3.14it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:01,  3.15it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:01,  3.15it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:09<00:00,  3.15it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:09<00:00,  3.15it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:09<00:00,  3.17it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:10<00:00,  3.17it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  3.21it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  3.21it/s, training_loss=0.312]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:10<00:00,  3.41it/s, training_loss=0.312]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.51s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.435]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:12,  2.95it/s, training_loss=0.435]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:12,  2.95it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  2.94it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:11,  2.94it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  3.00it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  3.00it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:11,  2.95it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:11,  2.95it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:10,  3.01it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:10,  3.01it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:10,  3.02it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:10,  3.02it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:09,  3.10it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:09,  3.10it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:09,  3.06it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:09,  3.06it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:09,  3.03it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  3.03it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:08,  3.02it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:08,  3.02it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  2.98it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  2.98it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:03<00:08,  3.02it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:08,  3.02it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.02it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.02it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  3.02it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  3.02it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:04<00:07,  3.00it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:07,  3.00it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:06,  3.05it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:06,  3.05it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:06,  3.05it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:06,  3.05it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:05<00:06,  3.00it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:06<00:06,  3.00it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:06<00:05,  3.06it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:06<00:05,  3.06it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:06<00:05,  3.10it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:06<00:05,  3.10it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:06<00:04,  3.25it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:07<00:04,  3.25it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:07<00:04,  3.12it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:07<00:04,  3.12it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:07<00:04,  3.06it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:07<00:04,  3.06it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:07<00:04,  3.12it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:08<00:04,  3.12it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:08<00:03,  3.64it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:08<00:03,  3.64it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:08<00:02,  4.03it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:08<00:02,  4.03it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:08<00:02,  4.45it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:08<00:02,  4.45it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:08<00:01,  4.64it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:08<00:01,  4.64it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:08<00:01,  4.72it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:08<00:01,  4.72it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:08<00:01,  4.89it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:09<00:01,  4.89it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:09<00:01,  4.98it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:09<00:01,  4.98it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:00,  5.07it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:00,  5.07it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:00,  5.13it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:00,  5.13it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:09<00:00,  5.18it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:10<00:00,  5.18it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:10<00:00,  4.55it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:10<00:00,  4.55it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  3.90it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  3.90it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:10<00:00,  3.93it/s, training_loss=0.358]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.61s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.446]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.10it/s, training_loss=0.446]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.10it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.13it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.13it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:00<00:11,  3.09it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  3.09it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:10,  3.04it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:10,  3.04it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:10,  3.02it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:10,  3.02it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:10,  3.01it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:10,  3.01it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:09,  3.01it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:09,  3.01it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:09,  3.03it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:09,  3.03it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:09,  3.07it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  3.07it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:08,  3.07it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:08,  3.07it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  3.06it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  3.06it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:03<00:08,  3.09it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:08,  3.09it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.05it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.05it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  3.08it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  3.08it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:04<00:07,  3.10it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:07,  3.10it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:06,  3.08it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:06,  3.08it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:06,  3.11it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:06,  3.11it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:05<00:06,  3.11it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:06<00:06,  3.11it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:06<00:05,  3.15it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:06<00:05,  3.15it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:06<00:05,  3.12it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:06<00:05,  3.12it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:06<00:05,  3.12it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:07<00:05,  3.12it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:07<00:04,  3.14it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:07<00:04,  3.14it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:07<00:04,  3.09it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:07<00:04,  3.09it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:07<00:04,  3.16it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:08<00:04,  3.16it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:08<00:03,  3.11it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:08<00:03,  3.11it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:08<00:03,  3.13it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:08<00:03,  3.13it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:08<00:03,  3.09it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:09<00:03,  3.09it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:09<00:02,  3.11it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:09<00:02,  3.11it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:09<00:02,  3.07it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:09<00:02,  3.07it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:09<00:02,  3.10it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:10<00:02,  3.10it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:10<00:01,  3.14it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:10<00:01,  3.14it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:10<00:01,  3.21it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:10<00:01,  3.21it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:10<00:01,  3.19it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:10<00:01,  3.19it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:10<00:00,  3.10it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:11<00:00,  3.10it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:11<00:00,  3.15it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:11<00:00,  3.15it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:11<00:00,  3.67it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:11<00:00,  3.67it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:11<00:00,  4.33it/s, training_loss=0.360]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.58s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:16,  2.18it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:16,  2.18it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:15,  2.28it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:15,  2.28it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:13,  2.46it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:13,  2.46it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:12,  2.58it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:12,  2.58it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:12,  2.66it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:12,  2.66it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:11,  2.76it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:11,  2.76it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:10,  2.83it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:10,  2.83it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:10,  2.88it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:10,  2.88it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  2.92it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  2.92it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:09,  2.98it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:09,  2.98it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  2.99it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:04<00:08,  2.99it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:08,  2.96it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:08,  2.96it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.03it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.03it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  3.04it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:05<00:07,  3.04it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:07,  3.02it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:07,  3.02it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:06,  3.02it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:06,  3.02it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:06,  3.06it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:06<00:06,  3.06it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:06<00:06,  3.03it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:06<00:06,  3.03it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:06<00:05,  3.04it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:06<00:05,  3.04it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:06<00:05,  3.06it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:07<00:05,  3.06it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:07<00:05,  3.10it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:07<00:05,  3.10it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:07<00:04,  3.06it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:07<00:04,  3.06it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:07<00:04,  3.09it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:08<00:04,  3.09it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:08<00:04,  3.06it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:08<00:04,  3.06it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:08<00:03,  3.08it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:08<00:03,  3.08it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:08<00:03,  3.06it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:09<00:03,  3.06it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:09<00:03,  3.08it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:09<00:03,  3.08it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:09<00:02,  3.07it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:09<00:02,  3.07it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:09<00:02,  3.04it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:10<00:02,  3.04it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:10<00:02,  2.99it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:10<00:02,  2.99it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:10<00:02,  3.00it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:10<00:02,  3.00it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:10<00:01,  3.01it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:11<00:01,  3.01it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:11<00:01,  3.04it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:11<00:01,  3.04it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:11<00:00,  3.04it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:11<00:00,  3.04it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:11<00:00,  3.05it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:12<00:00,  3.05it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:12<00:00,  3.03it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:12<00:00,  3.03it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:12<00:00,  3.28it/s, training_loss=0.376]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.43s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.437]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.08it/s, training_loss=0.437]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.08it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.07it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.07it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:00<00:10,  3.10it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:10,  3.10it/s, training_loss=0.418]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:10,  3.15it/s, training_loss=0.418]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:10,  3.15it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:09,  3.23it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:09,  3.23it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:09,  3.37it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:09,  3.37it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:10,  2.90it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:10,  2.90it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:11,  2.62it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:11,  2.62it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:10,  2.58it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:10,  2.58it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:10,  2.60it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:10,  2.60it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:09,  2.67it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:04<00:09,  2.67it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:09,  2.76it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:09,  2.76it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:08,  2.83it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:08,  2.83it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  2.90it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:05<00:07,  2.90it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:07,  2.95it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:07,  2.95it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:07,  2.99it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:07,  2.99it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:06,  3.00it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:06<00:06,  3.00it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:06<00:06,  3.01it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:06<00:06,  3.01it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:06<00:05,  3.02it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:06<00:05,  3.02it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:06<00:05,  3.04it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:07<00:05,  3.04it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:07<00:05,  3.06it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:07<00:05,  3.06it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:07<00:04,  3.05it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:07<00:04,  3.05it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:07<00:04,  3.03it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:08<00:04,  3.03it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:08<00:04,  3.05it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:08<00:04,  3.05it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:08<00:03,  3.05it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:08<00:03,  3.05it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:08<00:03,  3.46it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:08<00:03,  3.46it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:08<00:02,  3.95it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:09<00:02,  3.95it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:09<00:02,  4.37it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:09<00:02,  4.37it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:09<00:01,  4.67it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:09<00:01,  4.67it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:09<00:01,  4.91it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:09<00:01,  4.91it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:09<00:01,  5.14it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:09<00:01,  5.14it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:00,  5.29it/s, training_loss=0.317]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:00,  5.29it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:00,  5.30it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:10<00:00,  5.30it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:10<00:00,  5.45it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:10<00:00,  5.45it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:10<00:00,  5.37it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:10<00:00,  5.37it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  5.41it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  5.41it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:10<00:00,  5.83it/s, training_loss=0.342]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:16,  2.15it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:16,  2.15it/s, training_loss=0.419]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:13,  2.67it/s, training_loss=0.419]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:13,  2.67it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  2.97it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  2.97it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:10,  3.08it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:10,  3.08it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:10,  3.19it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:10,  3.19it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:09,  3.27it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:09,  3.27it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:08,  3.38it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:08,  3.38it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:08,  3.48it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:08,  3.48it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:07,  3.58it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:07,  3.58it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:07,  3.78it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:07,  3.78it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:06,  4.00it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:06,  4.00it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:03<00:06,  4.10it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:03<00:06,  4.10it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:03<00:05,  4.06it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:03<00:05,  4.06it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:03<00:05,  4.04it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:05,  4.04it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:04<00:05,  4.11it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:04<00:05,  4.11it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:04<00:05,  4.11it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:04<00:05,  4.11it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:04<00:04,  4.04it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:04<00:04,  4.04it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:04<00:04,  3.99it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:05<00:04,  3.99it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:05<00:04,  4.09it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:05<00:04,  4.09it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:05<00:04,  4.19it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:05<00:04,  4.19it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:05<00:03,  4.34it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:05<00:03,  4.34it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:05<00:03,  4.36it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:06<00:03,  4.36it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:06<00:03,  4.33it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:06<00:03,  4.33it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:02,  4.39it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:02,  4.39it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:06<00:02,  4.46it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:06<00:02,  4.46it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:06<00:02,  4.49it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:06<00:02,  4.49it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:06<00:02,  4.52it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:07<00:02,  4.52it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:07<00:01,  4.53it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:07<00:01,  4.53it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:07<00:01,  4.58it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:07<00:01,  4.58it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:07<00:01,  4.61it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:07<00:01,  4.61it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:07<00:01,  4.66it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:07<00:01,  4.66it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:08<00:01,  4.71it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:08<00:01,  4.71it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:08<00:00,  4.73it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:08<00:00,  4.73it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:08<00:00,  4.69it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:08<00:00,  4.69it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:08<00:00,  4.72it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:08<00:00,  4.72it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:08<00:00,  4.77it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:08<00:00,  4.77it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:08<00:00,  5.16it/s, training_loss=0.363]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.00s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.20it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.20it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:10,  3.20it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:10,  3.20it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:00<00:10,  3.28it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:10,  3.28it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:09,  3.45it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:09,  3.45it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:08,  3.96it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:08,  3.96it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:07,  4.34it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:07,  4.34it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:01<00:06,  4.62it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:01<00:06,  4.62it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:01<00:05,  4.94it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:05,  4.94it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:05,  5.06it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:05,  5.06it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:05,  5.14it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:05,  5.14it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:02<00:04,  5.33it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:02<00:04,  5.33it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:02<00:04,  5.47it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:02<00:04,  5.47it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:02<00:04,  5.04it/s, training_loss=0.391]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:03<00:04,  5.04it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:03<00:05,  4.57it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:03<00:05,  4.57it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:03<00:05,  4.30it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:03<00:05,  4.30it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:03<00:05,  3.89it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:04<00:05,  3.89it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:04<00:05,  3.59it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:04<00:05,  3.59it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:04<00:05,  3.47it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:04<00:05,  3.47it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:04<00:05,  3.33it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:05<00:05,  3.33it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:05<00:05,  3.24it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:05<00:05,  3.24it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:05<00:05,  3.18it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:05<00:05,  3.18it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:05<00:04,  3.19it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:05<00:04,  3.19it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:05<00:04,  3.14it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:06<00:04,  3.14it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:04,  3.16it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:04,  3.16it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:06<00:03,  3.08it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:06<00:03,  3.08it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:06<00:03,  3.12it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:07<00:03,  3.12it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:07<00:03,  3.10it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:07<00:03,  3.10it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:07<00:02,  3.04it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:07<00:02,  3.04it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:07<00:02,  3.04it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:08<00:02,  3.04it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:08<00:02,  2.98it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:08<00:02,  2.98it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:08<00:02,  2.98it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:08<00:02,  2.98it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:08<00:01,  3.01it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:01,  3.01it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:01,  3.07it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:01,  3.07it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:09<00:00,  3.04it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:09<00:00,  3.04it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:09<00:00,  2.98it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:10<00:00,  2.98it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  2.98it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  2.98it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:10<00:00,  3.19it/s, training_loss=0.354]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.56s/it]\u001b[A\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.07it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.07it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.02it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:11,  3.02it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  2.94it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  2.94it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:11,  2.95it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:11,  2.95it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:10,  2.94it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:10,  2.94it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:10,  2.95it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:10,  2.95it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:10,  2.92it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:10,  2.92it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:09,  2.95it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:09,  2.95it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  3.00it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  3.00it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:08,  3.00it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:08,  3.00it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  3.09it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  3.09it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:03<00:08,  3.12it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:08,  3.12it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.08it/s, training_loss=0.392]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:07,  3.08it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  3.05it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  3.05it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:04<00:06,  3.55it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:06,  3.55it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:05,  3.95it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:05,  3.95it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:04,  4.30it/s, training_loss=0.313]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:05<00:04,  4.30it/s, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:05<00:04,  4.52it/s, training_loss=0.396]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:05<00:04,  4.52it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:05<00:03,  4.64it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:05<00:03,  4.64it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:05<00:03,  4.75it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:05<00:03,  4.75it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:06<00:03,  4.76it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:06<00:03,  4.76it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:06<00:03,  4.80it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:06<00:03,  4.80it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:06<00:02,  4.89it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:06<00:02,  4.89it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:02,  5.02it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:06<00:02,  5.02it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:06<00:02,  4.27it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:07<00:02,  4.27it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:07<00:02,  3.84it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:07<00:02,  3.84it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:07<00:02,  3.48it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:07<00:02,  3.48it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:07<00:02,  3.33it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:08<00:02,  3.33it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:08<00:02,  3.21it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:08<00:02,  3.21it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:08<00:02,  3.16it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:08<00:02,  3.16it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:08<00:01,  3.06it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:09<00:01,  3.06it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:01,  3.04it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:09<00:01,  3.04it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:01,  3.06it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:09<00:01,  3.06it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:09<00:00,  3.06it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:10<00:00,  3.06it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:10<00:00,  3.09it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:10<00:00,  3.09it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  3.00it/s, training_loss=0.393]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:10<00:00,  3.00it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:10<00:00,  3.21it/s, training_loss=0.355]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.85s/it]\u001b[A\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:12,  2.81it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:12,  2.81it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:12,  2.80it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:12,  2.80it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  2.85it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  2.85it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:11,  2.93it/s, training_loss=0.323]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:11,  2.93it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:10,  2.92it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:10,  2.92it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:10,  2.95it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:10,  2.95it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:10,  2.98it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:10,  2.98it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:09,  3.02it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:09,  3.02it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  2.97it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:03<00:09,  2.97it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:09,  2.94it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:09,  2.94it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:08,  2.89it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:04<00:08,  2.89it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:08,  2.86it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:08,  2.86it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:08,  2.86it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:08,  2.86it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:07,  2.91it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:05<00:07,  2.91it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:08,  2.66it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:08,  2.66it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:08,  2.56it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:06<00:08,  2.56it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:06<00:08,  2.40it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:06<00:08,  2.40it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:06<00:08,  2.26it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:07<00:08,  2.26it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:07<00:08,  2.19it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:07<00:08,  2.19it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:07<00:07,  2.15it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:08<00:07,  2.15it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:08<00:07,  2.11it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:08<00:07,  2.11it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:08<00:07,  2.03it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:09<00:07,  2.03it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:09<00:06,  2.12it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:09<00:06,  2.12it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:09<00:05,  2.34it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:09<00:05,  2.34it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:09<00:04,  2.47it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:10<00:04,  2.47it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:10<00:04,  2.51it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:10<00:04,  2.51it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:10<00:03,  2.63it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:10<00:03,  2.63it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:10<00:03,  2.63it/s, training_loss=0.311]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:11<00:03,  2.63it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:11<00:03,  2.59it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:11<00:03,  2.59it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:11<00:02,  2.38it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:12<00:02,  2.38it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:12<00:02,  2.27it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:12<00:02,  2.27it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:12<00:02,  2.19it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:13<00:02,  2.19it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:13<00:01,  2.11it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:13<00:01,  2.11it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:13<00:01,  2.12it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:14<00:01,  2.12it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:14<00:00,  2.04it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:14<00:00,  2.04it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:14<00:00,  2.05it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:15<00:00,  2.05it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:15<00:00,  2.22it/s, training_loss=0.361]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.11s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:12,  2.80it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:12,  2.80it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.04it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.04it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:00<00:09,  3.66it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:09,  3.66it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:07,  4.18it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:07,  4.18it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:07,  4.53it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:07,  4.53it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:06,  4.55it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:06,  4.55it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:01<00:07,  4.26it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:07,  4.26it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:08,  3.57it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:08,  3.57it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:08,  3.31it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:08,  3.31it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:08,  3.15it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:03<00:08,  3.15it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:09,  2.73it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:09,  2.73it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:03<00:09,  2.53it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:09,  2.53it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:10,  2.38it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:04<00:10,  2.38it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:04<00:10,  2.26it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:05<00:10,  2.26it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:10,  2.16it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:10,  2.16it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:09,  2.13it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:06<00:09,  2.13it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:06<00:09,  2.11it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:06<00:09,  2.11it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:06<00:09,  2.10it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:07<00:09,  2.10it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:07<00:08,  2.07it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:07<00:08,  2.07it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:07<00:08,  2.05it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:08<00:08,  2.05it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:08<00:07,  2.03it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:08<00:07,  2.03it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:08<00:07,  2.04it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:09<00:07,  2.04it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:09<00:06,  2.03it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:09<00:06,  2.03it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:09<00:06,  2.04it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:10<00:06,  2.04it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:10<00:05,  2.05it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:10<00:05,  2.05it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:10<00:05,  2.03it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:11<00:05,  2.03it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:11<00:04,  2.04it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:11<00:04,  2.04it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:11<00:04,  2.07it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:12<00:04,  2.07it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:12<00:03,  2.04it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:12<00:03,  2.04it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:12<00:03,  2.05it/s, training_loss=0.399]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:13<00:03,  2.05it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:13<00:02,  2.03it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:13<00:02,  2.03it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:13<00:02,  2.04it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:14<00:02,  2.04it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:14<00:01,  2.03it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:14<00:01,  2.03it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:14<00:01,  2.04it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:15<00:01,  2.04it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:15<00:00,  2.03it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:15<00:00,  2.03it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:15<00:00,  2.02it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:15<00:00,  2.02it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:15<00:00,  2.17it/s, training_loss=0.338]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.93s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:18,  2.00it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:18,  2.00it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:16,  2.09it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:16,  2.09it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:16,  2.07it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:16,  2.07it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:16,  2.05it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:02<00:16,  2.05it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:15,  2.04it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:15,  2.04it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:15,  2.04it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  2.04it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:15,  2.00it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:15,  2.00it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:14,  2.01it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:04<00:14,  2.01it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:13,  2.03it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:13,  2.03it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:04<00:13,  2.05it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:05<00:13,  2.05it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:12,  2.07it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:12,  2.07it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:05<00:12,  2.03it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:06<00:12,  2.03it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.03it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.03it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:06<00:11,  2.05it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:07<00:11,  2.05it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:09,  2.28it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:09,  2.28it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:07<00:08,  2.46it/s, training_loss=0.404]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:07<00:08,  2.46it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:07<00:07,  2.63it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:07,  2.63it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:08<00:07,  2.69it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:08<00:07,  2.69it/s, training_loss=0.406]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:08<00:06,  2.77it/s, training_loss=0.406]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:08<00:06,  2.77it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:08<00:06,  2.81it/s, training_loss=0.423]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:09<00:06,  2.81it/s, training_loss=0.435]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:09<00:05,  2.96it/s, training_loss=0.435]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:09<00:05,  2.96it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:09<00:04,  3.40it/s, training_loss=0.394]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:09<00:04,  3.40it/s, training_loss=0.414]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:09<00:03,  3.85it/s, training_loss=0.414]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:09<00:03,  3.85it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:09<00:03,  4.29it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:09<00:03,  4.29it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:09<00:02,  4.60it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:10<00:02,  4.60it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:10<00:02,  4.78it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:10<00:02,  4.78it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:10<00:02,  4.85it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:10<00:02,  4.85it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:10<00:01,  4.87it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:10<00:01,  4.87it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:10<00:01,  4.89it/s, training_loss=0.388]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:10<00:01,  4.89it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:10<00:01,  5.01it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:11<00:01,  5.01it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:11<00:01,  4.87it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:11<00:01,  4.87it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:11<00:01,  4.78it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:11<00:01,  4.78it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:11<00:01,  3.86it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:12<00:01,  3.86it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:12<00:00,  3.46it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:12<00:00,  3.46it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:12<00:00,  3.19it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:12<00:00,  3.19it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:12<00:00,  2.87it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:13<00:00,  2.87it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:13<00:00,  2.81it/s, training_loss=0.348]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.22s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:17,  2.06it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:01<00:17,  2.06it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:17,  1.99it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:17,  1.99it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:16,  2.02it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:16,  2.02it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:16,  2.01it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:02<00:16,  2.01it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:15,  2.05it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:15,  2.05it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:15,  1.99it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  1.99it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:14,  2.02it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:14,  2.02it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:14,  2.03it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:04<00:14,  2.03it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:13,  2.02it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:13,  2.02it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:04<00:13,  2.02it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:05<00:13,  2.02it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:12,  2.01it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:12,  2.01it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:05<00:12,  2.02it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:06<00:12,  2.02it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.02it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.02it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:06<00:11,  2.03it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:07<00:11,  2.03it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:10,  2.02it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:10,  2.02it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:07<00:10,  2.01it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:08<00:10,  2.01it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:09,  2.01it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:09,  2.01it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:08<00:09,  2.03it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:09<00:09,  2.03it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:09<00:09,  2.00it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:09<00:09,  2.00it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:09<00:08,  2.02it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:10<00:08,  2.02it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:10<00:07,  2.03it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:10<00:07,  2.03it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:10<00:07,  2.02it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:11<00:07,  2.02it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:11<00:07,  1.99it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:11<00:07,  1.99it/s, training_loss=0.432]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:11<00:06,  1.99it/s, training_loss=0.432]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:12<00:06,  1.99it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:12<00:05,  2.02it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:12<00:05,  2.02it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:12<00:05,  2.05it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:13<00:05,  2.05it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:13<00:04,  2.03it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:13<00:04,  2.03it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:13<00:04,  2.05it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:14<00:04,  2.05it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:14<00:03,  2.03it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:14<00:03,  2.03it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:14<00:03,  2.04it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:15<00:03,  2.04it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:15<00:02,  2.02it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:15<00:02,  2.02it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:15<00:02,  2.04it/s, training_loss=0.328]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:16<00:02,  2.04it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:16<00:01,  2.05it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:16<00:01,  2.05it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:16<00:01,  2.05it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:17<00:01,  2.05it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:17<00:00,  2.06it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:17<00:00,  2.06it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:17<00:00,  2.04it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:18<00:00,  2.04it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:18<00:00,  2.15it/s, training_loss=0.318]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.19s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:22,  1.60it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:01<00:22,  1.60it/s, training_loss=0.465]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:19,  1.81it/s, training_loss=0.465]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:19,  1.81it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:17,  1.99it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:02<00:17,  1.99it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:02<00:17,  1.94it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:02<00:17,  1.94it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:16,  1.94it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:03<00:16,  1.94it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  1.96it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  1.96it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:15,  1.95it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:04<00:15,  1.95it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:04<00:14,  1.99it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:04<00:14,  1.99it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:14,  1.99it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:05<00:14,  1.99it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:05<00:13,  2.01it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:05<00:13,  2.01it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:12,  2.01it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:06<00:12,  2.01it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:06<00:12,  2.02it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:06<00:12,  2.02it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.02it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:07<00:11,  2.02it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:07<00:11,  1.99it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:07<00:11,  1.99it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:10,  2.02it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:08<00:10,  2.02it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:08<00:10,  2.02it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:08<00:10,  2.02it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:09,  2.02it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:09<00:09,  2.02it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:09<00:09,  2.01it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:09<00:09,  2.01it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:09<00:08,  2.01it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:10<00:08,  2.01it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:10<00:08,  2.03it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:10<00:08,  2.03it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:10<00:07,  2.04it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:11<00:07,  2.04it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:11<00:07,  2.05it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:11<00:07,  2.05it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:11<00:06,  2.03it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:12<00:06,  2.03it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:12<00:06,  2.02it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:12<00:06,  2.02it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:12<00:05,  2.03it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:13<00:05,  2.03it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:13<00:05,  2.00it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:13<00:05,  2.00it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:13<00:04,  2.02it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:14<00:04,  2.02it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:14<00:04,  2.01it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:14<00:04,  2.01it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:14<00:03,  2.03it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:15<00:03,  2.03it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:15<00:03,  2.00it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:15<00:03,  2.00it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:15<00:02,  2.04it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:15<00:02,  2.04it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:15<00:02,  2.02it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:16<00:02,  2.02it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:16<00:01,  2.00it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:16<00:01,  2.00it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:16<00:01,  2.04it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:17<00:01,  2.04it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:17<00:00,  2.04it/s, training_loss=0.304]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:17<00:00,  2.04it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:17<00:00,  2.01it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:18<00:00,  2.01it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:18<00:00,  2.13it/s, training_loss=0.371]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.38s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:19,  1.89it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:01<00:19,  1.89it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:17,  1.99it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:17,  1.99it/s, training_loss=0.395]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:16,  2.06it/s, training_loss=0.395]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:16,  2.06it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:15,  2.07it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:02<00:15,  2.07it/s, training_loss=0.432]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:16,  1.99it/s, training_loss=0.432]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:03<00:16,  1.99it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  1.97it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  1.97it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:14,  2.14it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:14,  2.14it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:12,  2.36it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:04<00:12,  2.36it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:11,  2.51it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:11,  2.51it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:04<00:10,  2.62it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:04<00:10,  2.62it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:04<00:08,  2.97it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:04<00:08,  2.97it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:04<00:07,  3.38it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:05<00:07,  3.38it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:05<00:06,  3.80it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:05<00:06,  3.80it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:05<00:05,  4.01it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:05<00:05,  4.01it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:05,  4.04it/s, training_loss=0.397]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:05<00:05,  4.04it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:05<00:05,  3.51it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:06<00:05,  3.51it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:06<00:06,  3.21it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:06<00:06,  3.21it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:06<00:06,  3.12it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:07<00:06,  3.12it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:07<00:06,  2.77it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:07<00:06,  2.77it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:07<00:06,  2.48it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:08<00:06,  2.48it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:08<00:06,  2.29it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:08<00:06,  2.29it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:08<00:06,  2.20it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:09<00:06,  2.20it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:09<00:06,  2.13it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:09<00:06,  2.13it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:09<00:06,  2.09it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:10<00:06,  2.09it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:10<00:05,  2.04it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:10<00:05,  2.04it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:10<00:05,  2.03it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:11<00:05,  2.03it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:11<00:04,  2.04it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:11<00:04,  2.04it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:11<00:04,  2.01it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:12<00:04,  2.01it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:12<00:03,  2.01it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:12<00:03,  2.01it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:12<00:03,  2.02it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:13<00:03,  2.02it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:13<00:02,  2.02it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:13<00:02,  2.02it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:13<00:02,  2.07it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:14<00:02,  2.07it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:14<00:01,  2.05it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:14<00:01,  2.05it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:14<00:01,  2.05it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:14<00:01,  2.05it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:14<00:00,  2.05it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:15<00:00,  2.05it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:15<00:00,  2.02it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:15<00:00,  2.02it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:15<00:00,  2.18it/s, training_loss=0.353]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.87s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:17,  2.01it/s, training_loss=0.398]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:17,  2.01it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:17,  2.04it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:17,  2.04it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:16,  2.03it/s, training_loss=0.338]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:16,  2.03it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:16,  2.04it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:02<00:16,  2.04it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:15,  2.02it/s, training_loss=0.405]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:15,  2.02it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:15,  2.05it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  2.05it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:14,  2.06it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:14,  2.06it/s, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:13,  2.10it/s, training_loss=0.424]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:04<00:13,  2.10it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:13,  2.08it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:13,  2.08it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:04<00:13,  2.06it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:05<00:13,  2.06it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:12,  2.02it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:12,  2.02it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:05<00:12,  2.04it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:06<00:12,  2.04it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.03it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.03it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:06<00:11,  2.03it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:07<00:11,  2.03it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:10,  2.06it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:10,  2.06it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:07<00:10,  2.02it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:08<00:10,  2.02it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:09,  2.00it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:09,  2.00it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:08<00:09,  1.96it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:09<00:09,  1.96it/s, training_loss=0.395]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:09<00:08,  2.09it/s, training_loss=0.395]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:09<00:08,  2.09it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:09<00:07,  2.29it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:09<00:07,  2.29it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:09<00:06,  2.47it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:10<00:06,  2.47it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:10<00:05,  2.60it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:10<00:05,  2.60it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:10<00:05,  2.71it/s, training_loss=0.347]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:10<00:05,  2.71it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:10<00:04,  2.81it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:11<00:04,  2.81it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:11<00:04,  2.84it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:11<00:04,  2.84it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:11<00:03,  2.89it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:11<00:03,  2.89it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:11<00:03,  2.92it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:12<00:03,  2.92it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:12<00:03,  2.98it/s, training_loss=0.346]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:12<00:03,  2.98it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:12<00:02,  3.04it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:12<00:02,  3.04it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:12<00:02,  3.02it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:13<00:02,  3.02it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:13<00:01,  3.00it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:13<00:01,  3.00it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:13<00:01,  2.93it/s, training_loss=0.340]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:13<00:01,  2.93it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:13<00:01,  3.12it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:14<00:01,  3.12it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:14<00:00,  3.57it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:14<00:00,  3.57it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:14<00:00,  3.98it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:14<00:00,  3.98it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:14<00:00,  4.34it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:14<00:00,  4.34it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:14<00:00,  4.70it/s, training_loss=0.356]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.63s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:18,  1.93it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:01<00:18,  1.93it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:17,  1.96it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:17,  1.96it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:17,  1.97it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:02<00:17,  1.97it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:02<00:16,  2.01it/s, training_loss=0.389]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:02<00:16,  2.01it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:15,  2.00it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:03<00:15,  2.00it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  1.98it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  1.98it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:15,  1.98it/s, training_loss=0.339]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:04<00:15,  1.98it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:04<00:14,  1.97it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:04<00:14,  1.97it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:14,  1.98it/s, training_loss=0.382]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:05<00:14,  1.98it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:05<00:13,  1.99it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:05<00:13,  1.99it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:13,  1.99it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:06<00:13,  1.99it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:06<00:12,  2.00it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:06<00:12,  2.00it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.04it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:07<00:11,  2.04it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:07<00:11,  2.00it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:07<00:11,  2.00it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:10,  2.00it/s, training_loss=0.331]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:08<00:10,  2.00it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:08<00:10,  2.02it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:08<00:10,  2.02it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:09,  2.03it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:09,  2.03it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:08<00:09,  2.04it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:09<00:09,  2.04it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:09<00:08,  2.03it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:09<00:08,  2.03it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:09<00:08,  2.00it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:10<00:08,  2.00it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:10<00:07,  2.04it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:10<00:07,  2.04it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:10<00:07,  2.03it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:11<00:07,  2.03it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:11<00:06,  2.04it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:11<00:06,  2.04it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:11<00:06,  2.02it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:12<00:06,  2.02it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:12<00:05,  2.04it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:12<00:05,  2.04it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:12<00:05,  2.01it/s, training_loss=0.332]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:13<00:05,  2.01it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:13<00:05,  1.98it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:13<00:05,  1.98it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:13<00:04,  2.01it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:14<00:04,  2.01it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:14<00:03,  2.04it/s, training_loss=0.379]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:14<00:03,  2.04it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:14<00:03,  2.05it/s, training_loss=0.387]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:15<00:03,  2.05it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:15<00:02,  2.01it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:15<00:02,  2.01it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:15<00:02,  2.01it/s, training_loss=0.352]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:16<00:02,  2.01it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:16<00:01,  2.02it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:16<00:01,  2.02it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:16<00:01,  2.03it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:17<00:01,  2.03it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:17<00:00,  2.02it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:17<00:00,  2.02it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:17<00:00,  2.00it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:18<00:00,  2.00it/s, training_loss=0.309]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:18<00:00,  2.16it/s, training_loss=0.309]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.28s/it]\u001b[A\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.00it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:11,  3.00it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:11,  3.11it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:11,  3.11it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  2.92it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:11,  2.92it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:11,  2.94it/s, training_loss=0.366]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:11,  2.94it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:11,  2.88it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:01<00:11,  2.88it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:01<00:10,  3.08it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:10,  3.08it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:08,  3.62it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:02<00:08,  3.62it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:07,  4.06it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:02<00:07,  4.06it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:06,  4.48it/s, training_loss=0.373]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:02<00:06,  4.48it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:05,  4.82it/s, training_loss=0.358]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:02<00:05,  4.82it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:02<00:05,  4.97it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:03<00:05,  4.97it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:03<00:04,  5.07it/s, training_loss=0.375]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:03<00:04,  5.07it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:03<00:04,  5.24it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:03<00:04,  5.24it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:03<00:04,  5.29it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:03<00:04,  5.29it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:03<00:04,  5.30it/s, training_loss=0.322]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:03<00:04,  5.30it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:03<00:03,  5.45it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:03<00:03,  5.45it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:03<00:03,  5.41it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:04<00:03,  5.41it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:04<00:03,  5.39it/s, training_loss=0.369]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:04<00:03,  5.39it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:04<00:03,  5.41it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:04<00:03,  5.41it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:04<00:03,  5.47it/s, training_loss=0.345]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:04<00:03,  5.47it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:04<00:02,  5.57it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:04<00:02,  5.57it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:04<00:02,  5.50it/s, training_loss=0.401]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:05<00:02,  5.50it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:05<00:02,  5.45it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:05<00:02,  5.45it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:05<00:02,  5.52it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:05<00:02,  5.52it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:05<00:02,  5.50it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:05<00:02,  5.50it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:05<00:02,  5.45it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:05<00:02,  5.45it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:05<00:01,  5.48it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:05<00:01,  5.48it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:06<00:01,  5.32it/s, training_loss=0.371]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:06<00:01,  5.32it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:06<00:01,  5.25it/s, training_loss=0.365]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:06<00:01,  5.25it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:06<00:01,  5.10it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:06<00:01,  5.10it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:06<00:01,  5.17it/s, training_loss=0.333]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:06<00:01,  5.17it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:06<00:00,  5.09it/s, training_loss=0.306]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:06<00:00,  5.09it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:06<00:00,  5.04it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:07<00:00,  5.04it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:07<00:00,  4.98it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:07<00:00,  4.98it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:07<00:00,  4.97it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:07<00:00,  4.97it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:07<00:00,  4.06it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:08<00:00,  4.06it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:08<00:00,  3.91it/s, training_loss=0.362]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:   0%|                                                             | 0/37 [00:00<?, ?it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:15,  2.29it/s, training_loss=0.403]\u001b[A\n",
      "Epoch 1:   3%|█▍                                                   | 1/37 [00:00<00:15,  2.29it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:00<00:15,  2.24it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:   5%|██▊                                                  | 2/37 [00:01<00:15,  2.24it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:15,  2.18it/s, training_loss=0.360]\u001b[A\n",
      "Epoch 1:   8%|████▎                                                | 3/37 [00:01<00:15,  2.18it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:01<00:15,  2.18it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  11%|█████▋                                               | 4/37 [00:02<00:15,  2.18it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:15,  2.08it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  14%|███████▏                                             | 5/37 [00:02<00:15,  2.08it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:02<00:15,  2.02it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  16%|████████▌                                            | 6/37 [00:03<00:15,  2.02it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:15,  2.00it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  19%|██████████                                           | 7/37 [00:03<00:15,  2.00it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:03<00:14,  1.99it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:  22%|███████████▍                                         | 8/37 [00:04<00:14,  1.99it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:13,  2.03it/s, training_loss=0.362]\u001b[A\n",
      "Epoch 1:  24%|████████████▉                                        | 9/37 [00:04<00:13,  2.03it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:04<00:13,  2.02it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  27%|██████████████                                      | 10/37 [00:05<00:13,  2.02it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:13,  2.00it/s, training_loss=0.342]\u001b[A\n",
      "Epoch 1:  30%|███████████████▍                                    | 11/37 [00:05<00:13,  2.00it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:05<00:12,  2.02it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:  32%|████████████████▊                                   | 12/37 [00:06<00:12,  2.02it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.03it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  35%|██████████████████▎                                 | 13/37 [00:06<00:11,  2.03it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:06<00:11,  2.02it/s, training_loss=0.350]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▋                                | 14/37 [00:07<00:11,  2.02it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:10,  2.03it/s, training_loss=0.359]\u001b[A\n",
      "Epoch 1:  41%|█████████████████████                               | 15/37 [00:07<00:10,  2.03it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:07<00:10,  2.02it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  43%|██████████████████████▍                             | 16/37 [00:08<00:10,  2.02it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:10,  2.00it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 1:  46%|███████████████████████▉                            | 17/37 [00:08<00:10,  2.00it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:08<00:09,  2.02it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▎                          | 18/37 [00:09<00:09,  2.02it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:09<00:09,  1.99it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▋                         | 19/37 [00:09<00:09,  1.99it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:09<00:08,  2.02it/s, training_loss=0.390]\u001b[A\n",
      "Epoch 1:  54%|████████████████████████████                        | 20/37 [00:10<00:08,  2.02it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:10<00:07,  2.01it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  57%|█████████████████████████████▌                      | 21/37 [00:10<00:07,  2.01it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:10<00:07,  2.01it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  59%|██████████████████████████████▉                     | 22/37 [00:11<00:07,  2.01it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:11<00:06,  2.02it/s, training_loss=0.374]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▎                   | 23/37 [00:11<00:06,  2.02it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:11<00:06,  2.01it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████▋                  | 24/37 [00:12<00:06,  2.01it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:12<00:05,  2.03it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████▏                | 25/37 [00:12<00:05,  2.03it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:12<00:05,  2.04it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████▌               | 26/37 [00:13<00:05,  2.04it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:13<00:04,  2.04it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▉              | 27/37 [00:13<00:04,  2.04it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:13<00:04,  2.00it/s, training_loss=0.372]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▎            | 28/37 [00:14<00:04,  2.00it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:14<00:03,  2.04it/s, training_loss=0.353]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▊           | 29/37 [00:14<00:03,  2.04it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:14<00:03,  2.04it/s, training_loss=0.344]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████▏         | 30/37 [00:15<00:03,  2.04it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:15<00:02,  2.01it/s, training_loss=0.378]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▌        | 31/37 [00:15<00:02,  2.01it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:15<00:02,  2.01it/s, training_loss=0.367]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████████████████████▉       | 32/37 [00:16<00:02,  2.01it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:16<00:01,  2.02it/s, training_loss=0.384]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▍     | 33/37 [00:16<00:01,  2.02it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:16<00:01,  2.03it/s, training_loss=0.349]\u001b[A\n",
      "Epoch 1:  92%|███████████████████████████████████████████████▊    | 34/37 [00:17<00:01,  2.03it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:17<00:00,  2.01it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 35/37 [00:17<00:00,  2.01it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:17<00:00,  2.02it/s, training_loss=0.357]\u001b[A\n",
      "Epoch 1:  97%|██████████████████████████████████████████████████▌ | 36/37 [00:18<00:00,  2.02it/s, training_loss=0.318]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████| 37/37 [00:18<00:00,  2.14it/s, training_loss=0.318]\u001b[A\n",
      "Epoch Progress: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.15s/it]\u001b[A\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " #install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def custom_train_test_split(df, test_size=0.2, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df[['ade', 'soc_code']]\n",
    "    y = df['label']\n",
    "    \n",
    "    # Identify classes and their counts\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    # Identify small classes\n",
    "    small_classes = classes[counts < 5]\n",
    "    \n",
    "    # Initialize lists for train and test sets\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Handle small classes separately\n",
    "    for cls in small_classes:\n",
    "        cls_mask = (y == cls)\n",
    "        cls_X = X[cls_mask]\n",
    "        cls_y = y[cls_mask]\n",
    "        cls_idx = df.index[cls_mask].tolist()\n",
    "        \n",
    "        if len(cls_X) == 1:\n",
    "            # If only one instance, put it in test set\n",
    "            test_indices.append(cls_idx[0])\n",
    "        else:\n",
    "            # Randomly choose one instance for testing\n",
    "            test_idx = np.random.choice(len(cls_X))\n",
    "            test_indices.append(cls_idx[test_idx])\n",
    "            \n",
    "            # Remaining instances go to training\n",
    "            train_indices.extend(np.delete(cls_idx, test_idx))\n",
    "    \n",
    "    # Combine the small class data into test and train sets\n",
    "    test_indices = np.array(test_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "    \n",
    "    X_test = df.loc[test_indices]\n",
    "    y_test = X_test['label']\n",
    "    \n",
    "    X_train = df.loc[train_indices]\n",
    "    y_train = X_train['label']\n",
    "    \n",
    "    # Handle large classes with stratified split\n",
    "    large_class_mask = ~np.isin(y, small_classes)\n",
    "    X_large = X[large_class_mask]\n",
    "    y_large = y[large_class_mask]\n",
    "    \n",
    "    X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "        X_large, y_large, test_size=test_size, random_state=random_state, stratify=y_large\n",
    "    )\n",
    "    \n",
    "    # Combine large class data with the small class data\n",
    "    X_train = pd.concat([X_train, X_train_large], axis=0)\n",
    "    y_train = pd.concat([y_train, y_train_large], axis=0)\n",
    "    \n",
    "    X_test = pd.concat([X_test, X_test_large], axis=0)\n",
    "    y_test = pd.concat([y_test, y_test_large], axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#evaluation\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "\n",
    "    return accuracy_dict, count_dict\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "\n",
    "# Function to calculate precision, recall, and F1 for each label\n",
    "def calculate_metrics(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score per label\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_flat, pred_flat, average=None, labels=np.unique(labels_flat))\n",
    "    \n",
    "    return precision, recall, f1\n",
    "    \n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='smm4h_top3_training_40ep_16bs_5e-5lr_log.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "cadec_csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "# read data from smm4h\n",
    "smm4h_csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n",
    "\n",
    "top3SMM4H = [10037175, 10018065,10029205]\n",
    "top3label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2\n",
    "}\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"ade\", \"soc_code\"]\n",
    "smm4h_all = pd.read_csv(smm4h_csv_url,names=column_names, sep = '\\t', header=None)\n",
    "\n",
    "smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n",
    "smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n",
    "smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# print(\"smm4h data:\",smm4h_all.shape)\n",
    "smm4h_soc_code_counts = smm4h_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",smm4h_soc_code_counts)\n",
    "# Filter DataFrame\n",
    "smm4h_filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(top3SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "top3inSMM4H = smm4h_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "top3inSMM4H.loc[:, 'label'] = top3inSMM4H['soc_code'].map(top3label_dict)\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(cadec_csv_url,names=column_names, header=None)\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(\"clean cadec data:\",cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "cadec_soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",cadec_soc_code_counts)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "cadec_filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(top3SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECtop3inSMM4H = cadec_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "\n",
    "# print(\"CADEC top3 in SMM4H:\",CADECtop3inSMM4H)\n",
    "\n",
    "# For SMM4H data\n",
    "df1 = top3inSMM4H.copy()\n",
    "df1.loc[:, 'label'] = df1['soc_code'].map(top3label_dict)\n",
    "\n",
    "# For CADEC data\n",
    "df2 = CADECtop3inSMM4H.copy()\n",
    "df2.loc[:, 'label'] = df2['soc_code'].map(top3label_dict)\n",
    "\n",
    "print(\"SMM4H top 3\",df1)\n",
    "print(\"CADEC top 3\",df2)\n",
    "\n",
    "#smm4h data\n",
    "df = df1\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 42, 2))\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "learningrate = 5e-5\n",
    "\n",
    "# Placeholder for accuracies\n",
    "all_accuracies = {label: [] for label in range(len(top3label_dict))}\n",
    "\n",
    "# Initialize dictionaries to hold metrics for each seed\n",
    "# seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': []} for seed_val in seed_values}\n",
    "seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'confusion_matrix': []} for seed_val in seed_values}\n",
    "\n",
    "\n",
    "# Main loop over seed values\n",
    "for seed_val in seed_values:\n",
    "    # Set seeds\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Data preparation\n",
    "    # Apply the custom train-test split\n",
    "    X_train, X_val, y_train, y_val = custom_train_test_split(df, test_size=0.2, random_state=seed_val)\n",
    "    \n",
    "    # Add data_type column\n",
    "    df['data_type'] = 'not_set'\n",
    "    df.loc[X_train.index, 'data_type'] = 'train'\n",
    "    df.loc[X_val.index, 'data_type'] = 'val'\n",
    "\n",
    "    logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    print(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'train'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'val'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n",
    "\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top3label_dict), output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learningrate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    logger.info(f\"Device used: {device}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "\n",
    "        # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n",
    "\n",
    "        logger.info(f'\\nEpoch {epoch}')\n",
    "        loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "        logger.info(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_vals.flatten(), np.argmax(predictions, axis=1).flatten(), average=None, labels=np.unique(true_vals.flatten()))\n",
    "\n",
    " # Ensure that you use `true_vals` for the true labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1).flatten()\n",
    "    true_labels = true_vals.flatten()\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    seed_metrics[seed_val]['accuracy'] = accuracy\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=np.unique(true_labels))\n",
    "    seed_metrics[seed_val]['confusion_matrix'] = conf_matrix\n",
    "    \n",
    "    for label in np.unique(true_vals):\n",
    "        seed_metrics[seed_val]['precision'].append((label, precision[label]))\n",
    "        seed_metrics[seed_val]['recall'].append((label, recall[label]))\n",
    "        seed_metrics[seed_val]['f1'].append((label, f1[label]))\n",
    "\n",
    "# Write the precision, recall, F1 scores, and seed values to a file\n",
    "with open('smm4h_top3_20times_results_with_seeds.txt', 'w') as f:\n",
    "    f.write('Seed\\tLabel\\tPrecision\\tRecall\\tF1\\tAccuracy\\n')\n",
    "    for seed_val in seed_values:\n",
    "        for label, precision_val in seed_metrics[seed_val]['precision']:\n",
    "            recall_val = next(val for lbl, val in seed_metrics[seed_val]['recall'] if lbl == label)\n",
    "            f1_val = next(val for lbl, val in seed_metrics[seed_val]['f1'] if lbl == label)\n",
    "            accuracy = seed_metrics[seed_val]['accuracy']\n",
    "            f.write(f'{seed_val}\\t{label}\\t{precision_val:.4f}\\t{recall_val:.4f}\\t{f1_val:.4f}\\t{accuracy:.4f}\\n')\n",
    "\n",
    "        # Save the confusion matrix\n",
    "        f.write(f'\\nConfusion Matrix for Seed {seed_val}:\\n')\n",
    "        f.write(np.array2string(seed_metrics[seed_val]['confusion_matrix'], separator=', '))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "# Initialize lists to hold precision, recall, and f1 values for each label\n",
    "precision_dict, recall_dict, f1_dict = {}, {}, {}\n",
    "\n",
    "# Collect metrics across seeds\n",
    "for seed in seed_metrics:\n",
    "    for label, value in seed_metrics[seed]['precision']:\n",
    "        precision_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['recall']:\n",
    "        recall_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['f1']:\n",
    "        f1_dict.setdefault(label, []).append(value)\n",
    "\n",
    "# Compute mean and std for precision, recall, and f1\n",
    "labels = sorted(precision_dict.keys())\n",
    "precision_mean = [np.mean(precision_dict[label]) for label in labels]\n",
    "precision_std = [np.std(precision_dict[label]) for label in labels]\n",
    "recall_mean = [np.mean(recall_dict[label]) for label in labels]\n",
    "recall_std = [np.std(recall_dict[label]) for label in labels]\n",
    "f1_mean = [np.mean(f1_dict[label]) for label in labels]\n",
    "f1_std = [np.std(f1_dict[label]) for label in labels]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(labels))  # label indices\n",
    "width = 0.25  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Bar plots with mean values\n",
    "bars_precision = ax.bar(x - width, precision_mean, width, label='Precision', color='b')\n",
    "bars_recall = ax.bar(x, recall_mean, width, label='Recall', color='g')\n",
    "bars_f1 = ax.bar(x + width, f1_mean, width, label='F1 Score', color='r')\n",
    "\n",
    "# Annotate bars with mean and std values\n",
    "# Annotate bars with mean and std values, with smaller font size\n",
    "for bars, means, stds in zip([bars_precision, bars_recall, bars_f1],\n",
    "                             [precision_mean, recall_mean, f1_mean],\n",
    "                             [precision_std, recall_std, f1_std]):\n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "                f'{mean:.2f}\\n±{std:.2f}', ha='center', va='bottom', fontsize=8)  # Smaller font size\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_title('Mean and Standard Deviation of Precision, Recall, and F1 Score by Label')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Set y-axis limit to [0, 1]\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Move legend outside the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the plot to fit the legend\n",
    "plt.savefig('SMM4H_top3_20times_results_plot.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b9c21-8b3f-439b-be51-c902a4f9563e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
