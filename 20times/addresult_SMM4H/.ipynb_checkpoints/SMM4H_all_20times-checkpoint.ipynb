{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99d9b9-8b0c-44f2-ab61-36ee6fc0ef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "smm4h data: (1712, 2)\n",
      "smm4h data after filtering: (1710, 2)\n",
      "SOC count in SMM4H:  soc_code\n",
      "10037175    287\n",
      "10018065    235\n",
      "10029205    212\n",
      "10017947     63\n",
      "10028395     58\n",
      "10022891     54\n",
      "10027433     48\n",
      "10040785     28\n",
      "10038738     22\n",
      "10022117     16\n",
      "10015919     16\n",
      "10038604     10\n",
      "10047065     10\n",
      "10021428      8\n",
      "10041244      7\n",
      "10007541      7\n",
      "10038359      6\n",
      "10021881      5\n",
      "10013993      4\n",
      "10019805      2\n",
      "10042613      2\n",
      "10029104      2\n",
      "10077536      1\n",
      "10010331      1\n",
      "10014698      1\n",
      "Name: count, dtype: Int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC count in CADEC:  soc_code\n",
      "10028395    962\n",
      "10018065    654\n",
      "10037175    401\n",
      "10017947    300\n",
      "10029205    286\n",
      "10040785    184\n",
      "10007541     92\n",
      "10038738     91\n",
      "10022891     82\n",
      "10015919     67\n",
      "10038604     59\n",
      "10038359     50\n",
      "10022117     35\n",
      "10047065     25\n",
      "10013993     16\n",
      "10019805     15\n",
      "10041244      7\n",
      "10027433      6\n",
      "10021881      5\n",
      "10021428      4\n",
      "10014698      3\n",
      "10005329      3\n",
      "10029104      1\n",
      "Name: count, dtype: int64\n",
      "SMM4H :                             ade  soc_code  label\n",
      "1                     allergies  10021428     13\n",
      "2               HURT YOUR Liver  10019805     19\n",
      "3                            AD  10037175      0\n",
      "4                         focus  10029205      2\n",
      "5                          died  10018065      1\n",
      "...                         ...       ...    ...\n",
      "1703                 chest hurt  10018065      1\n",
      "1704   got ten minutes of sleep  10037175      0\n",
      "1706                  Nosebleed  10038738      8\n",
      "1708  never have another orgasm  10037175      0\n",
      "1710        gain so much weight  10022891      5\n",
      "\n",
      "[1105 rows x 3 columns]\n",
      "CADEC :                                    ade  soc_code  label\n",
      "3                      ankles swelling  10007541     15\n",
      "4             sever swelling of ankles  10007541     15\n",
      "5                      Edema of ankles  10007541     15\n",
      "6                    severe arrythmias  10007541     15\n",
      "7                            arrythmia  10007541     15\n",
      "...                                ...       ...    ...\n",
      "5955                      hypertension  10047065     12\n",
      "5956           Elevated blood pressure  10047065     12\n",
      "5959  LITTLE CIRCULATION IN MY FINGERS  10047065     12\n",
      "5960                  going into shock  10047065     12\n",
      "5961     vein in my one leg is bulging  10047065     12\n",
      "\n",
      "[3345 rows x 3 columns]\n",
      "                          ade\n",
      "soc_code label data_type     \n",
      "10007541 15    train        6\n",
      "               val          1\n",
      "10010331 23    val          1\n",
      "10013993 18    train        3\n",
      "               val          1\n",
      "10014698 24    val          1\n",
      "10015919 10    train       13\n",
      "               val          3\n",
      "10017947 3     train       50\n",
      "               val         13\n",
      "10018065 1     train      188\n",
      "               val         47\n",
      "10019805 19    train        1\n",
      "               val          1\n",
      "10021428 13    train        6\n",
      "               val          2\n",
      "10021881 17    train        4\n",
      "               val          1\n",
      "10022117 9     train       13\n",
      "               val          3\n",
      "10022891 5     train       43\n",
      "               val         11\n",
      "10027433 6     train       38\n",
      "               val         10\n",
      "10028395 4     train       46\n",
      "               val         12\n",
      "10029104 21    train        1\n",
      "               val          1\n",
      "10029205 2     train      170\n",
      "               val         42\n",
      "10037175 0     train      229\n",
      "               val         58\n",
      "10038359 16    train        5\n",
      "               val          1\n",
      "10038604 11    train        8\n",
      "               val          2\n",
      "10038738 8     train       18\n",
      "               val          4\n",
      "10040785 7     train       22\n",
      "               val          6\n",
      "10041244 14    train        6\n",
      "               val          1\n",
      "10042613 20    train        1\n",
      "               val          1\n",
      "10047065 12    train        8\n",
      "               val          2\n",
      "10077536 22    val          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                                                  | 0/55 [00:00<?, ?it/s]\u001b[AC:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "Epoch 1:   0%|                                                             | 0/55 [00:00<?, ?it/s, training_loss=1.095]\u001b[A\n",
      "Epoch 1:   2%|▉                                                    | 1/55 [00:00<00:53,  1.01it/s, training_loss=1.095]\u001b[A\n",
      "Epoch 1:   2%|▉                                                    | 1/55 [00:01<00:53,  1.01it/s, training_loss=1.060]\u001b[A\n",
      "Epoch 1:   4%|█▉                                                   | 2/55 [00:01<00:37,  1.41it/s, training_loss=1.060]\u001b[A\n",
      "Epoch 1:   4%|█▉                                                   | 2/55 [00:01<00:37,  1.41it/s, training_loss=1.069]\u001b[A\n",
      "Epoch 1:   5%|██▉                                                  | 3/55 [00:01<00:31,  1.64it/s, training_loss=1.069]\u001b[A\n",
      "Epoch 1:   5%|██▉                                                  | 3/55 [00:02<00:31,  1.64it/s, training_loss=1.070]\u001b[A\n",
      "Epoch 1:   7%|███▊                                                 | 4/55 [00:02<00:28,  1.78it/s, training_loss=1.070]\u001b[A\n",
      "Epoch 1:   7%|███▊                                                 | 4/55 [00:02<00:28,  1.78it/s, training_loss=1.009]\u001b[A\n",
      "Epoch 1:   9%|████▊                                                | 5/55 [00:02<00:26,  1.88it/s, training_loss=1.009]\u001b[A\n",
      "Epoch 1:   9%|████▊                                                | 5/55 [00:03<00:26,  1.88it/s, training_loss=0.977]\u001b[A\n",
      "Epoch 1:  11%|█████▊                                               | 6/55 [00:03<00:25,  1.90it/s, training_loss=0.977]\u001b[A\n",
      "Epoch 1:  11%|█████▊                                               | 6/55 [00:03<00:25,  1.90it/s, training_loss=1.012]\u001b[A\n",
      "Epoch 1:  13%|██████▋                                              | 7/55 [00:03<00:24,  1.93it/s, training_loss=1.012]\u001b[A\n",
      "Epoch 1:  13%|██████▋                                              | 7/55 [00:04<00:24,  1.93it/s, training_loss=0.984]\u001b[A\n",
      "Epoch 1:  15%|███████▋                                             | 8/55 [00:04<00:23,  1.97it/s, training_loss=0.984]\u001b[A\n",
      "Epoch 1:  15%|███████▋                                             | 8/55 [00:04<00:23,  1.97it/s, training_loss=0.992]\u001b[A\n",
      "Epoch 1:  16%|████████▋                                            | 9/55 [00:04<00:20,  2.20it/s, training_loss=0.992]\u001b[A\n",
      "Epoch 1:  16%|████████▋                                            | 9/55 [00:05<00:20,  2.20it/s, training_loss=0.918]\u001b[A\n",
      "Epoch 1:  18%|█████████▍                                          | 10/55 [00:05<00:18,  2.39it/s, training_loss=0.918]\u001b[A\n",
      "Epoch 1:  18%|█████████▍                                          | 10/55 [00:05<00:18,  2.39it/s, training_loss=0.909]\u001b[A\n",
      "Epoch 1:  20%|██████████▍                                         | 11/55 [00:05<00:17,  2.56it/s, training_loss=0.909]\u001b[A\n",
      "Epoch 1:  20%|██████████▍                                         | 11/55 [00:05<00:17,  2.56it/s, training_loss=0.942]\u001b[A\n",
      "Epoch 1:  22%|███████████▎                                        | 12/55 [00:05<00:16,  2.61it/s, training_loss=0.942]\u001b[A\n",
      "Epoch 1:  22%|███████████▎                                        | 12/55 [00:06<00:16,  2.61it/s, training_loss=0.872]\u001b[A\n",
      "Epoch 1:  24%|████████████▎                                       | 13/55 [00:06<00:15,  2.71it/s, training_loss=0.872]\u001b[A\n",
      "Epoch 1:  24%|████████████▎                                       | 13/55 [00:06<00:15,  2.71it/s, training_loss=0.899]\u001b[A\n",
      "Epoch 1:  25%|█████████████▏                                      | 14/55 [00:06<00:14,  2.78it/s, training_loss=0.899]\u001b[A\n",
      "Epoch 1:  25%|█████████████▏                                      | 14/55 [00:07<00:14,  2.78it/s, training_loss=0.842]\u001b[A\n",
      "Epoch 1:  27%|██████████████▏                                     | 15/55 [00:07<00:16,  2.45it/s, training_loss=0.842]\u001b[A\n",
      "Epoch 1:  27%|██████████████▏                                     | 15/55 [00:07<00:16,  2.45it/s, training_loss=0.868]\u001b[A\n",
      "Epoch 1:  29%|███████████████▏                                    | 16/55 [00:07<00:16,  2.33it/s, training_loss=0.868]\u001b[A\n",
      "Epoch 1:  29%|███████████████▏                                    | 16/55 [00:08<00:16,  2.33it/s, training_loss=0.870]\u001b[A\n",
      "Epoch 1:  31%|████████████████                                    | 17/55 [00:08<00:17,  2.21it/s, training_loss=0.870]\u001b[A\n",
      "Epoch 1:  31%|████████████████                                    | 17/55 [00:08<00:17,  2.21it/s, training_loss=0.880]\u001b[A\n",
      "Epoch 1:  33%|█████████████████                                   | 18/55 [00:08<00:16,  2.20it/s, training_loss=0.880]\u001b[A\n",
      "Epoch 1:  33%|█████████████████                                   | 18/55 [00:08<00:16,  2.20it/s, training_loss=0.756]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▉                                  | 19/55 [00:08<00:16,  2.19it/s, training_loss=0.756]\u001b[A\n",
      "Epoch 1:  35%|█████████████████▉                                  | 19/55 [00:09<00:16,  2.19it/s, training_loss=0.834]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▉                                 | 20/55 [00:09<00:16,  2.15it/s, training_loss=0.834]\u001b[A\n",
      "Epoch 1:  36%|██████████████████▉                                 | 20/55 [00:09<00:16,  2.15it/s, training_loss=0.742]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▊                                | 21/55 [00:09<00:15,  2.13it/s, training_loss=0.742]\u001b[A\n",
      "Epoch 1:  38%|███████████████████▊                                | 21/55 [00:10<00:15,  2.13it/s, training_loss=0.861]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▊                               | 22/55 [00:10<00:15,  2.11it/s, training_loss=0.861]\u001b[A\n",
      "Epoch 1:  40%|████████████████████▊                               | 22/55 [00:10<00:15,  2.11it/s, training_loss=0.839]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▋                              | 23/55 [00:10<00:15,  2.11it/s, training_loss=0.839]\u001b[A\n",
      "Epoch 1:  42%|█████████████████████▋                              | 23/55 [00:11<00:15,  2.11it/s, training_loss=0.837]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▋                             | 24/55 [00:11<00:15,  2.06it/s, training_loss=0.837]\u001b[A\n",
      "Epoch 1:  44%|██████████████████████▋                             | 24/55 [00:11<00:15,  2.06it/s, training_loss=0.783]\u001b[A\n",
      "Epoch 1:  45%|███████████████████████▋                            | 25/55 [00:11<00:14,  2.02it/s, training_loss=0.783]\u001b[A\n",
      "Epoch 1:  45%|███████████████████████▋                            | 25/55 [00:12<00:14,  2.02it/s, training_loss=0.762]\u001b[A\n",
      "Epoch 1:  47%|████████████████████████▌                           | 26/55 [00:12<00:13,  2.22it/s, training_loss=0.762]\u001b[A\n",
      "Epoch 1:  47%|████████████████████████▌                           | 26/55 [00:12<00:13,  2.22it/s, training_loss=0.681]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▌                          | 27/55 [00:12<00:11,  2.48it/s, training_loss=0.681]\u001b[A\n",
      "Epoch 1:  49%|█████████████████████████▌                          | 27/55 [00:12<00:11,  2.48it/s, training_loss=0.742]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▍                         | 28/55 [00:12<00:10,  2.60it/s, training_loss=0.742]\u001b[A\n",
      "Epoch 1:  51%|██████████████████████████▍                         | 28/55 [00:13<00:10,  2.60it/s, training_loss=0.869]\u001b[A\n",
      "Epoch 1:  53%|███████████████████████████▍                        | 29/55 [00:13<00:09,  2.71it/s, training_loss=0.869]\u001b[A\n",
      "Epoch 1:  53%|███████████████████████████▍                        | 29/55 [00:13<00:09,  2.71it/s, training_loss=0.805]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████▎                       | 30/55 [00:13<00:09,  2.77it/s, training_loss=0.805]\u001b[A\n",
      "Epoch 1:  55%|████████████████████████████▎                       | 30/55 [00:13<00:09,  2.77it/s, training_loss=0.693]\u001b[A\n",
      "Epoch 1:  56%|█████████████████████████████▎                      | 31/55 [00:13<00:08,  2.87it/s, training_loss=0.693]\u001b[A\n",
      "Epoch 1:  56%|█████████████████████████████▎                      | 31/55 [00:14<00:08,  2.87it/s, training_loss=0.750]\u001b[A\n",
      "Epoch 1:  58%|██████████████████████████████▎                     | 32/55 [00:14<00:07,  2.93it/s, training_loss=0.750]\u001b[A\n",
      "Epoch 1:  58%|██████████████████████████████▎                     | 32/55 [00:14<00:07,  2.93it/s, training_loss=0.734]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████▏                    | 33/55 [00:14<00:07,  2.96it/s, training_loss=0.734]\u001b[A\n",
      "Epoch 1:  60%|███████████████████████████████▏                    | 33/55 [00:14<00:07,  2.96it/s, training_loss=0.813]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▏                   | 34/55 [00:14<00:07,  2.95it/s, training_loss=0.813]\u001b[A\n",
      "Epoch 1:  62%|████████████████████████████████▏                   | 34/55 [00:15<00:07,  2.95it/s, training_loss=0.704]\u001b[A\n",
      "Epoch 1:  64%|█████████████████████████████████                   | 35/55 [00:15<00:06,  2.97it/s, training_loss=0.704]\u001b[A\n",
      "Epoch 1:  64%|█████████████████████████████████                   | 35/55 [00:15<00:06,  2.97it/s, training_loss=0.829]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████                  | 36/55 [00:15<00:06,  2.99it/s, training_loss=0.829]\u001b[A\n",
      "Epoch 1:  65%|██████████████████████████████████                  | 36/55 [00:15<00:06,  2.99it/s, training_loss=0.784]\u001b[A\n",
      "Epoch 1:  67%|██████████████████████████████████▉                 | 37/55 [00:15<00:06,  2.99it/s, training_loss=0.784]\u001b[A\n",
      "Epoch 1:  67%|██████████████████████████████████▉                 | 37/55 [00:16<00:06,  2.99it/s, training_loss=0.741]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████▉                | 38/55 [00:16<00:05,  3.00it/s, training_loss=0.741]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████▉                | 38/55 [00:16<00:05,  3.00it/s, training_loss=0.782]\u001b[A\n",
      "Epoch 1:  71%|████████████████████████████████████▊               | 39/55 [00:16<00:05,  3.01it/s, training_loss=0.782]\u001b[A\n",
      "Epoch 1:  71%|████████████████████████████████████▊               | 39/55 [00:16<00:05,  3.01it/s, training_loss=0.720]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▊              | 40/55 [00:16<00:04,  3.02it/s, training_loss=0.720]\u001b[A\n",
      "Epoch 1:  73%|█████████████████████████████████████▊              | 40/55 [00:17<00:04,  3.02it/s, training_loss=0.723]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████▊             | 41/55 [00:17<00:04,  3.03it/s, training_loss=0.723]\u001b[A\n",
      "Epoch 1:  75%|██████████████████████████████████████▊             | 41/55 [00:17<00:04,  3.03it/s, training_loss=0.709]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▋            | 42/55 [00:17<00:04,  2.98it/s, training_loss=0.709]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████▋            | 42/55 [00:17<00:04,  2.98it/s, training_loss=0.746]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▋           | 43/55 [00:17<00:04,  2.99it/s, training_loss=0.746]\u001b[A\n",
      "Epoch 1:  78%|████████████████████████████████████████▋           | 43/55 [00:18<00:04,  2.99it/s, training_loss=0.700]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████▌          | 44/55 [00:18<00:03,  3.02it/s, training_loss=0.700]\u001b[A\n",
      "Epoch 1:  80%|█████████████████████████████████████████▌          | 44/55 [00:18<00:03,  3.02it/s, training_loss=0.715]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████▌         | 45/55 [00:18<00:03,  2.98it/s, training_loss=0.715]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████████████████████▌         | 45/55 [00:18<00:03,  2.98it/s, training_loss=0.673]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▍        | 46/55 [00:18<00:03,  2.99it/s, training_loss=0.673]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████████████████████▍        | 46/55 [00:19<00:03,  2.99it/s, training_loss=0.630]\u001b[A\n",
      "Epoch 1:  85%|████████████████████████████████████████████▍       | 47/55 [00:19<00:02,  3.02it/s, training_loss=0.630]\u001b[A\n",
      "Epoch 1:  85%|████████████████████████████████████████████▍       | 47/55 [00:19<00:02,  3.02it/s, training_loss=0.758]\u001b[A\n",
      "Epoch 1:  87%|█████████████████████████████████████████████▍      | 48/55 [00:19<00:02,  3.00it/s, training_loss=0.758]\u001b[A\n",
      "Epoch 1:  87%|█████████████████████████████████████████████▍      | 48/55 [00:19<00:02,  3.00it/s, training_loss=0.642]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▎     | 49/55 [00:19<00:01,  3.02it/s, training_loss=0.642]\u001b[A\n",
      "Epoch 1:  89%|██████████████████████████████████████████████▎     | 49/55 [00:20<00:01,  3.02it/s, training_loss=0.776]\u001b[A\n",
      "Epoch 1:  91%|███████████████████████████████████████████████▎    | 50/55 [00:20<00:01,  3.05it/s, training_loss=0.776]\u001b[A\n",
      "Epoch 1:  91%|███████████████████████████████████████████████▎    | 50/55 [00:20<00:01,  3.05it/s, training_loss=0.688]\u001b[A\n",
      "Epoch 1:  93%|████████████████████████████████████████████████▏   | 51/55 [00:20<00:01,  3.01it/s, training_loss=0.688]\u001b[A\n",
      "Epoch 1:  93%|████████████████████████████████████████████████▏   | 51/55 [00:20<00:01,  3.01it/s, training_loss=0.796]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 52/55 [00:20<00:00,  3.03it/s, training_loss=0.796]\u001b[A\n",
      "Epoch 1:  95%|█████████████████████████████████████████████████▏  | 52/55 [00:21<00:00,  3.03it/s, training_loss=0.800]\u001b[A\n",
      "Epoch 1:  96%|██████████████████████████████████████████████████  | 53/55 [00:21<00:00,  3.02it/s, training_loss=0.800]\u001b[A"
     ]
    }
   ],
   "source": [
    " #install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def custom_train_test_split(df, test_size=0.2, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df[['ade', 'soc_code']]\n",
    "    y = df['label']\n",
    "    \n",
    "    # Identify classes and their counts\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    # Identify small classes\n",
    "    small_classes = classes[counts < 5]\n",
    "    \n",
    "    # Initialize lists for train and test sets\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Handle small classes separately\n",
    "    for cls in small_classes:\n",
    "        cls_mask = (y == cls)\n",
    "        cls_X = X[cls_mask]\n",
    "        cls_y = y[cls_mask]\n",
    "        cls_idx = df.index[cls_mask].tolist()\n",
    "        \n",
    "        if len(cls_X) == 1:\n",
    "            # If only one instance, put it in test set\n",
    "            test_indices.append(cls_idx[0])\n",
    "        else:\n",
    "            # Randomly choose one instance for testing\n",
    "            test_idx = np.random.choice(len(cls_X))\n",
    "            test_indices.append(cls_idx[test_idx])\n",
    "            \n",
    "            # Remaining instances go to training\n",
    "            train_indices.extend(np.delete(cls_idx, test_idx))\n",
    "    \n",
    "    # Combine the small class data into test and train sets\n",
    "    test_indices = np.array(test_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "    \n",
    "    X_test = df.loc[test_indices]\n",
    "    y_test = X_test['label']\n",
    "    \n",
    "    X_train = df.loc[train_indices]\n",
    "    y_train = X_train['label']\n",
    "    \n",
    "    # Handle large classes with stratified split\n",
    "    large_class_mask = ~np.isin(y, small_classes)\n",
    "    X_large = X[large_class_mask]\n",
    "    y_large = y[large_class_mask]\n",
    "    \n",
    "    X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "        X_large, y_large, test_size=test_size, random_state=random_state, stratify=y_large\n",
    "    )\n",
    "    \n",
    "    # Combine large class data with the small class data\n",
    "    X_train = pd.concat([X_train, X_train_large], axis=0)\n",
    "    y_train = pd.concat([y_train, y_train_large], axis=0)\n",
    "    \n",
    "    X_test = pd.concat([X_test, X_test_large], axis=0)\n",
    "    y_test = pd.concat([y_test, y_test_large], axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#evaluation\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "\n",
    "    return accuracy_dict, count_dict\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "\n",
    "# Function to calculate precision, recall, and F1 for each label\n",
    "def calculate_metrics(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score per label\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_flat, pred_flat, average=None, labels=np.unique(labels_flat))\n",
    "    \n",
    "    return precision, recall, f1\n",
    "    \n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='smm4h_all_training_40ep_16bs_5e-5lr_log_fix.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "cadec_csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "# read data from smm4h\n",
    "smm4h_csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n",
    "\n",
    "allSMM4H = [10037175, 10018065,10029205, 10017947, 10028395, 10022891, 10027433, 10040785, 10038738, 10022117, 10015919, 10038604, 10047065, \n",
    "            10021428,10041244, 10007541, 10038359, 10021881, 10013993, 10019805, 10042613, 10029104, 10077536, 10010331, 10014698]\n",
    "\n",
    "label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2,\n",
    "    10017947: 3,\n",
    "    10028395: 4,\n",
    "    10022891: 5,\n",
    "    10027433: 6,\n",
    "    10040785: 7,\n",
    "    10038738: 8,\n",
    "    10022117: 9,\n",
    "    10015919: 10,\n",
    "    10038604: 11,\n",
    "    10047065: 12,\n",
    "    10021428: 13,\n",
    "    10041244: 14,\n",
    "    10007541: 15,\n",
    "    10038359: 16,\n",
    "    10021881: 17,\n",
    "    10013993: 18,\n",
    "    10019805: 19,\n",
    "    10042613: 20,\n",
    "    10029104: 21,\n",
    "    10077536: 22,\n",
    "    10010331: 23,\n",
    "    10014698: 24\n",
    "}\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"ade\", \"soc_code\"]\n",
    "smm4h_all = pd.read_csv(smm4h_csv_url,names=column_names, sep = '\\t', header=None)\n",
    "print(\"smm4h data:\",smm4h_all.shape)\n",
    "\n",
    "smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n",
    "smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n",
    "\n",
    "smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n",
    "\n",
    "print(\"smm4h data after filtering:\",smm4h_all.shape)\n",
    "smm4h_soc_code_counts = smm4h_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "print(\"SOC count in SMM4H: \",smm4h_soc_code_counts)\n",
    "# Filter DataFrame\n",
    "smm4h_filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(allSMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "allinSMM4H = smm4h_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(cadec_csv_url,names=column_names, header=None)\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(\"clean cadec data:\",cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "cadec_soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "print(\"SOC count in CADEC: \",cadec_soc_code_counts)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "cadec_filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(allSMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECallinSMM4H = cadec_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "\n",
    "# For SMM4H data\n",
    "df1 = allinSMM4H.copy()\n",
    "df1.loc[:, 'label'] = df1['soc_code'].map(label_dict)\n",
    "\n",
    "# For CADEC data\n",
    "df2 = CADECallinSMM4H.copy()\n",
    "df2.loc[:, 'label'] = df2['soc_code'].map(label_dict)\n",
    "\n",
    "print(\"SMM4H :\",df1)\n",
    "print(\"CADEC :\",df2)\n",
    "\n",
    "#smm4h data\n",
    "df = df1\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 42, 2))\n",
    "batch_size = 16\n",
    "epochs = 1\n",
    "learningrate = 5e-5\n",
    "\n",
    "# Placeholder for accuracies\n",
    "all_accuracies = {label: [] for label in range(len(label_dict))}\n",
    "\n",
    "# Initialize dictionaries to hold metrics for each seed\n",
    "# seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': []} for seed_val in seed_values}\n",
    "seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'confusion_matrix': []} for seed_val in seed_values}\n",
    "\n",
    "\n",
    "# Main loop over seed values\n",
    "for seed_val in seed_values:\n",
    "    # Set seeds\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Data preparation\n",
    "    # Apply the custom train-test split\n",
    "    X_train, X_val, y_train, y_val = custom_train_test_split(df, test_size=0.2, random_state=seed_val)\n",
    "    \n",
    "    # Add data_type column\n",
    "    df['data_type'] = 'not_set'\n",
    "    df.loc[X_train.index, 'data_type'] = 'train'\n",
    "    df.loc[X_val.index, 'data_type'] = 'val'\n",
    "\n",
    "    logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    print(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'train'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'val'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n",
    "\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict), output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learningrate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    logger.info(f\"Device used: {device}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "\n",
    "        # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n",
    "\n",
    "        logger.info(f'\\nEpoch {epoch}')\n",
    "        loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "        logger.info(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_vals.flatten(), np.argmax(predictions, axis=1).flatten(), average=None, labels=np.unique(true_vals.flatten()))\n",
    "\n",
    " # Ensure that you use `true_vals` for the true labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1).flatten()\n",
    "    true_labels = true_vals.flatten()\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    seed_metrics[seed_val]['accuracy'] = accuracy\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=np.unique(true_labels))\n",
    "    seed_metrics[seed_val]['confusion_matrix'] = conf_matrix\n",
    "    \n",
    "    for label in np.unique(true_vals):\n",
    "        seed_metrics[seed_val]['precision'].append((label, precision[label]))\n",
    "        seed_metrics[seed_val]['recall'].append((label, recall[label]))\n",
    "        seed_metrics[seed_val]['f1'].append((label, f1[label]))\n",
    "\n",
    "# Write the precision, recall, F1 scores, and seed values to a file\n",
    "with open('smm4h_all_20times_results_with_seeds.txt', 'w') as f:\n",
    "    f.write('Seed\\tLabel\\tPrecision\\tRecall\\tF1\\tAccuracy\\n')\n",
    "    for seed_val in seed_values:\n",
    "        for label, precision_val in seed_metrics[seed_val]['precision']:\n",
    "            recall_val = next(val for lbl, val in seed_metrics[seed_val]['recall'] if lbl == label)\n",
    "            f1_val = next(val for lbl, val in seed_metrics[seed_val]['f1'] if lbl == label)\n",
    "            accuracy = seed_metrics[seed_val]['accuracy']\n",
    "            f.write(f'{seed_val}\\t{label}\\t{precision_val:.4f}\\t{recall_val:.4f}\\t{f1_val:.4f}\\t{accuracy:.4f}\\n')\n",
    "\n",
    "        # Save the confusion matrix\n",
    "        f.write(f'\\nConfusion Matrix for Seed {seed_val}:\\n')\n",
    "        f.write(np.array2string(seed_metrics[seed_val]['confusion_matrix'], separator=', '))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "# Initialize lists to hold precision, recall, and f1 values for each label\n",
    "precision_dict, recall_dict, f1_dict = {}, {}, {}\n",
    "\n",
    "# Collect metrics across seeds\n",
    "for seed in seed_metrics:\n",
    "    for label, value in seed_metrics[seed]['precision']:\n",
    "        precision_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['recall']:\n",
    "        recall_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['f1']:\n",
    "        f1_dict.setdefault(label, []).append(value)\n",
    "\n",
    "# Compute mean and std for precision, recall, and f1\n",
    "labels = sorted(precision_dict.keys())\n",
    "precision_mean = [np.mean(precision_dict[label]) for label in labels]\n",
    "precision_std = [np.std(precision_dict[label]) for label in labels]\n",
    "recall_mean = [np.mean(recall_dict[label]) for label in labels]\n",
    "recall_std = [np.std(recall_dict[label]) for label in labels]\n",
    "f1_mean = [np.mean(f1_dict[label]) for label in labels]\n",
    "f1_std = [np.std(f1_dict[label]) for label in labels]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(labels))  # label indices\n",
    "width = 0.25  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Bar plots with mean values\n",
    "bars_precision = ax.bar(x - width, precision_mean, width, label='Precision', color='b')\n",
    "bars_recall = ax.bar(x, recall_mean, width, label='Recall', color='g')\n",
    "bars_f1 = ax.bar(x + width, f1_mean, width, label='F1 Score', color='r')\n",
    "\n",
    "# # Annotate bars with mean and std values\n",
    "# Annotate bars with mean and std values, with smaller font size\n",
    "# for bars, means, stds in zip([bars_precision, bars_recall, bars_f1],\n",
    "#                              [precision_mean, recall_mean, f1_mean],\n",
    "#                              [precision_std, recall_std, f1_std]):\n",
    "#     for bar, mean, std in zip(bars, means, stds):\n",
    "#         height = bar.get_height()\n",
    "#         ax.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "#                 f'{mean:.2f}\\n±{std:.2f}', ha='center', va='bottom', fontsize=8)  # Smaller font size\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_title('Mean and Standard Deviation of Precision, Recall, and F1 Score by Label')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Set y-axis limit to [0, 1]\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Move legend outside the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the plot to fit the legend\n",
    "plt.savefig('SMM4H_all_20times_results_plot_fix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5aa6d-713d-41a0-ab20-dda355b1f11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
