{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c2b8b-caed-4d77-8143-799791ce8a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fd\\anaconda3\\envs\\torch\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "SMM4H top 3                             ade  soc_code  label\n",
      "3                            AD  10037175      0\n",
      "4                         focus  10029205      2\n",
      "5                          died  10018065      1\n",
      "8                        dreams  10037175      0\n",
      "10                   withdrawal  10018065      1\n",
      "...                         ...       ...    ...\n",
      "1695       talk a mile a minute  10037175      0\n",
      "1698     can't go back to sleep  10037175      0\n",
      "1703                 chest hurt  10018065      1\n",
      "1704   got ten minutes of sleep  10037175      0\n",
      "1708  never have another orgasm  10037175      0\n",
      "\n",
      "[734 rows x 3 columns]\n",
      "CADEC top 3                             ade  soc_code  label\n",
      "926            voracious hunger  10018065      1\n",
      "927            loss of appetite  10018065      1\n",
      "929            lack of appetite  10018065      1\n",
      "931                    anorexia  10018065      1\n",
      "932                    anorexic  10018065      1\n",
      "...                         ...       ...    ...\n",
      "5326  short term memory lacking  10037175      0\n",
      "5328      couldn't eat or drink  10037175      0\n",
      "5329              Could not eat  10037175      0\n",
      "5331           can't eat normal  10037175      0\n",
      "5332   Disturbed sleep patterns  10037175      0\n",
      "\n",
      "[1341 rows x 3 columns]\n",
      "df:                              ade  soc_code  label data_type\n",
      "3                            AD  10037175      0     train\n",
      "4                         focus  10029205      2     train\n",
      "5                          died  10018065      1     train\n",
      "8                        dreams  10037175      0     train\n",
      "10                   withdrawal  10018065      1     train\n",
      "...                         ...       ...    ...       ...\n",
      "5326  short term memory lacking  10037175      0      val2\n",
      "5328      couldn't eat or drink  10037175      0     train\n",
      "5329              Could not eat  10037175      0     train\n",
      "5331           can't eat normal  10037175      0     train\n",
      "5332   Disturbed sleep patterns  10037175      0      val2\n",
      "\n",
      "[2075 rows x 4 columns]\n",
      "                          ade\n",
      "soc_code label data_type     \n",
      "10018065 1     train      711\n",
      "               val         47\n",
      "               val2       131\n",
      "10029205 2     train      399\n",
      "               val         42\n",
      "               val2        57\n",
      "10037175 0     train      549\n",
      "               val         58\n",
      "               val2        81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch Progress:   0%|                                                         | 0/40 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|                                                               | 0/104 [00:00<?, ?it/s]\u001b[AC:\\Users\\fd\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "Epoch 1:   0%|                                          | 0/104 [00:00<?, ?it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:   1%|▎                                 | 1/104 [00:00<00:50,  2.04it/s, training_loss=0.385]\u001b[A\n",
      "Epoch 1:   1%|▎                                 | 1/104 [00:00<00:50,  2.04it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   2%|▋                                 | 2/104 [00:00<00:29,  3.41it/s, training_loss=0.381]\u001b[A\n",
      "Epoch 1:   2%|▋                                 | 2/104 [00:00<00:29,  3.41it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:   3%|▉                                 | 3/104 [00:00<00:24,  4.19it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:   3%|▉                                 | 3/104 [00:00<00:24,  4.19it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:   4%|█▎                                | 4/104 [00:00<00:21,  4.75it/s, training_loss=0.386]\u001b[A\n",
      "Epoch 1:   4%|█▎                                | 4/104 [00:01<00:21,  4.75it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:   5%|█▋                                | 5/104 [00:01<00:19,  5.12it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:   5%|█▋                                | 5/104 [00:01<00:19,  5.12it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:   6%|█▉                                | 6/104 [00:01<00:17,  5.46it/s, training_loss=0.335]\u001b[A\n",
      "Epoch 1:   6%|█▉                                | 6/104 [00:01<00:17,  5.46it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:   7%|██▎                               | 7/104 [00:01<00:17,  5.57it/s, training_loss=0.343]\u001b[A\n",
      "Epoch 1:   7%|██▎                               | 7/104 [00:01<00:17,  5.57it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:   8%|██▌                               | 8/104 [00:01<00:16,  5.83it/s, training_loss=0.407]\u001b[A\n",
      "Epoch 1:   8%|██▌                               | 8/104 [00:01<00:16,  5.83it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:   9%|██▉                               | 9/104 [00:01<00:16,  5.82it/s, training_loss=0.361]\u001b[A\n",
      "Epoch 1:   9%|██▉                               | 9/104 [00:01<00:16,  5.82it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  10%|███▏                             | 10/104 [00:01<00:15,  5.99it/s, training_loss=0.330]\u001b[A\n",
      "Epoch 1:  10%|███▏                             | 10/104 [00:02<00:15,  5.99it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  11%|███▍                             | 11/104 [00:02<00:15,  5.99it/s, training_loss=0.281]\u001b[A\n",
      "Epoch 1:  11%|███▍                             | 11/104 [00:02<00:15,  5.99it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  12%|███▊                             | 12/104 [00:02<00:15,  5.88it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  12%|███▊                             | 12/104 [00:02<00:15,  5.88it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  12%|████▏                            | 13/104 [00:02<00:14,  6.09it/s, training_loss=0.356]\u001b[A\n",
      "Epoch 1:  12%|████▏                            | 13/104 [00:02<00:14,  6.09it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  13%|████▍                            | 14/104 [00:02<00:14,  6.01it/s, training_loss=0.376]\u001b[A\n",
      "Epoch 1:  13%|████▍                            | 14/104 [00:02<00:14,  6.01it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 1:  14%|████▊                            | 15/104 [00:02<00:14,  6.12it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 1:  14%|████▊                            | 15/104 [00:02<00:14,  6.12it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  15%|█████                            | 16/104 [00:02<00:14,  5.94it/s, training_loss=0.364]\u001b[A\n",
      "Epoch 1:  15%|█████                            | 16/104 [00:03<00:14,  5.94it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  16%|█████▍                           | 17/104 [00:03<00:14,  5.95it/s, training_loss=0.354]\u001b[A\n",
      "Epoch 1:  16%|█████▍                           | 17/104 [00:03<00:14,  5.95it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  17%|█████▋                           | 18/104 [00:03<00:14,  6.08it/s, training_loss=0.314]\u001b[A\n",
      "Epoch 1:  17%|█████▋                           | 18/104 [00:03<00:14,  6.08it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  18%|██████                           | 19/104 [00:03<00:13,  6.11it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  18%|██████                           | 19/104 [00:03<00:13,  6.11it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  19%|██████▎                          | 20/104 [00:03<00:14,  5.96it/s, training_loss=0.348]\u001b[A\n",
      "Epoch 1:  19%|██████▎                          | 20/104 [00:03<00:14,  5.96it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  20%|██████▋                          | 21/104 [00:03<00:13,  5.97it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  20%|██████▋                          | 21/104 [00:03<00:13,  5.97it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  21%|██████▉                          | 22/104 [00:03<00:13,  6.16it/s, training_loss=0.380]\u001b[A\n",
      "Epoch 1:  21%|██████▉                          | 22/104 [00:04<00:13,  6.16it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  22%|███████▎                         | 23/104 [00:04<00:13,  6.07it/s, training_loss=0.363]\u001b[A\n",
      "Epoch 1:  22%|███████▎                         | 23/104 [00:04<00:13,  6.07it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  23%|███████▌                         | 24/104 [00:04<00:13,  6.08it/s, training_loss=0.337]\u001b[A\n",
      "Epoch 1:  23%|███████▌                         | 24/104 [00:04<00:13,  6.08it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  24%|███████▉                         | 25/104 [00:04<00:13,  6.07it/s, training_loss=0.334]\u001b[A\n",
      "Epoch 1:  24%|███████▉                         | 25/104 [00:04<00:13,  6.07it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  25%|████████▎                        | 26/104 [00:04<00:13,  5.99it/s, training_loss=0.326]\u001b[A\n",
      "Epoch 1:  25%|████████▎                        | 26/104 [00:04<00:13,  5.99it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  26%|████████▌                        | 27/104 [00:04<00:12,  6.11it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  26%|████████▌                        | 27/104 [00:04<00:12,  6.11it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  27%|████████▉                        | 28/104 [00:04<00:12,  5.94it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  27%|████████▉                        | 28/104 [00:05<00:12,  5.94it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  28%|█████████▏                       | 29/104 [00:05<00:12,  5.96it/s, training_loss=0.336]\u001b[A\n",
      "Epoch 1:  28%|█████████▏                       | 29/104 [00:05<00:12,  5.96it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  29%|█████████▌                       | 30/104 [00:05<00:12,  6.06it/s, training_loss=0.307]\u001b[A\n",
      "Epoch 1:  29%|█████████▌                       | 30/104 [00:05<00:12,  6.06it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  30%|█████████▊                       | 31/104 [00:05<00:11,  6.16it/s, training_loss=0.320]\u001b[A\n",
      "Epoch 1:  30%|█████████▊                       | 31/104 [00:05<00:11,  6.16it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  31%|██████████▏                      | 32/104 [00:05<00:11,  6.05it/s, training_loss=0.321]\u001b[A\n",
      "Epoch 1:  31%|██████████▏                      | 32/104 [00:05<00:11,  6.05it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  32%|██████████▍                      | 33/104 [00:05<00:11,  6.09it/s, training_loss=0.351]\u001b[A\n",
      "Epoch 1:  32%|██████████▍                      | 33/104 [00:05<00:11,  6.09it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  33%|██████████▊                      | 34/104 [00:05<00:11,  6.06it/s, training_loss=0.256]\u001b[A\n",
      "Epoch 1:  33%|██████████▊                      | 34/104 [00:06<00:11,  6.06it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  34%|███████████                      | 35/104 [00:06<00:11,  5.99it/s, training_loss=0.260]\u001b[A\n",
      "Epoch 1:  34%|███████████                      | 35/104 [00:06<00:11,  5.99it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  35%|███████████▍                     | 36/104 [00:06<00:11,  6.03it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  35%|███████████▍                     | 36/104 [00:06<00:11,  6.03it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 1:  36%|███████████▋                     | 37/104 [00:06<00:11,  5.95it/s, training_loss=0.302]\u001b[A\n",
      "Epoch 1:  36%|███████████▋                     | 37/104 [00:06<00:11,  5.95it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 1:  37%|████████████                     | 38/104 [00:06<00:10,  6.09it/s, training_loss=0.236]\u001b[A\n",
      "Epoch 1:  37%|████████████                     | 38/104 [00:06<00:10,  6.09it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 1:  38%|████████████▍                    | 39/104 [00:06<00:10,  5.98it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 1:  38%|████████████▍                    | 39/104 [00:06<00:10,  5.98it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  38%|████████████▋                    | 40/104 [00:06<00:10,  5.97it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  38%|████████████▋                    | 40/104 [00:07<00:10,  5.97it/s, training_loss=0.208]\u001b[A\n",
      "Epoch 1:  39%|█████████████                    | 41/104 [00:07<00:10,  6.00it/s, training_loss=0.208]\u001b[A\n",
      "Epoch 1:  39%|█████████████                    | 41/104 [00:07<00:10,  6.00it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 1:  40%|█████████████▎                   | 42/104 [00:07<00:10,  6.03it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 1:  40%|█████████████▎                   | 42/104 [00:07<00:10,  6.03it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  41%|█████████████▋                   | 43/104 [00:07<00:09,  6.13it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 1:  41%|█████████████▋                   | 43/104 [00:07<00:09,  6.13it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 1:  42%|█████████████▉                   | 44/104 [00:07<00:09,  6.04it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 1:  42%|█████████████▉                   | 44/104 [00:07<00:09,  6.04it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 1:  43%|██████████████▎                  | 45/104 [00:07<00:09,  6.15it/s, training_loss=0.159]\u001b[A\n",
      "Epoch 1:  43%|██████████████▎                  | 45/104 [00:07<00:09,  6.15it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 1:  44%|██████████████▌                  | 46/104 [00:07<00:09,  6.09it/s, training_loss=0.225]\u001b[A\n",
      "Epoch 1:  44%|██████████████▌                  | 46/104 [00:08<00:09,  6.09it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 1:  45%|██████████████▉                  | 47/104 [00:08<00:09,  6.04it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 1:  45%|██████████████▉                  | 47/104 [00:08<00:09,  6.04it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  46%|███████████████▏                 | 48/104 [00:08<00:09,  6.04it/s, training_loss=0.355]\u001b[A\n",
      "Epoch 1:  46%|███████████████▏                 | 48/104 [00:08<00:09,  6.04it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  47%|███████████████▌                 | 49/104 [00:08<00:09,  5.89it/s, training_loss=0.327]\u001b[A\n",
      "Epoch 1:  47%|███████████████▌                 | 49/104 [00:08<00:09,  5.89it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  48%|███████████████▊                 | 50/104 [00:08<00:09,  5.93it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  48%|███████████████▊                 | 50/104 [00:08<00:09,  5.93it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  49%|████████████████▏                | 51/104 [00:08<00:08,  5.96it/s, training_loss=0.319]\u001b[A\n",
      "Epoch 1:  49%|████████████████▏                | 51/104 [00:08<00:08,  5.96it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 1:  50%|████████████████▌                | 52/104 [00:08<00:08,  6.00it/s, training_loss=0.272]\u001b[A\n",
      "Epoch 1:  50%|████████████████▌                | 52/104 [00:09<00:08,  6.00it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 1:  51%|████████████████▊                | 53/104 [00:09<00:08,  5.98it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 1:  51%|████████████████▊                | 53/104 [00:09<00:08,  5.98it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 1:  52%|█████████████████▏               | 54/104 [00:09<00:08,  6.11it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 1:  52%|█████████████████▏               | 54/104 [00:09<00:08,  6.11it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  53%|█████████████████▍               | 55/104 [00:09<00:08,  6.02it/s, training_loss=0.370]\u001b[A\n",
      "Epoch 1:  53%|█████████████████▍               | 55/104 [00:09<00:08,  6.02it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  54%|█████████████████▊               | 56/104 [00:09<00:07,  6.04it/s, training_loss=0.324]\u001b[A\n",
      "Epoch 1:  54%|█████████████████▊               | 56/104 [00:09<00:07,  6.04it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  55%|██████████████████               | 57/104 [00:09<00:07,  6.02it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  55%|██████████████████               | 57/104 [00:09<00:07,  6.02it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  56%|██████████████████▍              | 58/104 [00:09<00:07,  6.01it/s, training_loss=0.341]\u001b[A\n",
      "Epoch 1:  56%|██████████████████▍              | 58/104 [00:10<00:07,  6.01it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 1:  57%|██████████████████▋              | 59/104 [00:10<00:07,  6.01it/s, training_loss=0.295]\u001b[A\n",
      "Epoch 1:  57%|██████████████████▋              | 59/104 [00:10<00:07,  6.01it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 1:  58%|███████████████████              | 60/104 [00:10<00:07,  6.02it/s, training_loss=0.249]\u001b[A\n",
      "Epoch 1:  58%|███████████████████              | 60/104 [00:10<00:07,  6.02it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  59%|███████████████████▎             | 61/104 [00:10<00:07,  6.00it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  59%|███████████████████▎             | 61/104 [00:10<00:07,  6.00it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  60%|███████████████████▋             | 62/104 [00:10<00:07,  5.99it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  60%|███████████████████▋             | 62/104 [00:10<00:07,  5.99it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  61%|███████████████████▉             | 63/104 [00:10<00:06,  5.99it/s, training_loss=0.325]\u001b[A\n",
      "Epoch 1:  61%|███████████████████▉             | 63/104 [00:10<00:06,  5.99it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  62%|████████████████████▎            | 64/104 [00:10<00:06,  6.04it/s, training_loss=0.308]\u001b[A\n",
      "Epoch 1:  62%|████████████████████▎            | 64/104 [00:11<00:06,  6.04it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 1:  62%|████████████████████▋            | 65/104 [00:11<00:06,  6.15it/s, training_loss=0.240]\u001b[A\n",
      "Epoch 1:  62%|████████████████████▋            | 65/104 [00:11<00:06,  6.15it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  63%|████████████████████▉            | 66/104 [00:11<00:06,  5.99it/s, training_loss=0.264]\u001b[A\n",
      "Epoch 1:  63%|████████████████████▉            | 66/104 [00:11<00:06,  5.99it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  64%|█████████████████████▎           | 67/104 [00:11<00:06,  6.09it/s, training_loss=0.274]\u001b[A\n",
      "Epoch 1:  64%|█████████████████████▎           | 67/104 [00:11<00:06,  6.09it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████▌           | 68/104 [00:11<00:05,  6.09it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████▌           | 68/104 [00:11<00:05,  6.09it/s, training_loss=0.254]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████▉           | 69/104 [00:11<00:05,  6.06it/s, training_loss=0.254]\u001b[A\n",
      "Epoch 1:  66%|█████████████████████▉           | 69/104 [00:11<00:05,  6.06it/s, training_loss=0.250]\u001b[A\n",
      "Epoch 1:  67%|██████████████████████▏          | 70/104 [00:11<00:05,  6.08it/s, training_loss=0.250]\u001b[A\n",
      "Epoch 1:  67%|██████████████████████▏          | 70/104 [00:12<00:05,  6.08it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████▌          | 71/104 [00:12<00:05,  6.01it/s, training_loss=0.290]\u001b[A\n",
      "Epoch 1:  68%|██████████████████████▌          | 71/104 [00:12<00:05,  6.01it/s, training_loss=0.166]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████▊          | 72/104 [00:12<00:05,  6.01it/s, training_loss=0.166]\u001b[A\n",
      "Epoch 1:  69%|██████████████████████▊          | 72/104 [00:12<00:05,  6.01it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████▏         | 73/104 [00:12<00:05,  6.01it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 1:  70%|███████████████████████▏         | 73/104 [00:12<00:05,  6.01it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████▍         | 74/104 [00:12<00:04,  6.19it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 1:  71%|███████████████████████▍         | 74/104 [00:12<00:04,  6.19it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████▊         | 75/104 [00:12<00:04,  5.98it/s, training_loss=0.284]\u001b[A\n",
      "Epoch 1:  72%|███████████████████████▊         | 75/104 [00:12<00:04,  5.98it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████         | 76/104 [00:12<00:04,  6.12it/s, training_loss=0.263]\u001b[A\n",
      "Epoch 1:  73%|████████████████████████         | 76/104 [00:13<00:04,  6.12it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  74%|████████████████████████▍        | 77/104 [00:13<00:04,  6.08it/s, training_loss=0.278]\u001b[A\n",
      "Epoch 1:  74%|████████████████████████▍        | 77/104 [00:13<00:04,  6.08it/s, training_loss=0.197]\u001b[A\n",
      "Epoch 1:  75%|████████████████████████▊        | 78/104 [00:13<00:04,  6.06it/s, training_loss=0.197]\u001b[A\n",
      "Epoch 1:  75%|████████████████████████▊        | 78/104 [00:13<00:04,  6.06it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  76%|█████████████████████████        | 79/104 [00:13<00:04,  6.04it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  76%|█████████████████████████        | 79/104 [00:13<00:04,  6.04it/s, training_loss=0.224]\u001b[A\n",
      "Epoch 1:  77%|█████████████████████████▍       | 80/104 [00:13<00:03,  6.04it/s, training_loss=0.224]\u001b[A\n",
      "Epoch 1:  77%|█████████████████████████▍       | 80/104 [00:13<00:03,  6.04it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  78%|█████████████████████████▋       | 81/104 [00:13<00:03,  6.04it/s, training_loss=0.368]\u001b[A\n",
      "Epoch 1:  78%|█████████████████████████▋       | 81/104 [00:13<00:03,  6.04it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  79%|██████████████████████████       | 82/104 [00:13<00:03,  6.00it/s, training_loss=0.415]\u001b[A\n",
      "Epoch 1:  79%|██████████████████████████       | 82/104 [00:14<00:03,  6.00it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 1:  80%|██████████████████████████▎      | 83/104 [00:14<00:03,  5.95it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 1:  80%|██████████████████████████▎      | 83/104 [00:14<00:03,  5.95it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████▋      | 84/104 [00:14<00:03,  6.07it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████▋      | 84/104 [00:14<00:03,  6.07it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████▉      | 85/104 [00:14<00:03,  6.06it/s, training_loss=0.298]\u001b[A\n",
      "Epoch 1:  82%|██████████████████████████▉      | 85/104 [00:14<00:03,  6.06it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████▎     | 86/104 [00:14<00:02,  6.16it/s, training_loss=0.377]\u001b[A\n",
      "Epoch 1:  83%|███████████████████████████▎     | 86/104 [00:14<00:02,  6.16it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████▌     | 87/104 [00:14<00:02,  5.99it/s, training_loss=0.230]\u001b[A\n",
      "Epoch 1:  84%|███████████████████████████▌     | 87/104 [00:14<00:02,  5.99it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 1:  85%|███████████████████████████▉     | 88/104 [00:14<00:02,  6.18it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 1:  85%|███████████████████████████▉     | 88/104 [00:15<00:02,  6.18it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████▏    | 89/104 [00:15<00:02,  6.05it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 1:  86%|████████████████████████████▏    | 89/104 [00:15<00:02,  6.05it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 1:  87%|████████████████████████████▌    | 90/104 [00:15<00:02,  6.03it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 1:  87%|████████████████████████████▌    | 90/104 [00:15<00:02,  6.03it/s, training_loss=0.218]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████▉    | 91/104 [00:15<00:02,  6.02it/s, training_loss=0.218]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████▉    | 91/104 [00:15<00:02,  6.02it/s, training_loss=0.178]\u001b[A\n",
      "Epoch 1:  88%|█████████████████████████████▏   | 92/104 [00:15<00:01,  6.13it/s, training_loss=0.178]\u001b[A\n",
      "Epoch 1:  88%|█████████████████████████████▏   | 92/104 [00:15<00:01,  6.13it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  89%|█████████████████████████████▌   | 93/104 [00:15<00:01,  6.06it/s, training_loss=0.316]\u001b[A\n",
      "Epoch 1:  89%|█████████████████████████████▌   | 93/104 [00:15<00:01,  6.06it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████▊   | 94/104 [00:15<00:01,  6.13it/s, training_loss=0.265]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████▊   | 94/104 [00:16<00:01,  6.13it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 1:  91%|██████████████████████████████▏  | 95/104 [00:16<00:01,  6.09it/s, training_loss=0.280]\u001b[A\n",
      "Epoch 1:  91%|██████████████████████████████▏  | 95/104 [00:16<00:01,  6.09it/s, training_loss=0.165]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████▍  | 96/104 [00:16<00:01,  5.88it/s, training_loss=0.165]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████▍  | 96/104 [00:16<00:01,  5.88it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████▊  | 97/104 [00:16<00:01,  5.91it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 1:  93%|██████████████████████████████▊  | 97/104 [00:16<00:01,  5.91it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████  | 98/104 [00:16<00:01,  5.94it/s, training_loss=0.229]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████  | 98/104 [00:16<00:01,  5.94it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████▍ | 99/104 [00:16<00:00,  5.78it/s, training_loss=0.247]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████▍ | 99/104 [00:16<00:00,  5.78it/s, training_loss=0.096]\u001b[A\n",
      "Epoch 1:  96%|██████████████████████████████▊ | 100/104 [00:16<00:00,  5.91it/s, training_loss=0.096]\u001b[A\n",
      "Epoch 1:  96%|██████████████████████████████▊ | 100/104 [00:17<00:00,  5.91it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 1:  97%|███████████████████████████████ | 101/104 [00:17<00:00,  5.93it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 1:  97%|███████████████████████████████ | 101/104 [00:17<00:00,  5.93it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 1:  98%|███████████████████████████████▍| 102/104 [00:17<00:00,  6.06it/s, training_loss=0.246]\u001b[A\n",
      "Epoch 1:  98%|███████████████████████████████▍| 102/104 [00:17<00:00,  6.06it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████▋| 103/104 [00:17<00:00,  5.82it/s, training_loss=0.182]\u001b[A\n",
      "Epoch 1:  99%|███████████████████████████████▋| 103/104 [00:17<00:00,  5.82it/s, training_loss=0.233]\u001b[A\n",
      "Epoch 1: 100%|████████████████████████████████| 104/104 [00:17<00:00,  6.31it/s, training_loss=0.233]\u001b[A\n",
      "Epoch Progress:   2%|█▏                                               | 1/40 [00:17<11:24, 17.56s/it]\u001b[A\n",
      "Epoch 2:   0%|                                                               | 0/104 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:   0%|                                          | 0/104 [00:00<?, ?it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 2:   1%|▎                                 | 1/104 [00:00<00:17,  5.94it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 2:   1%|▎                                 | 1/104 [00:00<00:17,  5.94it/s, training_loss=0.146]\u001b[A\n",
      "Epoch 2:   2%|▋                                 | 2/104 [00:00<00:17,  5.72it/s, training_loss=0.146]\u001b[A\n",
      "Epoch 2:   2%|▋                                 | 2/104 [00:00<00:17,  5.72it/s, training_loss=0.096]\u001b[A\n",
      "Epoch 2:   3%|▉                                 | 3/104 [00:00<00:17,  5.78it/s, training_loss=0.096]\u001b[A\n",
      "Epoch 2:   3%|▉                                 | 3/104 [00:00<00:17,  5.78it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 2:   4%|█▎                                | 4/104 [00:00<00:17,  5.87it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 2:   4%|█▎                                | 4/104 [00:00<00:17,  5.87it/s, training_loss=0.129]\u001b[A\n",
      "Epoch 2:   5%|█▋                                | 5/104 [00:00<00:16,  5.91it/s, training_loss=0.129]\u001b[A\n",
      "Epoch 2:   5%|█▋                                | 5/104 [00:01<00:16,  5.91it/s, training_loss=0.095]\u001b[A\n",
      "Epoch 2:   6%|█▉                                | 6/104 [00:01<00:16,  5.94it/s, training_loss=0.095]\u001b[A\n",
      "Epoch 2:   6%|█▉                                | 6/104 [00:01<00:16,  5.94it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 2:   7%|██▎                               | 7/104 [00:01<00:16,  6.00it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 2:   7%|██▎                               | 7/104 [00:01<00:16,  6.00it/s, training_loss=0.169]\u001b[A\n",
      "Epoch 2:   8%|██▌                               | 8/104 [00:01<00:16,  5.96it/s, training_loss=0.169]\u001b[A\n",
      "Epoch 2:   8%|██▌                               | 8/104 [00:01<00:16,  5.96it/s, training_loss=0.087]\u001b[A\n",
      "Epoch 2:   9%|██▉                               | 9/104 [00:01<00:16,  5.78it/s, training_loss=0.087]\u001b[A\n",
      "Epoch 2:   9%|██▉                               | 9/104 [00:01<00:16,  5.78it/s, training_loss=0.199]\u001b[A\n",
      "Epoch 2:  10%|███▏                             | 10/104 [00:01<00:16,  5.85it/s, training_loss=0.199]\u001b[A\n",
      "Epoch 2:  10%|███▏                             | 10/104 [00:01<00:16,  5.85it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 2:  11%|███▍                             | 11/104 [00:01<00:15,  5.90it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 2:  11%|███▍                             | 11/104 [00:02<00:15,  5.90it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 2:  12%|███▊                             | 12/104 [00:02<00:15,  5.92it/s, training_loss=0.277]\u001b[A\n",
      "Epoch 2:  12%|███▊                             | 12/104 [00:02<00:15,  5.92it/s, training_loss=0.072]\u001b[A\n",
      "Epoch 2:  12%|████▏                            | 13/104 [00:02<00:15,  5.95it/s, training_loss=0.072]\u001b[A\n",
      "Epoch 2:  12%|████▏                            | 13/104 [00:02<00:15,  5.95it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  13%|████▍                            | 14/104 [00:02<00:15,  5.96it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  13%|████▍                            | 14/104 [00:02<00:15,  5.96it/s, training_loss=0.176]\u001b[A\n",
      "Epoch 2:  14%|████▊                            | 15/104 [00:02<00:14,  5.98it/s, training_loss=0.176]\u001b[A\n",
      "Epoch 2:  14%|████▊                            | 15/104 [00:02<00:14,  5.98it/s, training_loss=0.106]\u001b[A\n",
      "Epoch 2:  15%|█████                            | 16/104 [00:02<00:14,  5.97it/s, training_loss=0.106]\u001b[A\n",
      "Epoch 2:  15%|█████                            | 16/104 [00:02<00:14,  5.97it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  16%|█████▍                           | 17/104 [00:02<00:14,  5.98it/s, training_loss=0.315]\u001b[A\n",
      "Epoch 2:  16%|█████▍                           | 17/104 [00:03<00:14,  5.98it/s, training_loss=0.131]\u001b[A\n",
      "Epoch 2:  17%|█████▋                           | 18/104 [00:03<00:14,  6.04it/s, training_loss=0.131]\u001b[A\n",
      "Epoch 2:  17%|█████▋                           | 18/104 [00:03<00:14,  6.04it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 2:  18%|██████                           | 19/104 [00:03<00:14,  5.97it/s, training_loss=0.301]\u001b[A\n",
      "Epoch 2:  18%|██████                           | 19/104 [00:03<00:14,  5.97it/s, training_loss=0.077]\u001b[A\n",
      "Epoch 2:  19%|██████▎                          | 20/104 [00:03<00:13,  6.03it/s, training_loss=0.077]\u001b[A\n",
      "Epoch 2:  19%|██████▎                          | 20/104 [00:03<00:13,  6.03it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 2:  20%|██████▋                          | 21/104 [00:03<00:13,  6.08it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 2:  20%|██████▋                          | 21/104 [00:03<00:13,  6.08it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  21%|██████▉                          | 22/104 [00:03<00:13,  6.00it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  21%|██████▉                          | 22/104 [00:03<00:13,  6.00it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  22%|███████▎                         | 23/104 [00:03<00:13,  6.11it/s, training_loss=0.245]\u001b[A\n",
      "Epoch 2:  22%|███████▎                         | 23/104 [00:04<00:13,  6.11it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  23%|███████▌                         | 24/104 [00:04<00:13,  6.08it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  23%|███████▌                         | 24/104 [00:04<00:13,  6.08it/s, training_loss=0.071]\u001b[A\n",
      "Epoch 2:  24%|███████▉                         | 25/104 [00:04<00:13,  6.05it/s, training_loss=0.071]\u001b[A\n",
      "Epoch 2:  24%|███████▉                         | 25/104 [00:04<00:13,  6.05it/s, training_loss=0.233]\u001b[A\n",
      "Epoch 2:  25%|████████▎                        | 26/104 [00:04<00:12,  6.15it/s, training_loss=0.233]\u001b[A\n",
      "Epoch 2:  25%|████████▎                        | 26/104 [00:04<00:12,  6.15it/s, training_loss=0.139]\u001b[A\n",
      "Epoch 2:  26%|████████▌                        | 27/104 [00:04<00:12,  5.99it/s, training_loss=0.139]\u001b[A\n",
      "Epoch 2:  26%|████████▌                        | 27/104 [00:04<00:12,  5.99it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 2:  27%|████████▉                        | 28/104 [00:04<00:12,  6.01it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 2:  27%|████████▉                        | 28/104 [00:04<00:12,  6.01it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  28%|█████████▏                       | 29/104 [00:04<00:12,  6.02it/s, training_loss=0.251]\u001b[A\n",
      "Epoch 2:  28%|█████████▏                       | 29/104 [00:05<00:12,  6.02it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 2:  29%|█████████▌                       | 30/104 [00:05<00:12,  5.98it/s, training_loss=0.248]\u001b[A\n",
      "Epoch 2:  29%|█████████▌                       | 30/104 [00:05<00:12,  5.98it/s, training_loss=0.193]\u001b[A\n",
      "Epoch 2:  30%|█████████▊                       | 31/104 [00:05<00:12,  6.01it/s, training_loss=0.193]\u001b[A\n",
      "Epoch 2:  30%|█████████▊                       | 31/104 [00:05<00:12,  6.01it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 2:  31%|██████████▏                      | 32/104 [00:05<00:11,  6.16it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 2:  31%|██████████▏                      | 32/104 [00:05<00:11,  6.16it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 2:  32%|██████████▍                      | 33/104 [00:05<00:11,  5.95it/s, training_loss=0.310]\u001b[A\n",
      "Epoch 2:  32%|██████████▍                      | 33/104 [00:05<00:11,  5.95it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 2:  33%|██████████▊                      | 34/104 [00:05<00:11,  5.96it/s, training_loss=0.219]\u001b[A\n",
      "Epoch 2:  33%|██████████▊                      | 34/104 [00:05<00:11,  5.96it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 2:  34%|███████████                      | 35/104 [00:05<00:11,  6.09it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 2:  34%|███████████                      | 35/104 [00:06<00:11,  6.09it/s, training_loss=0.036]\u001b[A\n",
      "Epoch 2:  35%|███████████▍                     | 36/104 [00:06<00:11,  5.99it/s, training_loss=0.036]\u001b[A\n",
      "Epoch 2:  35%|███████████▍                     | 36/104 [00:06<00:11,  5.99it/s, training_loss=0.090]\u001b[A\n",
      "Epoch 2:  36%|███████████▋                     | 37/104 [00:06<00:11,  6.01it/s, training_loss=0.090]\u001b[A\n",
      "Epoch 2:  36%|███████████▋                     | 37/104 [00:06<00:11,  6.01it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 2:  37%|████████████                     | 38/104 [00:06<00:11,  5.98it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 2:  37%|████████████                     | 38/104 [00:06<00:11,  5.98it/s, training_loss=0.155]\u001b[A\n",
      "Epoch 2:  38%|████████████▍                    | 39/104 [00:06<00:10,  6.10it/s, training_loss=0.155]\u001b[A\n",
      "Epoch 2:  38%|████████████▍                    | 39/104 [00:06<00:10,  6.10it/s, training_loss=0.223]\u001b[A\n",
      "Epoch 2:  38%|████████████▋                    | 40/104 [00:06<00:10,  6.09it/s, training_loss=0.223]\u001b[A\n",
      "Epoch 2:  38%|████████████▋                    | 40/104 [00:06<00:10,  6.09it/s, training_loss=0.076]\u001b[A\n",
      "Epoch 2:  39%|█████████████                    | 41/104 [00:06<00:10,  5.95it/s, training_loss=0.076]\u001b[A\n",
      "Epoch 2:  39%|█████████████                    | 41/104 [00:07<00:10,  5.95it/s, training_loss=0.101]\u001b[A\n",
      "Epoch 2:  40%|█████████████▎                   | 42/104 [00:07<00:10,  6.07it/s, training_loss=0.101]\u001b[A\n",
      "Epoch 2:  40%|█████████████▎                   | 42/104 [00:07<00:10,  6.07it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 2:  41%|█████████████▋                   | 43/104 [00:07<00:10,  6.04it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 2:  41%|█████████████▋                   | 43/104 [00:07<00:10,  6.04it/s, training_loss=0.116]\u001b[A\n",
      "Epoch 2:  42%|█████████████▉                   | 44/104 [00:07<00:10,  5.99it/s, training_loss=0.116]\u001b[A\n",
      "Epoch 2:  42%|█████████████▉                   | 44/104 [00:07<00:10,  5.99it/s, training_loss=0.115]\u001b[A\n",
      "Epoch 2:  43%|██████████████▎                  | 45/104 [00:07<00:09,  6.05it/s, training_loss=0.115]\u001b[A\n",
      "Epoch 2:  43%|██████████████▎                  | 45/104 [00:07<00:09,  6.05it/s, training_loss=0.194]\u001b[A\n",
      "Epoch 2:  44%|██████████████▌                  | 46/104 [00:07<00:09,  6.02it/s, training_loss=0.194]\u001b[A\n",
      "Epoch 2:  44%|██████████████▌                  | 46/104 [00:07<00:09,  6.02it/s, training_loss=0.076]\u001b[A\n",
      "Epoch 2:  45%|██████████████▉                  | 47/104 [00:07<00:09,  6.00it/s, training_loss=0.076]\u001b[A\n",
      "Epoch 2:  45%|██████████████▉                  | 47/104 [00:08<00:09,  6.00it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 2:  46%|███████████████▏                 | 48/104 [00:08<00:09,  6.00it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 2:  46%|███████████████▏                 | 48/104 [00:08<00:09,  6.00it/s, training_loss=0.057]\u001b[A\n",
      "Epoch 2:  47%|███████████████▌                 | 49/104 [00:08<00:09,  5.97it/s, training_loss=0.057]\u001b[A\n",
      "Epoch 2:  47%|███████████████▌                 | 49/104 [00:08<00:09,  5.97it/s, training_loss=0.099]\u001b[A\n",
      "Epoch 2:  48%|███████████████▊                 | 50/104 [00:08<00:09,  5.99it/s, training_loss=0.099]\u001b[A\n",
      "Epoch 2:  48%|███████████████▊                 | 50/104 [00:08<00:09,  5.99it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 2:  49%|████████████████▏                | 51/104 [00:08<00:08,  6.09it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 2:  49%|████████████████▏                | 51/104 [00:08<00:08,  6.09it/s, training_loss=0.153]\u001b[A\n",
      "Epoch 2:  50%|████████████████▌                | 52/104 [00:08<00:08,  5.98it/s, training_loss=0.153]\u001b[A\n",
      "Epoch 2:  50%|████████████████▌                | 52/104 [00:08<00:08,  5.98it/s, training_loss=0.143]\u001b[A\n",
      "Epoch 2:  51%|████████████████▊                | 53/104 [00:08<00:08,  5.99it/s, training_loss=0.143]\u001b[A\n",
      "Epoch 2:  51%|████████████████▊                | 53/104 [00:09<00:08,  5.99it/s, training_loss=0.129]\u001b[A\n",
      "Epoch 2:  52%|█████████████████▏               | 54/104 [00:09<00:08,  5.98it/s, training_loss=0.129]\u001b[A\n",
      "Epoch 2:  52%|█████████████████▏               | 54/104 [00:09<00:08,  5.98it/s, training_loss=0.089]\u001b[A\n",
      "Epoch 2:  53%|█████████████████▍               | 55/104 [00:09<00:08,  6.11it/s, training_loss=0.089]\u001b[A\n",
      "Epoch 2:  53%|█████████████████▍               | 55/104 [00:09<00:08,  6.11it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 2:  54%|█████████████████▊               | 56/104 [00:09<00:08,  5.95it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 2:  54%|█████████████████▊               | 56/104 [00:09<00:08,  5.95it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  55%|██████████████████               | 57/104 [00:09<00:07,  5.97it/s, training_loss=0.231]\u001b[A\n",
      "Epoch 2:  55%|██████████████████               | 57/104 [00:09<00:07,  5.97it/s, training_loss=0.090]\u001b[A\n",
      "Epoch 2:  56%|██████████████████▍              | 58/104 [00:09<00:07,  6.02it/s, training_loss=0.090]\u001b[A\n",
      "Epoch 2:  56%|██████████████████▍              | 58/104 [00:09<00:07,  6.02it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 2:  57%|██████████████████▋              | 59/104 [00:09<00:07,  6.14it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 2:  57%|██████████████████▋              | 59/104 [00:10<00:07,  6.14it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  58%|███████████████████              | 60/104 [00:10<00:07,  5.94it/s, training_loss=0.210]\u001b[A\n",
      "Epoch 2:  58%|███████████████████              | 60/104 [00:10<00:07,  5.94it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 2:  59%|███████████████████▎             | 61/104 [00:10<00:07,  5.99it/s, training_loss=0.305]\u001b[A\n",
      "Epoch 2:  59%|███████████████████▎             | 61/104 [00:10<00:07,  5.99it/s, training_loss=0.205]\u001b[A\n",
      "Epoch 2:  60%|███████████████████▋             | 62/104 [00:10<00:06,  6.03it/s, training_loss=0.205]\u001b[A\n",
      "Epoch 2:  60%|███████████████████▋             | 62/104 [00:10<00:06,  6.03it/s, training_loss=0.190]\u001b[A\n",
      "Epoch 2:  61%|███████████████████▉             | 63/104 [00:10<00:06,  5.95it/s, training_loss=0.190]\u001b[A\n",
      "Epoch 2:  61%|███████████████████▉             | 63/104 [00:10<00:06,  5.95it/s, training_loss=0.131]\u001b[A\n",
      "Epoch 2:  62%|████████████████████▎            | 64/104 [00:10<00:06,  5.98it/s, training_loss=0.131]\u001b[A\n",
      "Epoch 2:  62%|████████████████████▎            | 64/104 [00:10<00:06,  5.98it/s, training_loss=0.148]\u001b[A\n",
      "Epoch 2:  62%|████████████████████▋            | 65/104 [00:10<00:06,  6.12it/s, training_loss=0.148]\u001b[A\n",
      "Epoch 2:  62%|████████████████████▋            | 65/104 [00:10<00:06,  6.12it/s, training_loss=0.038]\u001b[A\n",
      "Epoch 2:  63%|████████████████████▉            | 66/104 [00:10<00:06,  6.05it/s, training_loss=0.038]\u001b[A\n",
      "Epoch 2:  63%|████████████████████▉            | 66/104 [00:11<00:06,  6.05it/s, training_loss=0.069]\u001b[A\n",
      "Epoch 2:  64%|█████████████████████▎           | 67/104 [00:11<00:06,  5.96it/s, training_loss=0.069]\u001b[A\n",
      "Epoch 2:  64%|█████████████████████▎           | 67/104 [00:11<00:06,  5.96it/s, training_loss=0.188]\u001b[A\n",
      "Epoch 2:  65%|█████████████████████▌           | 68/104 [00:11<00:05,  6.08it/s, training_loss=0.188]\u001b[A\n",
      "Epoch 2:  65%|█████████████████████▌           | 68/104 [00:11<00:05,  6.08it/s, training_loss=0.174]\u001b[A\n",
      "Epoch 2:  66%|█████████████████████▉           | 69/104 [00:11<00:05,  6.05it/s, training_loss=0.174]\u001b[A\n",
      "Epoch 2:  66%|█████████████████████▉           | 69/104 [00:11<00:05,  6.05it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 2:  67%|██████████████████████▏          | 70/104 [00:11<00:05,  6.06it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 2:  67%|██████████████████████▏          | 70/104 [00:11<00:05,  6.06it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 2:  68%|██████████████████████▌          | 71/104 [00:11<00:05,  6.01it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 2:  68%|██████████████████████▌          | 71/104 [00:12<00:05,  6.01it/s, training_loss=0.161]\u001b[A\n",
      "Epoch 2:  69%|██████████████████████▊          | 72/104 [00:12<00:05,  5.84it/s, training_loss=0.161]\u001b[A\n",
      "Epoch 2:  69%|██████████████████████▊          | 72/104 [00:12<00:05,  5.84it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 2:  70%|███████████████████████▏         | 73/104 [00:12<00:05,  5.91it/s, training_loss=0.383]\u001b[A\n",
      "Epoch 2:  70%|███████████████████████▏         | 73/104 [00:12<00:05,  5.91it/s, training_loss=0.151]\u001b[A\n",
      "Epoch 2:  71%|███████████████████████▍         | 74/104 [00:12<00:05,  5.91it/s, training_loss=0.151]\u001b[A\n",
      "Epoch 2:  71%|███████████████████████▍         | 74/104 [00:12<00:05,  5.91it/s, training_loss=0.155]\u001b[A\n",
      "Epoch 2:  72%|███████████████████████▊         | 75/104 [00:12<00:04,  5.92it/s, training_loss=0.155]\u001b[A\n",
      "Epoch 2:  72%|███████████████████████▊         | 75/104 [00:12<00:04,  5.92it/s, training_loss=0.151]\u001b[A\n",
      "Epoch 2:  73%|████████████████████████         | 76/104 [00:12<00:04,  5.94it/s, training_loss=0.151]\u001b[A\n",
      "Epoch 2:  73%|████████████████████████         | 76/104 [00:12<00:04,  5.94it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  74%|████████████████████████▍        | 77/104 [00:12<00:04,  5.96it/s, training_loss=0.329]\u001b[A\n",
      "Epoch 2:  74%|████████████████████████▍        | 77/104 [00:13<00:04,  5.96it/s, training_loss=0.116]\u001b[A\n",
      "Epoch 2:  75%|████████████████████████▊        | 78/104 [00:13<00:04,  5.79it/s, training_loss=0.116]\u001b[A\n",
      "Epoch 2:  75%|████████████████████████▊        | 78/104 [00:13<00:04,  5.79it/s, training_loss=0.072]\u001b[A\n",
      "Epoch 2:  76%|█████████████████████████        | 79/104 [00:13<00:04,  5.86it/s, training_loss=0.072]\u001b[A\n",
      "Epoch 2:  76%|█████████████████████████        | 79/104 [00:13<00:04,  5.86it/s, training_loss=0.136]\u001b[A\n",
      "Epoch 2:  77%|█████████████████████████▍       | 80/104 [00:13<00:04,  5.85it/s, training_loss=0.136]\u001b[A\n",
      "Epoch 2:  77%|█████████████████████████▍       | 80/104 [00:13<00:04,  5.85it/s, training_loss=0.167]\u001b[A\n",
      "Epoch 2:  78%|█████████████████████████▋       | 81/104 [00:13<00:03,  5.99it/s, training_loss=0.167]\u001b[A\n",
      "Epoch 2:  78%|█████████████████████████▋       | 81/104 [00:13<00:03,  5.99it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 2:  79%|██████████████████████████       | 82/104 [00:13<00:03,  5.95it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 2:  79%|██████████████████████████       | 82/104 [00:13<00:03,  5.95it/s, training_loss=0.107]\u001b[A\n",
      "Epoch 2:  80%|██████████████████████████▎      | 83/104 [00:13<00:03,  5.91it/s, training_loss=0.107]\u001b[A\n",
      "Epoch 2:  80%|██████████████████████████▎      | 83/104 [00:14<00:03,  5.91it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 2:  81%|██████████████████████████▋      | 84/104 [00:14<00:03,  5.97it/s, training_loss=0.228]\u001b[A\n",
      "Epoch 2:  81%|██████████████████████████▋      | 84/104 [00:14<00:03,  5.97it/s, training_loss=0.238]\u001b[A\n",
      "Epoch 2:  82%|██████████████████████████▉      | 85/104 [00:14<00:03,  5.98it/s, training_loss=0.238]\u001b[A\n",
      "Epoch 2:  82%|██████████████████████████▉      | 85/104 [00:14<00:03,  5.98it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  83%|███████████████████████████▎     | 86/104 [00:14<00:03,  5.81it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 2:  83%|███████████████████████████▎     | 86/104 [00:14<00:03,  5.81it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 2:  84%|███████████████████████████▌     | 87/104 [00:14<00:02,  5.86it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 2:  84%|███████████████████████████▌     | 87/104 [00:14<00:02,  5.86it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  85%|███████████████████████████▉     | 88/104 [00:14<00:02,  5.75it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 2:  85%|███████████████████████████▉     | 88/104 [00:14<00:02,  5.75it/s, training_loss=0.233]\u001b[A\n",
      "Epoch 2:  86%|████████████████████████████▏    | 89/104 [00:14<00:02,  5.80it/s, training_loss=0.233]\u001b[A\n",
      "Epoch 2:  86%|████████████████████████████▏    | 89/104 [00:15<00:02,  5.80it/s, training_loss=0.175]\u001b[A\n",
      "Epoch 2:  87%|████████████████████████████▌    | 90/104 [00:15<00:02,  5.71it/s, training_loss=0.175]\u001b[A\n",
      "Epoch 2:  87%|████████████████████████████▌    | 90/104 [00:15<00:02,  5.71it/s, training_loss=0.163]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████▉    | 91/104 [00:15<00:02,  5.77it/s, training_loss=0.163]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████▉    | 91/104 [00:15<00:02,  5.77it/s, training_loss=0.051]\u001b[A\n",
      "Epoch 2:  88%|█████████████████████████████▏   | 92/104 [00:15<00:02,  5.73it/s, training_loss=0.051]\u001b[A\n",
      "Epoch 2:  88%|█████████████████████████████▏   | 92/104 [00:15<00:02,  5.73it/s, training_loss=0.149]\u001b[A\n",
      "Epoch 2:  89%|█████████████████████████████▌   | 93/104 [00:15<00:01,  5.92it/s, training_loss=0.149]\u001b[A\n",
      "Epoch 2:  89%|█████████████████████████████▌   | 93/104 [00:15<00:01,  5.92it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 2:  90%|█████████████████████████████▊   | 94/104 [00:15<00:01,  5.94it/s, training_loss=0.120]\u001b[A\n",
      "Epoch 2:  90%|█████████████████████████████▊   | 94/104 [00:15<00:01,  5.94it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 2:  91%|██████████████████████████████▏  | 95/104 [00:15<00:01,  5.96it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 2:  91%|██████████████████████████████▏  | 95/104 [00:16<00:01,  5.96it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 2:  92%|██████████████████████████████▍  | 96/104 [00:16<00:01,  5.87it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 2:  92%|██████████████████████████████▍  | 96/104 [00:16<00:01,  5.87it/s, training_loss=0.160]\u001b[A\n",
      "Epoch 2:  93%|██████████████████████████████▊  | 97/104 [00:16<00:01,  6.03it/s, training_loss=0.160]\u001b[A\n",
      "Epoch 2:  93%|██████████████████████████████▊  | 97/104 [00:16<00:01,  6.03it/s, training_loss=0.064]\u001b[A\n",
      "Epoch 2:  94%|███████████████████████████████  | 98/104 [00:16<00:01,  5.99it/s, training_loss=0.064]\u001b[A\n",
      "Epoch 2:  94%|███████████████████████████████  | 98/104 [00:16<00:01,  5.99it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████▍ | 99/104 [00:16<00:00,  6.02it/s, training_loss=0.181]\u001b[A\n",
      "Epoch 2:  95%|███████████████████████████████▍ | 99/104 [00:16<00:00,  6.02it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 2:  96%|██████████████████████████████▊ | 100/104 [00:16<00:00,  5.99it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 2:  96%|██████████████████████████████▊ | 100/104 [00:16<00:00,  5.99it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 2:  97%|███████████████████████████████ | 101/104 [00:16<00:00,  5.99it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 2:  97%|███████████████████████████████ | 101/104 [00:17<00:00,  5.99it/s, training_loss=0.166]\u001b[A\n",
      "Epoch 2:  98%|███████████████████████████████▍| 102/104 [00:17<00:00,  5.82it/s, training_loss=0.166]\u001b[A\n",
      "Epoch 2:  98%|███████████████████████████████▍| 102/104 [00:17<00:00,  5.82it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 2:  99%|███████████████████████████████▋| 103/104 [00:17<00:00,  5.87it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 2:  99%|███████████████████████████████▋| 103/104 [00:17<00:00,  5.87it/s, training_loss=0.039]\u001b[A\n",
      "Epoch 2: 100%|████████████████████████████████| 104/104 [00:17<00:00,  6.45it/s, training_loss=0.039]\u001b[A\n",
      "Epoch Progress:   5%|██▍                                              | 2/40 [00:34<11:03, 17.46s/it]\u001b[A\n",
      "Epoch 3:   0%|                                                               | 0/104 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:   0%|                                          | 0/104 [00:00<?, ?it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 3:   1%|▎                                 | 1/104 [00:00<00:17,  5.82it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 3:   1%|▎                                 | 1/104 [00:00<00:17,  5.82it/s, training_loss=0.149]\u001b[A\n",
      "Epoch 3:   2%|▋                                 | 2/104 [00:00<00:16,  6.07it/s, training_loss=0.149]\u001b[A\n",
      "Epoch 3:   2%|▋                                 | 2/104 [00:00<00:16,  6.07it/s, training_loss=0.124]\u001b[A\n",
      "Epoch 3:   3%|▉                                 | 3/104 [00:00<00:16,  6.07it/s, training_loss=0.124]\u001b[A\n",
      "Epoch 3:   3%|▉                                 | 3/104 [00:00<00:16,  6.07it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 3:   4%|█▎                                | 4/104 [00:00<00:16,  6.01it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 3:   4%|█▎                                | 4/104 [00:00<00:16,  6.01it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 3:   5%|█▋                                | 5/104 [00:00<00:16,  6.00it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 3:   5%|█▋                                | 5/104 [00:00<00:16,  6.00it/s, training_loss=0.023]\u001b[A\n",
      "Epoch 3:   6%|█▉                                | 6/104 [00:01<00:16,  5.86it/s, training_loss=0.023]\u001b[A\n",
      "Epoch 3:   6%|█▉                                | 6/104 [00:01<00:16,  5.86it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:   7%|██▎                               | 7/104 [00:01<00:15,  6.07it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:   7%|██▎                               | 7/104 [00:01<00:15,  6.07it/s, training_loss=0.054]\u001b[A\n",
      "Epoch 3:   8%|██▌                               | 8/104 [00:01<00:16,  5.85it/s, training_loss=0.054]\u001b[A\n",
      "Epoch 3:   8%|██▌                               | 8/104 [00:01<00:16,  5.85it/s, training_loss=0.095]\u001b[A\n",
      "Epoch 3:   9%|██▉                               | 9/104 [00:01<00:16,  5.87it/s, training_loss=0.095]\u001b[A\n",
      "Epoch 3:   9%|██▉                               | 9/104 [00:01<00:16,  5.87it/s, training_loss=0.019]\u001b[A\n",
      "Epoch 3:  10%|███▏                             | 10/104 [00:01<00:15,  5.90it/s, training_loss=0.019]\u001b[A\n",
      "Epoch 3:  10%|███▏                             | 10/104 [00:01<00:15,  5.90it/s, training_loss=0.016]\u001b[A\n",
      "Epoch 3:  11%|███▍                             | 11/104 [00:01<00:15,  5.94it/s, training_loss=0.016]\u001b[A\n",
      "Epoch 3:  11%|███▍                             | 11/104 [00:02<00:15,  5.94it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:  12%|███▊                             | 12/104 [00:02<00:15,  5.78it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 3:  12%|███▊                             | 12/104 [00:02<00:15,  5.78it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 3:  12%|████▏                            | 13/104 [00:02<00:15,  5.84it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 3:  12%|████▏                            | 13/104 [00:02<00:15,  5.84it/s, training_loss=0.163]\u001b[A\n",
      "Epoch 3:  13%|████▍                            | 14/104 [00:02<00:15,  5.87it/s, training_loss=0.163]\u001b[A\n",
      "Epoch 3:  13%|████▍                            | 14/104 [00:02<00:15,  5.87it/s, training_loss=0.077]\u001b[A\n",
      "Epoch 3:  14%|████▊                            | 15/104 [00:02<00:15,  5.75it/s, training_loss=0.077]\u001b[A\n",
      "Epoch 3:  14%|████▊                            | 15/104 [00:02<00:15,  5.75it/s, training_loss=0.200]\u001b[A\n",
      "Epoch 3:  15%|█████                            | 16/104 [00:02<00:15,  5.82it/s, training_loss=0.200]\u001b[A\n",
      "Epoch 3:  15%|█████                            | 16/104 [00:02<00:15,  5.82it/s, training_loss=0.101]\u001b[A\n",
      "Epoch 3:  16%|█████▍                           | 17/104 [00:02<00:14,  5.87it/s, training_loss=0.101]\u001b[A\n",
      "Epoch 3:  16%|█████▍                           | 17/104 [00:03<00:14,  5.87it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 3:  17%|█████▋                           | 18/104 [00:03<00:14,  5.93it/s, training_loss=0.133]\u001b[A\n",
      "Epoch 3:  17%|█████▋                           | 18/104 [00:03<00:14,  5.93it/s, training_loss=0.035]\u001b[A\n",
      "Epoch 3:  18%|██████                           | 19/104 [00:03<00:14,  5.96it/s, training_loss=0.035]\u001b[A\n",
      "Epoch 3:  18%|██████                           | 19/104 [00:03<00:14,  5.96it/s, training_loss=0.190]\u001b[A\n",
      "Epoch 3:  19%|██████▎                          | 20/104 [00:03<00:14,  5.94it/s, training_loss=0.190]\u001b[A\n",
      "Epoch 3:  19%|██████▎                          | 20/104 [00:03<00:14,  5.94it/s, training_loss=0.128]\u001b[A\n",
      "Epoch 3:  20%|██████▋                          | 21/104 [00:03<00:13,  5.95it/s, training_loss=0.128]\u001b[A\n",
      "Epoch 3:  20%|██████▋                          | 21/104 [00:03<00:13,  5.95it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 3:  21%|██████▉                          | 22/104 [00:03<00:13,  6.01it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 3:  21%|██████▉                          | 22/104 [00:03<00:13,  6.01it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 3:  22%|███████▎                         | 23/104 [00:03<00:13,  5.96it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 3:  22%|███████▎                         | 23/104 [00:04<00:13,  5.96it/s, training_loss=0.141]\u001b[A\n",
      "Epoch 3:  23%|███████▌                         | 24/104 [00:04<00:13,  5.98it/s, training_loss=0.141]\u001b[A\n",
      "Epoch 3:  23%|███████▌                         | 24/104 [00:04<00:13,  5.98it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 3:  24%|███████▉                         | 25/104 [00:04<00:13,  5.99it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 3:  24%|███████▉                         | 25/104 [00:04<00:13,  5.99it/s, training_loss=0.139]\u001b[A\n",
      "Epoch 3:  25%|████████▎                        | 26/104 [00:04<00:13,  5.99it/s, training_loss=0.139]\u001b[A\n",
      "Epoch 3:  25%|████████▎                        | 26/104 [00:04<00:13,  5.99it/s, training_loss=0.091]\u001b[A\n",
      "Epoch 3:  26%|████████▌                        | 27/104 [00:04<00:12,  6.16it/s, training_loss=0.091]\u001b[A\n",
      "Epoch 3:  26%|████████▌                        | 27/104 [00:04<00:12,  6.16it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  27%|████████▉                        | 28/104 [00:04<00:12,  5.95it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  27%|████████▉                        | 28/104 [00:04<00:12,  5.95it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 3:  28%|█████████▏                       | 29/104 [00:04<00:12,  6.11it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 3:  28%|█████████▏                       | 29/104 [00:05<00:12,  6.11it/s, training_loss=0.043]\u001b[A\n",
      "Epoch 3:  29%|█████████▌                       | 30/104 [00:05<00:12,  5.96it/s, training_loss=0.043]\u001b[A\n",
      "Epoch 3:  29%|█████████▌                       | 30/104 [00:05<00:12,  5.96it/s, training_loss=0.099]\u001b[A\n",
      "Epoch 3:  30%|█████████▊                       | 31/104 [00:05<00:12,  5.96it/s, training_loss=0.099]\u001b[A\n",
      "Epoch 3:  30%|█████████▊                       | 31/104 [00:05<00:12,  5.96it/s, training_loss=0.047]\u001b[A\n",
      "Epoch 3:  31%|██████████▏                      | 32/104 [00:05<00:12,  5.92it/s, training_loss=0.047]\u001b[A\n",
      "Epoch 3:  31%|██████████▏                      | 32/104 [00:05<00:12,  5.92it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  32%|██████████▍                      | 33/104 [00:05<00:11,  5.95it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  32%|██████████▍                      | 33/104 [00:05<00:11,  5.95it/s, training_loss=0.089]\u001b[A\n",
      "Epoch 3:  33%|██████████▊                      | 34/104 [00:05<00:11,  5.96it/s, training_loss=0.089]\u001b[A\n",
      "Epoch 3:  33%|██████████▊                      | 34/104 [00:05<00:11,  5.96it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 3:  34%|███████████                      | 35/104 [00:05<00:11,  5.97it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 3:  34%|███████████                      | 35/104 [00:06<00:11,  5.97it/s, training_loss=0.094]\u001b[A\n",
      "Epoch 3:  35%|███████████▍                     | 36/104 [00:06<00:11,  5.96it/s, training_loss=0.094]\u001b[A\n",
      "Epoch 3:  35%|███████████▍                     | 36/104 [00:06<00:11,  5.96it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 3:  36%|███████████▋                     | 37/104 [00:06<00:11,  5.99it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 3:  36%|███████████▋                     | 37/104 [00:06<00:11,  5.99it/s, training_loss=0.086]\u001b[A\n",
      "Epoch 3:  37%|████████████                     | 38/104 [00:06<00:11,  5.99it/s, training_loss=0.086]\u001b[A\n",
      "Epoch 3:  37%|████████████                     | 38/104 [00:06<00:11,  5.99it/s, training_loss=0.091]\u001b[A\n",
      "Epoch 3:  38%|████████████▍                    | 39/104 [00:06<00:10,  5.99it/s, training_loss=0.091]\u001b[A\n",
      "Epoch 3:  38%|████████████▍                    | 39/104 [00:06<00:10,  5.99it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 3:  38%|████████████▋                    | 40/104 [00:06<00:10,  6.01it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 3:  38%|████████████▋                    | 40/104 [00:06<00:10,  6.01it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 3:  39%|█████████████                    | 41/104 [00:06<00:10,  5.99it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 3:  39%|█████████████                    | 41/104 [00:07<00:10,  5.99it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 3:  40%|█████████████▎                   | 42/104 [00:07<00:10,  5.96it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 3:  40%|█████████████▎                   | 42/104 [00:07<00:10,  5.96it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 3:  41%|█████████████▋                   | 43/104 [00:07<00:10,  5.99it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 3:  41%|█████████████▋                   | 43/104 [00:07<00:10,  5.99it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 3:  42%|█████████████▉                   | 44/104 [00:07<00:10,  5.84it/s, training_loss=0.150]\u001b[A\n",
      "Epoch 3:  42%|█████████████▉                   | 44/104 [00:07<00:10,  5.84it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 3:  43%|██████████████▎                  | 45/104 [00:07<00:10,  5.87it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 3:  43%|██████████████▎                  | 45/104 [00:07<00:10,  5.87it/s, training_loss=0.117]\u001b[A\n",
      "Epoch 3:  44%|██████████████▌                  | 46/104 [00:07<00:10,  5.74it/s, training_loss=0.117]\u001b[A\n",
      "Epoch 3:  44%|██████████████▌                  | 46/104 [00:07<00:10,  5.74it/s, training_loss=0.056]\u001b[A\n",
      "Epoch 3:  45%|██████████████▉                  | 47/104 [00:07<00:09,  5.82it/s, training_loss=0.056]\u001b[A\n",
      "Epoch 3:  45%|██████████████▉                  | 47/104 [00:08<00:09,  5.82it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  46%|███████████████▏                 | 48/104 [00:08<00:09,  5.90it/s, training_loss=0.164]\u001b[A\n",
      "Epoch 3:  46%|███████████████▏                 | 48/104 [00:08<00:09,  5.90it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 3:  47%|███████████████▌                 | 49/104 [00:08<00:09,  5.88it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 3:  47%|███████████████▌                 | 49/104 [00:08<00:09,  5.88it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 3:  48%|███████████████▊                 | 50/104 [00:08<00:09,  5.75it/s, training_loss=0.100]\u001b[A\n",
      "Epoch 3:  48%|███████████████▊                 | 50/104 [00:08<00:09,  5.75it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 3:  49%|████████████████▏                | 51/104 [00:08<00:09,  5.87it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 3:  49%|████████████████▏                | 51/104 [00:08<00:09,  5.87it/s, training_loss=0.026]\u001b[A\n",
      "Epoch 3:  50%|████████████████▌                | 52/104 [00:08<00:08,  5.86it/s, training_loss=0.026]\u001b[A\n",
      "Epoch 3:  50%|████████████████▌                | 52/104 [00:08<00:08,  5.86it/s, training_loss=0.101]\u001b[A\n",
      "Epoch 3:  51%|████████████████▊                | 53/104 [00:08<00:08,  5.73it/s, training_loss=0.101]\u001b[A\n",
      "Epoch 3:  51%|████████████████▊                | 53/104 [00:09<00:08,  5.73it/s, training_loss=0.081]\u001b[A\n",
      "Epoch 3:  52%|█████████████████▏               | 54/104 [00:09<00:08,  5.82it/s, training_loss=0.081]\u001b[A\n",
      "Epoch 3:  52%|█████████████████▏               | 54/104 [00:09<00:08,  5.82it/s, training_loss=0.038]\u001b[A\n",
      "Epoch 3:  53%|█████████████████▍               | 55/104 [00:09<00:08,  5.82it/s, training_loss=0.038]\u001b[A\n",
      "Epoch 3:  53%|█████████████████▍               | 55/104 [00:09<00:08,  5.82it/s, training_loss=0.036]\u001b[A\n",
      "Epoch 3:  54%|█████████████████▊               | 56/104 [00:09<00:08,  5.74it/s, training_loss=0.036]\u001b[A\n",
      "Epoch 3:  54%|█████████████████▊               | 56/104 [00:09<00:08,  5.74it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 3:  55%|██████████████████               | 57/104 [00:09<00:08,  5.82it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 3:  55%|██████████████████               | 57/104 [00:09<00:08,  5.82it/s, training_loss=0.102]\u001b[A\n",
      "Epoch 3:  56%|██████████████████▍              | 58/104 [00:09<00:07,  5.87it/s, training_loss=0.102]\u001b[A\n",
      "Epoch 3:  56%|██████████████████▍              | 58/104 [00:09<00:07,  5.87it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  57%|██████████████████▋              | 59/104 [00:09<00:07,  5.73it/s, training_loss=0.154]\u001b[A\n",
      "Epoch 3:  57%|██████████████████▋              | 59/104 [00:10<00:07,  5.73it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 3:  58%|███████████████████              | 60/104 [00:10<00:07,  5.86it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 3:  58%|███████████████████              | 60/104 [00:10<00:07,  5.86it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 3:  59%|███████████████████▎             | 61/104 [00:10<00:07,  5.85it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 3:  59%|███████████████████▎             | 61/104 [00:10<00:07,  5.85it/s, training_loss=0.097]\u001b[A\n",
      "Epoch 3:  60%|███████████████████▋             | 62/104 [00:10<00:07,  5.87it/s, training_loss=0.097]\u001b[A\n",
      "Epoch 3:  60%|███████████████████▋             | 62/104 [00:10<00:07,  5.87it/s, training_loss=0.142]\u001b[A\n",
      "Epoch 3:  61%|███████████████████▉             | 63/104 [00:10<00:06,  6.00it/s, training_loss=0.142]\u001b[A\n",
      "Epoch 3:  61%|███████████████████▉             | 63/104 [00:10<00:06,  6.00it/s, training_loss=0.106]\u001b[A\n",
      "Epoch 3:  62%|████████████████████▎            | 64/104 [00:10<00:06,  5.93it/s, training_loss=0.106]\u001b[A\n",
      "Epoch 3:  62%|████████████████████▎            | 64/104 [00:10<00:06,  5.93it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 3:  62%|████████████████████▋            | 65/104 [00:10<00:06,  5.96it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 3:  62%|████████████████████▋            | 65/104 [00:11<00:06,  5.96it/s, training_loss=0.122]\u001b[A\n",
      "Epoch 3:  63%|████████████████████▉            | 66/104 [00:11<00:06,  5.94it/s, training_loss=0.122]\u001b[A\n",
      "Epoch 3:  63%|████████████████████▉            | 66/104 [00:11<00:06,  5.94it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 3:  64%|█████████████████████▎           | 67/104 [00:11<00:06,  5.80it/s, training_loss=0.213]\u001b[A\n",
      "Epoch 3:  64%|█████████████████████▎           | 67/104 [00:11<00:06,  5.80it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 3:  65%|█████████████████████▌           | 68/104 [00:11<00:06,  5.83it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 3:  65%|█████████████████████▌           | 68/104 [00:11<00:06,  5.83it/s, training_loss=0.171]\u001b[A\n",
      "Epoch 3:  66%|█████████████████████▉           | 69/104 [00:11<00:05,  5.94it/s, training_loss=0.171]\u001b[A\n",
      "Epoch 3:  66%|█████████████████████▉           | 69/104 [00:11<00:05,  5.94it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 3:  67%|██████████████████████▏          | 70/104 [00:11<00:05,  5.92it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 3:  67%|██████████████████████▏          | 70/104 [00:12<00:05,  5.92it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 3:  68%|██████████████████████▌          | 71/104 [00:12<00:05,  5.94it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 3:  68%|██████████████████████▌          | 71/104 [00:12<00:05,  5.94it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 3:  69%|██████████████████████▊          | 72/104 [00:12<00:05,  5.78it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 3:  69%|██████████████████████▊          | 72/104 [00:12<00:05,  5.78it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 3:  70%|███████████████████████▏         | 73/104 [00:12<00:05,  5.86it/s, training_loss=0.127]\u001b[A\n",
      "Epoch 3:  70%|███████████████████████▏         | 73/104 [00:12<00:05,  5.86it/s, training_loss=0.038]\u001b[A\n",
      "Epoch 3:  71%|███████████████████████▍         | 74/104 [00:12<00:05,  5.88it/s, training_loss=0.038]\u001b[A\n",
      "Epoch 3:  71%|███████████████████████▍         | 74/104 [00:12<00:05,  5.88it/s, training_loss=0.040]\u001b[A\n",
      "Epoch 3:  72%|███████████████████████▊         | 75/104 [00:12<00:05,  5.78it/s, training_loss=0.040]\u001b[A\n",
      "Epoch 3:  72%|███████████████████████▊         | 75/104 [00:12<00:05,  5.78it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 3:  73%|████████████████████████         | 76/104 [00:12<00:04,  5.83it/s, training_loss=0.237]\u001b[A\n",
      "Epoch 3:  73%|████████████████████████         | 76/104 [00:13<00:04,  5.83it/s, training_loss=0.096]\u001b[A\n",
      "Epoch 3:  74%|████████████████████████▍        | 77/104 [00:13<00:04,  5.85it/s, training_loss=0.096]\u001b[A\n",
      "Epoch 3:  74%|████████████████████████▍        | 77/104 [00:13<00:04,  5.85it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 3:  75%|████████████████████████▊        | 78/104 [00:13<00:04,  5.90it/s, training_loss=0.018]\u001b[A\n",
      "Epoch 3:  75%|████████████████████████▊        | 78/104 [00:13<00:04,  5.90it/s, training_loss=0.016]\u001b[A\n",
      "Epoch 3:  76%|█████████████████████████        | 79/104 [00:13<00:04,  5.84it/s, training_loss=0.016]\u001b[A\n",
      "Epoch 3:  76%|█████████████████████████        | 79/104 [00:13<00:04,  5.84it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 3:  77%|█████████████████████████▍       | 80/104 [00:13<00:04,  6.00it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 3:  77%|█████████████████████████▍       | 80/104 [00:13<00:04,  6.00it/s, training_loss=0.067]\u001b[A\n",
      "Epoch 3:  78%|█████████████████████████▋       | 81/104 [00:13<00:03,  5.98it/s, training_loss=0.067]\u001b[A\n",
      "Epoch 3:  78%|█████████████████████████▋       | 81/104 [00:13<00:03,  5.98it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  79%|██████████████████████████       | 82/104 [00:13<00:03,  5.80it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  79%|██████████████████████████       | 82/104 [00:14<00:03,  5.80it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 3:  80%|██████████████████████████▎      | 83/104 [00:14<00:03,  5.89it/s, training_loss=0.068]\u001b[A\n",
      "Epoch 3:  80%|██████████████████████████▎      | 83/104 [00:14<00:03,  5.89it/s, training_loss=0.071]\u001b[A\n",
      "Epoch 3:  81%|██████████████████████████▋      | 84/104 [00:14<00:03,  5.77it/s, training_loss=0.071]\u001b[A\n",
      "Epoch 3:  81%|██████████████████████████▋      | 84/104 [00:14<00:03,  5.77it/s, training_loss=0.118]\u001b[A\n",
      "Epoch 3:  82%|██████████████████████████▉      | 85/104 [00:14<00:03,  6.00it/s, training_loss=0.118]\u001b[A\n",
      "Epoch 3:  82%|██████████████████████████▉      | 85/104 [00:14<00:03,  6.00it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 3:  83%|███████████████████████████▎     | 86/104 [00:14<00:03,  5.98it/s, training_loss=0.177]\u001b[A\n",
      "Epoch 3:  83%|███████████████████████████▎     | 86/104 [00:14<00:03,  5.98it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 3:  84%|███████████████████████████▌     | 87/104 [00:14<00:02,  5.99it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 3:  84%|███████████████████████████▌     | 87/104 [00:14<00:02,  5.99it/s, training_loss=0.089]\u001b[A\n",
      "Epoch 3:  85%|███████████████████████████▉     | 88/104 [00:14<00:02,  5.94it/s, training_loss=0.089]\u001b[A\n",
      "Epoch 3:  85%|███████████████████████████▉     | 88/104 [00:15<00:02,  5.94it/s, training_loss=0.039]\u001b[A\n",
      "Epoch 3:  86%|████████████████████████████▏    | 89/104 [00:15<00:02,  5.97it/s, training_loss=0.039]\u001b[A\n",
      "Epoch 3:  86%|████████████████████████████▏    | 89/104 [00:15<00:02,  5.97it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 3:  87%|████████████████████████████▌    | 90/104 [00:15<00:02,  5.84it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 3:  87%|████████████████████████████▌    | 90/104 [00:15<00:02,  5.84it/s, training_loss=0.118]\u001b[A\n",
      "Epoch 3:  88%|████████████████████████████▉    | 91/104 [00:15<00:02,  6.03it/s, training_loss=0.118]\u001b[A\n",
      "Epoch 3:  88%|████████████████████████████▉    | 91/104 [00:15<00:02,  6.03it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 3:  88%|█████████████████████████████▏   | 92/104 [00:15<00:02,  5.84it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 3:  88%|█████████████████████████████▏   | 92/104 [00:15<00:02,  5.84it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 3:  89%|█████████████████████████████▌   | 93/104 [00:15<00:01,  5.74it/s, training_loss=0.258]\u001b[A\n",
      "Epoch 3:  89%|█████████████████████████████▌   | 93/104 [00:15<00:01,  5.74it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 3:  90%|█████████████████████████████▊   | 94/104 [00:15<00:01,  5.97it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 3:  90%|█████████████████████████████▊   | 94/104 [00:16<00:01,  5.97it/s, training_loss=0.049]\u001b[A\n",
      "Epoch 3:  91%|██████████████████████████████▏  | 95/104 [00:16<00:01,  5.97it/s, training_loss=0.049]\u001b[A\n",
      "Epoch 3:  91%|██████████████████████████████▏  | 95/104 [00:16<00:01,  5.97it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  92%|██████████████████████████████▍  | 96/104 [00:16<00:01,  5.84it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 3:  92%|██████████████████████████████▍  | 96/104 [00:16<00:01,  5.84it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 3:  93%|██████████████████████████████▊  | 97/104 [00:16<00:01,  6.02it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 3:  93%|██████████████████████████████▊  | 97/104 [00:16<00:01,  6.02it/s, training_loss=0.092]\u001b[A\n",
      "Epoch 3:  94%|███████████████████████████████  | 98/104 [00:16<00:01,  5.86it/s, training_loss=0.092]\u001b[A\n",
      "Epoch 3:  94%|███████████████████████████████  | 98/104 [00:16<00:01,  5.86it/s, training_loss=0.102]\u001b[A\n",
      "Epoch 3:  95%|███████████████████████████████▍ | 99/104 [00:16<00:00,  6.02it/s, training_loss=0.102]\u001b[A\n",
      "Epoch 3:  95%|███████████████████████████████▍ | 99/104 [00:16<00:00,  6.02it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  96%|██████████████████████████████▊ | 100/104 [00:16<00:00,  5.90it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 3:  96%|██████████████████████████████▊ | 100/104 [00:17<00:00,  5.90it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 3:  97%|███████████████████████████████ | 101/104 [00:17<00:00,  5.91it/s, training_loss=0.232]\u001b[A\n",
      "Epoch 3:  97%|███████████████████████████████ | 101/104 [00:17<00:00,  5.91it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 3:  98%|███████████████████████████████▍| 102/104 [00:17<00:00,  5.94it/s, training_loss=0.123]\u001b[A\n",
      "Epoch 3:  98%|███████████████████████████████▍| 102/104 [00:17<00:00,  5.94it/s, training_loss=0.166]\u001b[A\n",
      "Epoch 3:  99%|███████████████████████████████▋| 103/104 [00:17<00:00,  6.07it/s, training_loss=0.166]\u001b[A\n",
      "Epoch 3:  99%|███████████████████████████████▋| 103/104 [00:17<00:00,  6.07it/s, training_loss=0.190]\u001b[A\n",
      "Epoch 3: 100%|████████████████████████████████| 104/104 [00:17<00:00,  6.40it/s, training_loss=0.190]\u001b[A\n",
      "Epoch Progress:   8%|███▋                                             | 3/40 [00:52<10:47, 17.51s/it]\u001b[A\n",
      "Epoch 4:   0%|                                                               | 0/104 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:   0%|                                          | 0/104 [00:00<?, ?it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 4:   1%|▎                                 | 1/104 [00:00<00:17,  6.02it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 4:   1%|▎                                 | 1/104 [00:00<00:17,  6.02it/s, training_loss=0.025]\u001b[A\n",
      "Epoch 4:   2%|▋                                 | 2/104 [00:00<00:16,  6.00it/s, training_loss=0.025]\u001b[A\n",
      "Epoch 4:   2%|▋                                 | 2/104 [00:00<00:16,  6.00it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 4:   3%|▉                                 | 3/104 [00:00<00:16,  6.23it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 4:   3%|▉                                 | 3/104 [00:00<00:16,  6.23it/s, training_loss=0.032]\u001b[A\n",
      "Epoch 4:   4%|█▎                                | 4/104 [00:00<00:16,  6.14it/s, training_loss=0.032]\u001b[A\n",
      "Epoch 4:   4%|█▎                                | 4/104 [00:00<00:16,  6.14it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:   5%|█▋                                | 5/104 [00:00<00:16,  6.08it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:   5%|█▋                                | 5/104 [00:01<00:16,  6.08it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 4:   6%|█▉                                | 6/104 [00:01<00:16,  5.84it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 4:   6%|█▉                                | 6/104 [00:01<00:16,  5.84it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 4:   7%|██▎                               | 7/104 [00:01<00:16,  5.89it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 4:   7%|██▎                               | 7/104 [00:01<00:16,  5.89it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 4:   8%|██▌                               | 8/104 [00:01<00:16,  5.74it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 4:   8%|██▌                               | 8/104 [00:01<00:16,  5.74it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 4:   9%|██▉                               | 9/104 [00:01<00:16,  5.74it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 4:   9%|██▉                               | 9/104 [00:01<00:16,  5.74it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 4:  10%|███▏                             | 10/104 [00:01<00:16,  5.77it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 4:  10%|███▏                             | 10/104 [00:01<00:16,  5.77it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  11%|███▍                             | 11/104 [00:01<00:16,  5.80it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  11%|███▍                             | 11/104 [00:02<00:16,  5.80it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 4:  12%|███▊                             | 12/104 [00:02<00:15,  5.84it/s, training_loss=0.073]\u001b[A\n",
      "Epoch 4:  12%|███▊                             | 12/104 [00:02<00:15,  5.84it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 4:  12%|████▏                            | 13/104 [00:02<00:15,  5.89it/s, training_loss=0.109]\u001b[A\n",
      "Epoch 4:  12%|████▏                            | 13/104 [00:02<00:15,  5.89it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 4:  13%|████▍                            | 14/104 [00:02<00:15,  5.93it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 4:  13%|████▍                            | 14/104 [00:02<00:15,  5.93it/s, training_loss=0.040]\u001b[A\n",
      "Epoch 4:  14%|████▊                            | 15/104 [00:02<00:15,  5.89it/s, training_loss=0.040]\u001b[A\n",
      "Epoch 4:  14%|████▊                            | 15/104 [00:02<00:15,  5.89it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 4:  15%|█████                            | 16/104 [00:02<00:14,  5.98it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 4:  15%|█████                            | 16/104 [00:02<00:14,  5.98it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 4:  16%|█████▍                           | 17/104 [00:02<00:14,  5.88it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 4:  16%|█████▍                           | 17/104 [00:03<00:14,  5.88it/s, training_loss=0.183]\u001b[A\n",
      "Epoch 4:  17%|█████▋                           | 18/104 [00:03<00:14,  6.02it/s, training_loss=0.183]\u001b[A\n",
      "Epoch 4:  17%|█████▋                           | 18/104 [00:03<00:14,  6.02it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:  18%|██████                           | 19/104 [00:03<00:14,  5.82it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:  18%|██████                           | 19/104 [00:03<00:14,  5.82it/s, training_loss=0.054]\u001b[A\n",
      "Epoch 4:  19%|██████▎                          | 20/104 [00:03<00:14,  5.90it/s, training_loss=0.054]\u001b[A\n",
      "Epoch 4:  19%|██████▎                          | 20/104 [00:03<00:14,  5.90it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 4:  20%|██████▋                          | 21/104 [00:03<00:14,  5.84it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 4:  20%|██████▋                          | 21/104 [00:03<00:14,  5.84it/s, training_loss=0.019]\u001b[A\n",
      "Epoch 4:  21%|██████▉                          | 22/104 [00:03<00:13,  5.98it/s, training_loss=0.019]\u001b[A\n",
      "Epoch 4:  21%|██████▉                          | 22/104 [00:03<00:13,  5.98it/s, training_loss=0.011]\u001b[A\n",
      "Epoch 4:  22%|███████▎                         | 23/104 [00:03<00:13,  5.96it/s, training_loss=0.011]\u001b[A\n",
      "Epoch 4:  22%|███████▎                         | 23/104 [00:04<00:13,  5.96it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 4:  23%|███████▌                         | 24/104 [00:04<00:13,  5.80it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 4:  23%|███████▌                         | 24/104 [00:04<00:13,  5.80it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:  24%|███████▉                         | 25/104 [00:04<00:13,  5.80it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:  24%|███████▉                         | 25/104 [00:04<00:13,  5.80it/s, training_loss=0.075]\u001b[A\n",
      "Epoch 4:  25%|████████▎                        | 26/104 [00:04<00:13,  5.91it/s, training_loss=0.075]\u001b[A\n",
      "Epoch 4:  25%|████████▎                        | 26/104 [00:04<00:13,  5.91it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 4:  26%|████████▌                        | 27/104 [00:04<00:13,  5.77it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 4:  26%|████████▌                        | 27/104 [00:04<00:13,  5.77it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 4:  27%|████████▉                        | 28/104 [00:04<00:13,  5.74it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 4:  27%|████████▉                        | 28/104 [00:04<00:13,  5.74it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 4:  28%|█████████▏                       | 29/104 [00:04<00:12,  5.93it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 4:  28%|█████████▏                       | 29/104 [00:05<00:12,  5.93it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 4:  29%|█████████▌                       | 30/104 [00:05<00:12,  5.81it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 4:  29%|█████████▌                       | 30/104 [00:05<00:12,  5.81it/s, training_loss=0.054]\u001b[A\n",
      "Epoch 4:  30%|█████████▊                       | 31/104 [00:05<00:12,  5.99it/s, training_loss=0.054]\u001b[A\n",
      "Epoch 4:  30%|█████████▊                       | 31/104 [00:05<00:12,  5.99it/s, training_loss=0.084]\u001b[A\n",
      "Epoch 4:  31%|██████████▏                      | 32/104 [00:05<00:12,  5.84it/s, training_loss=0.084]\u001b[A\n",
      "Epoch 4:  31%|██████████▏                      | 32/104 [00:05<00:12,  5.84it/s, training_loss=0.155]\u001b[A\n",
      "Epoch 4:  32%|██████████▍                      | 33/104 [00:05<00:12,  5.89it/s, training_loss=0.155]\u001b[A\n",
      "Epoch 4:  32%|██████████▍                      | 33/104 [00:05<00:12,  5.89it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 4:  33%|██████████▊                      | 34/104 [00:05<00:11,  5.92it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 4:  33%|██████████▊                      | 34/104 [00:05<00:11,  5.92it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 4:  34%|███████████                      | 35/104 [00:05<00:11,  5.92it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 4:  34%|███████████                      | 35/104 [00:06<00:11,  5.92it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 4:  35%|███████████▍                     | 36/104 [00:06<00:11,  5.93it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 4:  35%|███████████▍                     | 36/104 [00:06<00:11,  5.93it/s, training_loss=0.086]\u001b[A\n",
      "Epoch 4:  36%|███████████▋                     | 37/104 [00:06<00:11,  5.93it/s, training_loss=0.086]\u001b[A\n",
      "Epoch 4:  36%|███████████▋                     | 37/104 [00:06<00:11,  5.93it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  37%|████████████                     | 38/104 [00:06<00:11,  5.96it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  37%|████████████                     | 38/104 [00:06<00:11,  5.96it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  38%|████████████▍                    | 39/104 [00:06<00:10,  5.99it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  38%|████████████▍                    | 39/104 [00:06<00:10,  5.99it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:  38%|████████████▋                    | 40/104 [00:06<00:10,  5.97it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:  38%|████████████▋                    | 40/104 [00:06<00:10,  5.97it/s, training_loss=0.077]\u001b[A\n",
      "Epoch 4:  39%|█████████████                    | 41/104 [00:06<00:10,  5.76it/s, training_loss=0.077]\u001b[A\n",
      "Epoch 4:  39%|█████████████                    | 41/104 [00:07<00:10,  5.76it/s, training_loss=0.039]\u001b[A\n",
      "Epoch 4:  40%|█████████████▎                   | 42/104 [00:07<00:10,  5.78it/s, training_loss=0.039]\u001b[A\n",
      "Epoch 4:  40%|█████████████▎                   | 42/104 [00:07<00:10,  5.78it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  41%|█████████████▋                   | 43/104 [00:07<00:10,  5.66it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  41%|█████████████▋                   | 43/104 [00:07<00:10,  5.66it/s, training_loss=0.096]\u001b[A\n",
      "Epoch 4:  42%|█████████████▉                   | 44/104 [00:07<00:10,  5.71it/s, training_loss=0.096]\u001b[A\n",
      "Epoch 4:  42%|█████████████▉                   | 44/104 [00:07<00:10,  5.71it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 4:  43%|██████████████▎                  | 45/104 [00:07<00:10,  5.78it/s, training_loss=0.061]\u001b[A\n",
      "Epoch 4:  43%|██████████████▎                  | 45/104 [00:07<00:10,  5.78it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  44%|██████████████▌                  | 46/104 [00:07<00:09,  5.85it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  44%|██████████████▌                  | 46/104 [00:07<00:09,  5.85it/s, training_loss=0.040]\u001b[A\n",
      "Epoch 4:  45%|██████████████▉                  | 47/104 [00:07<00:09,  5.89it/s, training_loss=0.040]\u001b[A\n",
      "Epoch 4:  45%|██████████████▉                  | 47/104 [00:08<00:09,  5.89it/s, training_loss=0.080]\u001b[A\n",
      "Epoch 4:  46%|███████████████▏                 | 48/104 [00:08<00:09,  5.84it/s, training_loss=0.080]\u001b[A\n",
      "Epoch 4:  46%|███████████████▏                 | 48/104 [00:08<00:09,  5.84it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 4:  47%|███████████████▌                 | 49/104 [00:08<00:09,  5.87it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 4:  47%|███████████████▌                 | 49/104 [00:08<00:09,  5.87it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 4:  48%|███████████████▊                 | 50/104 [00:08<00:09,  5.88it/s, training_loss=0.201]\u001b[A\n",
      "Epoch 4:  48%|███████████████▊                 | 50/104 [00:08<00:09,  5.88it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 4:  49%|████████████████▏                | 51/104 [00:08<00:09,  5.86it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 4:  49%|████████████████▏                | 51/104 [00:08<00:09,  5.86it/s, training_loss=0.093]\u001b[A\n",
      "Epoch 4:  50%|████████████████▌                | 52/104 [00:08<00:08,  5.87it/s, training_loss=0.093]\u001b[A\n",
      "Epoch 4:  50%|████████████████▌                | 52/104 [00:09<00:08,  5.87it/s, training_loss=0.160]\u001b[A\n",
      "Epoch 4:  51%|████████████████▊                | 53/104 [00:09<00:08,  5.88it/s, training_loss=0.160]\u001b[A\n",
      "Epoch 4:  51%|████████████████▊                | 53/104 [00:09<00:08,  5.88it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  52%|█████████████████▏               | 54/104 [00:09<00:08,  5.98it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  52%|█████████████████▏               | 54/104 [00:09<00:08,  5.98it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 4:  53%|█████████████████▍               | 55/104 [00:09<00:08,  5.96it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 4:  53%|█████████████████▍               | 55/104 [00:09<00:08,  5.96it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 4:  54%|█████████████████▊               | 56/104 [00:09<00:08,  5.86it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 4:  54%|█████████████████▊               | 56/104 [00:09<00:08,  5.86it/s, training_loss=0.015]\u001b[A\n",
      "Epoch 4:  55%|██████████████████               | 57/104 [00:09<00:07,  5.88it/s, training_loss=0.015]\u001b[A\n",
      "Epoch 4:  55%|██████████████████               | 57/104 [00:09<00:07,  5.88it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 4:  56%|██████████████████▍              | 58/104 [00:09<00:07,  5.89it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 4:  56%|██████████████████▍              | 58/104 [00:10<00:07,  5.89it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  57%|██████████████████▋              | 59/104 [00:10<00:07,  5.85it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  57%|██████████████████▋              | 59/104 [00:10<00:07,  5.85it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 4:  58%|███████████████████              | 60/104 [00:10<00:07,  5.85it/s, training_loss=0.162]\u001b[A\n",
      "Epoch 4:  58%|███████████████████              | 60/104 [00:10<00:07,  5.85it/s, training_loss=0.008]\u001b[A\n",
      "Epoch 4:  59%|███████████████████▎             | 61/104 [00:10<00:07,  5.85it/s, training_loss=0.008]\u001b[A\n",
      "Epoch 4:  59%|███████████████████▎             | 61/104 [00:10<00:07,  5.85it/s, training_loss=0.014]\u001b[A\n",
      "Epoch 4:  60%|███████████████████▋             | 62/104 [00:10<00:07,  5.82it/s, training_loss=0.014]\u001b[A\n",
      "Epoch 4:  60%|███████████████████▋             | 62/104 [00:10<00:07,  5.82it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 4:  61%|███████████████████▉             | 63/104 [00:10<00:07,  5.74it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 4:  61%|███████████████████▉             | 63/104 [00:10<00:07,  5.74it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 4:  62%|████████████████████▎            | 64/104 [00:10<00:06,  5.74it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 4:  62%|████████████████████▎            | 64/104 [00:11<00:06,  5.74it/s, training_loss=0.063]\u001b[A\n",
      "Epoch 4:  62%|████████████████████▋            | 65/104 [00:11<00:06,  5.80it/s, training_loss=0.063]\u001b[A\n",
      "Epoch 4:  62%|████████████████████▋            | 65/104 [00:11<00:06,  5.80it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 4:  63%|████████████████████▉            | 66/104 [00:11<00:06,  5.83it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 4:  63%|████████████████████▉            | 66/104 [00:11<00:06,  5.83it/s, training_loss=0.019]\u001b[A\n",
      "Epoch 4:  64%|█████████████████████▎           | 67/104 [00:11<00:06,  5.85it/s, training_loss=0.019]\u001b[A\n",
      "Epoch 4:  64%|█████████████████████▎           | 67/104 [00:11<00:06,  5.85it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  65%|█████████████████████▌           | 68/104 [00:11<00:06,  5.89it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  65%|█████████████████████▌           | 68/104 [00:11<00:06,  5.89it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  66%|█████████████████████▉           | 69/104 [00:11<00:05,  5.87it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  66%|█████████████████████▉           | 69/104 [00:11<00:05,  5.87it/s, training_loss=0.075]\u001b[A\n",
      "Epoch 4:  67%|██████████████████████▏          | 70/104 [00:11<00:05,  5.90it/s, training_loss=0.075]\u001b[A\n",
      "Epoch 4:  67%|██████████████████████▏          | 70/104 [00:12<00:05,  5.90it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 4:  68%|██████████████████████▌          | 71/104 [00:12<00:05,  5.90it/s, training_loss=0.083]\u001b[A\n",
      "Epoch 4:  68%|██████████████████████▌          | 71/104 [00:12<00:05,  5.90it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 4:  69%|██████████████████████▊          | 72/104 [00:12<00:05,  5.85it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 4:  69%|██████████████████████▊          | 72/104 [00:12<00:05,  5.85it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 4:  70%|███████████████████████▏         | 73/104 [00:12<00:05,  5.66it/s, training_loss=0.234]\u001b[A\n",
      "Epoch 4:  70%|███████████████████████▏         | 73/104 [00:12<00:05,  5.66it/s, training_loss=0.011]\u001b[A\n",
      "Epoch 4:  71%|███████████████████████▍         | 74/104 [00:12<00:05,  5.63it/s, training_loss=0.011]\u001b[A\n",
      "Epoch 4:  71%|███████████████████████▍         | 74/104 [00:12<00:05,  5.63it/s, training_loss=0.015]\u001b[A\n",
      "Epoch 4:  72%|███████████████████████▊         | 75/104 [00:12<00:05,  5.63it/s, training_loss=0.015]\u001b[A\n",
      "Epoch 4:  72%|███████████████████████▊         | 75/104 [00:13<00:05,  5.63it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  73%|████████████████████████         | 76/104 [00:13<00:05,  5.47it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  73%|████████████████████████         | 76/104 [00:13<00:05,  5.47it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 4:  74%|████████████████████████▍        | 77/104 [00:13<00:04,  5.54it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 4:  74%|████████████████████████▍        | 77/104 [00:13<00:04,  5.54it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  75%|████████████████████████▊        | 78/104 [00:13<00:04,  5.56it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  75%|████████████████████████▊        | 78/104 [00:13<00:04,  5.56it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  76%|█████████████████████████        | 79/104 [00:13<00:04,  5.64it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 4:  76%|█████████████████████████        | 79/104 [00:13<00:04,  5.64it/s, training_loss=0.104]\u001b[A\n",
      "Epoch 4:  77%|█████████████████████████▍       | 80/104 [00:13<00:04,  5.54it/s, training_loss=0.104]\u001b[A\n",
      "Epoch 4:  77%|█████████████████████████▍       | 80/104 [00:13<00:04,  5.54it/s, training_loss=0.008]\u001b[A\n",
      "Epoch 4:  78%|█████████████████████████▋       | 81/104 [00:13<00:04,  5.62it/s, training_loss=0.008]\u001b[A\n",
      "Epoch 4:  78%|█████████████████████████▋       | 81/104 [00:14<00:04,  5.62it/s, training_loss=0.104]\u001b[A\n",
      "Epoch 4:  79%|██████████████████████████       | 82/104 [00:14<00:03,  5.68it/s, training_loss=0.104]\u001b[A\n",
      "Epoch 4:  79%|██████████████████████████       | 82/104 [00:14<00:03,  5.68it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:  80%|██████████████████████████▎      | 83/104 [00:14<00:03,  5.62it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 4:  80%|██████████████████████████▎      | 83/104 [00:14<00:03,  5.62it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  81%|██████████████████████████▋      | 84/104 [00:14<00:03,  5.68it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  81%|██████████████████████████▋      | 84/104 [00:14<00:03,  5.68it/s, training_loss=0.029]\u001b[A\n",
      "Epoch 4:  82%|██████████████████████████▉      | 85/104 [00:14<00:03,  5.72it/s, training_loss=0.029]\u001b[A\n",
      "Epoch 4:  82%|██████████████████████████▉      | 85/104 [00:14<00:03,  5.72it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 4:  83%|███████████████████████████▎     | 86/104 [00:14<00:03,  5.75it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 4:  83%|███████████████████████████▎     | 86/104 [00:14<00:03,  5.75it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 4:  84%|███████████████████████████▌     | 87/104 [00:14<00:02,  5.77it/s, training_loss=0.082]\u001b[A\n",
      "Epoch 4:  84%|███████████████████████████▌     | 87/104 [00:15<00:02,  5.77it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 4:  85%|███████████████████████████▉     | 88/104 [00:15<00:02,  5.79it/s, training_loss=0.289]\u001b[A\n",
      "Epoch 4:  85%|███████████████████████████▉     | 88/104 [00:15<00:02,  5.79it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  86%|████████████████████████████▏    | 89/104 [00:15<00:02,  5.80it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 4:  86%|████████████████████████████▏    | 89/104 [00:15<00:02,  5.80it/s, training_loss=0.140]\u001b[A\n",
      "Epoch 4:  87%|████████████████████████████▌    | 90/104 [00:15<00:02,  5.85it/s, training_loss=0.140]\u001b[A\n",
      "Epoch 4:  87%|████████████████████████████▌    | 90/104 [00:15<00:02,  5.85it/s, training_loss=0.016]\u001b[A\n",
      "Epoch 4:  88%|████████████████████████████▉    | 91/104 [00:15<00:02,  5.84it/s, training_loss=0.016]\u001b[A\n",
      "Epoch 4:  88%|████████████████████████████▉    | 91/104 [00:15<00:02,  5.84it/s, training_loss=0.175]\u001b[A\n",
      "Epoch 4:  88%|█████████████████████████████▏   | 92/104 [00:15<00:01,  6.00it/s, training_loss=0.175]\u001b[A\n",
      "Epoch 4:  88%|█████████████████████████████▏   | 92/104 [00:15<00:01,  6.00it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 4:  89%|█████████████████████████████▌   | 93/104 [00:15<00:01,  5.82it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 4:  89%|█████████████████████████████▌   | 93/104 [00:16<00:01,  5.82it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 4:  90%|█████████████████████████████▊   | 94/104 [00:16<00:01,  5.98it/s, training_loss=0.028]\u001b[A\n",
      "Epoch 4:  90%|█████████████████████████████▊   | 94/104 [00:16<00:01,  5.98it/s, training_loss=0.116]\u001b[A\n",
      "Epoch 4:  91%|██████████████████████████████▏  | 95/104 [00:16<00:01,  5.93it/s, training_loss=0.116]\u001b[A\n",
      "Epoch 4:  91%|██████████████████████████████▏  | 95/104 [00:16<00:01,  5.93it/s, training_loss=0.074]\u001b[A\n",
      "Epoch 4:  92%|██████████████████████████████▍  | 96/104 [00:16<00:01,  5.90it/s, training_loss=0.074]\u001b[A\n",
      "Epoch 4:  92%|██████████████████████████████▍  | 96/104 [00:16<00:01,  5.90it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 4:  93%|██████████████████████████████▊  | 97/104 [00:16<00:01,  5.87it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 4:  93%|██████████████████████████████▊  | 97/104 [00:16<00:01,  5.87it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 4:  94%|███████████████████████████████  | 98/104 [00:16<00:01,  5.97it/s, training_loss=0.303]\u001b[A\n",
      "Epoch 4:  94%|███████████████████████████████  | 98/104 [00:16<00:01,  5.97it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 4:  95%|███████████████████████████████▍ | 99/104 [00:16<00:00,  5.92it/s, training_loss=0.138]\u001b[A\n",
      "Epoch 4:  95%|███████████████████████████████▍ | 99/104 [00:17<00:00,  5.92it/s, training_loss=0.112]\u001b[A\n",
      "Epoch 4:  96%|██████████████████████████████▊ | 100/104 [00:17<00:00,  5.89it/s, training_loss=0.112]\u001b[A\n",
      "Epoch 4:  96%|██████████████████████████████▊ | 100/104 [00:17<00:00,  5.89it/s, training_loss=0.039]\u001b[A\n",
      "Epoch 4:  97%|███████████████████████████████ | 101/104 [00:17<00:00,  5.87it/s, training_loss=0.039]\u001b[A\n",
      "Epoch 4:  97%|███████████████████████████████ | 101/104 [00:17<00:00,  5.87it/s, training_loss=0.118]\u001b[A\n",
      "Epoch 4:  98%|███████████████████████████████▍| 102/104 [00:17<00:00,  5.85it/s, training_loss=0.118]\u001b[A\n",
      "Epoch 4:  98%|███████████████████████████████▍| 102/104 [00:17<00:00,  5.85it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 4:  99%|███████████████████████████████▋| 103/104 [00:17<00:00,  5.84it/s, training_loss=0.114]\u001b[A\n",
      "Epoch 4:  99%|███████████████████████████████▋| 103/104 [00:17<00:00,  5.84it/s, training_loss=0.131]\u001b[A\n",
      "Epoch 4: 100%|████████████████████████████████| 104/104 [00:17<00:00,  6.25it/s, training_loss=0.131]\u001b[A\n",
      "Epoch Progress:  10%|████▉                                            | 4/40 [01:10<10:34, 17.62s/it]\u001b[A\n",
      "Epoch 5:   0%|                                                               | 0/104 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:   0%|                                          | 0/104 [00:00<?, ?it/s, training_loss=0.065]\u001b[A\n",
      "Epoch 5:   1%|▎                                 | 1/104 [00:00<00:17,  6.04it/s, training_loss=0.065]\u001b[A\n",
      "Epoch 5:   1%|▎                                 | 1/104 [00:00<00:17,  6.04it/s, training_loss=0.080]\u001b[A\n",
      "Epoch 5:   2%|▋                                 | 2/104 [00:00<00:17,  5.91it/s, training_loss=0.080]\u001b[A\n",
      "Epoch 5:   2%|▋                                 | 2/104 [00:00<00:17,  5.91it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:   3%|▉                                 | 3/104 [00:00<00:17,  5.81it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:   3%|▉                                 | 3/104 [00:00<00:17,  5.81it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 5:   4%|█▎                                | 4/104 [00:00<00:17,  5.82it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 5:   4%|█▎                                | 4/104 [00:00<00:17,  5.82it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 5:   5%|█▋                                | 5/104 [00:00<00:17,  5.82it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 5:   5%|█▋                                | 5/104 [00:01<00:17,  5.82it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:   6%|█▉                                | 6/104 [00:01<00:17,  5.64it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:   6%|█▉                                | 6/104 [00:01<00:17,  5.64it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:   7%|██▎                               | 7/104 [00:01<00:16,  5.87it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:   7%|██▎                               | 7/104 [00:01<00:16,  5.87it/s, training_loss=0.024]\u001b[A\n",
      "Epoch 5:   8%|██▌                               | 8/104 [00:01<00:16,  5.85it/s, training_loss=0.024]\u001b[A\n",
      "Epoch 5:   8%|██▌                               | 8/104 [00:01<00:16,  5.85it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:   9%|██▉                               | 9/104 [00:01<00:16,  5.68it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:   9%|██▉                               | 9/104 [00:01<00:16,  5.68it/s, training_loss=0.063]\u001b[A\n",
      "Epoch 5:  10%|███▏                             | 10/104 [00:01<00:16,  5.75it/s, training_loss=0.063]\u001b[A\n",
      "Epoch 5:  10%|███▏                             | 10/104 [00:01<00:16,  5.75it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 5:  11%|███▍                             | 11/104 [00:01<00:15,  5.91it/s, training_loss=0.020]\u001b[A\n",
      "Epoch 5:  11%|███▍                             | 11/104 [00:02<00:15,  5.91it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 5:  12%|███▊                             | 12/104 [00:02<00:15,  5.88it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 5:  12%|███▊                             | 12/104 [00:02<00:15,  5.88it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 5:  12%|████▏                            | 13/104 [00:02<00:15,  5.70it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 5:  12%|████▏                            | 13/104 [00:02<00:15,  5.70it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 5:  13%|████▍                            | 14/104 [00:02<00:15,  5.74it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 5:  13%|████▍                            | 14/104 [00:02<00:15,  5.74it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  14%|████▊                            | 15/104 [00:02<00:15,  5.79it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  14%|████▊                            | 15/104 [00:02<00:15,  5.79it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  15%|█████                            | 16/104 [00:02<00:15,  5.83it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  15%|█████                            | 16/104 [00:02<00:15,  5.83it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 5:  16%|█████▍                           | 17/104 [00:02<00:14,  5.99it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 5:  16%|█████▍                           | 17/104 [00:03<00:14,  5.99it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 5:  17%|█████▋                           | 18/104 [00:03<00:14,  5.94it/s, training_loss=0.212]\u001b[A\n",
      "Epoch 5:  17%|█████▋                           | 18/104 [00:03<00:14,  5.94it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  18%|██████                           | 19/104 [00:03<00:15,  5.64it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  18%|██████                           | 19/104 [00:03<00:15,  5.64it/s, training_loss=0.069]\u001b[A\n",
      "Epoch 5:  19%|██████▎                          | 20/104 [00:03<00:14,  5.82it/s, training_loss=0.069]\u001b[A\n",
      "Epoch 5:  19%|██████▎                          | 20/104 [00:03<00:14,  5.82it/s, training_loss=0.015]\u001b[A\n",
      "Epoch 5:  20%|██████▋                          | 21/104 [00:03<00:14,  5.82it/s, training_loss=0.015]\u001b[A\n",
      "Epoch 5:  20%|██████▋                          | 21/104 [00:03<00:14,  5.82it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 5:  21%|██████▉                          | 22/104 [00:03<00:14,  5.72it/s, training_loss=0.125]\u001b[A\n",
      "Epoch 5:  21%|██████▉                          | 22/104 [00:03<00:14,  5.72it/s, training_loss=0.089]\u001b[A\n",
      "Epoch 5:  22%|███████▎                         | 23/104 [00:03<00:14,  5.74it/s, training_loss=0.089]\u001b[A\n",
      "Epoch 5:  22%|███████▎                         | 23/104 [00:04<00:14,  5.74it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 5:  23%|███████▌                         | 24/104 [00:04<00:13,  5.76it/s, training_loss=0.012]\u001b[A\n",
      "Epoch 5:  23%|███████▌                         | 24/104 [00:04<00:13,  5.76it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  24%|███████▉                         | 25/104 [00:04<00:14,  5.58it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  24%|███████▉                         | 25/104 [00:04<00:14,  5.58it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 5:  25%|████████▎                        | 26/104 [00:04<00:14,  5.51it/s, training_loss=0.034]\u001b[A\n",
      "Epoch 5:  25%|████████▎                        | 26/104 [00:04<00:14,  5.51it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 5:  26%|████████▌                        | 27/104 [00:04<00:14,  5.45it/s, training_loss=0.010]\u001b[A\n",
      "Epoch 5:  26%|████████▌                        | 27/104 [00:04<00:14,  5.45it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  27%|████████▉                        | 28/104 [00:04<00:14,  5.42it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  27%|████████▉                        | 28/104 [00:05<00:14,  5.42it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  28%|█████████▏                       | 29/104 [00:05<00:13,  5.40it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  28%|█████████▏                       | 29/104 [00:05<00:13,  5.40it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  29%|█████████▌                       | 30/104 [00:05<00:13,  5.49it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  29%|█████████▌                       | 30/104 [00:05<00:13,  5.49it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  30%|█████████▊                       | 31/104 [00:05<00:13,  5.45it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  30%|█████████▊                       | 31/104 [00:05<00:13,  5.45it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  31%|██████████▏                      | 32/104 [00:05<00:13,  5.53it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  31%|██████████▏                      | 32/104 [00:05<00:13,  5.53it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 5:  32%|██████████▍                      | 33/104 [00:05<00:13,  5.13it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 5:  32%|██████████▍                      | 33/104 [00:06<00:13,  5.13it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 5:  33%|██████████▊                      | 34/104 [00:06<00:15,  4.58it/s, training_loss=0.048]\u001b[A\n",
      "Epoch 5:  33%|██████████▊                      | 34/104 [00:06<00:15,  4.58it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  34%|███████████                      | 35/104 [00:06<00:15,  4.58it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  34%|███████████                      | 35/104 [00:06<00:15,  4.58it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 5:  35%|███████████▍                     | 36/104 [00:06<00:17,  3.92it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 5:  35%|███████████▍                     | 36/104 [00:07<00:17,  3.92it/s, training_loss=0.033]\u001b[A\n",
      "Epoch 5:  36%|███████████▋                     | 37/104 [00:07<00:18,  3.57it/s, training_loss=0.033]\u001b[A\n",
      "Epoch 5:  36%|███████████▋                     | 37/104 [00:07<00:18,  3.57it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  37%|████████████                     | 38/104 [00:07<00:19,  3.41it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  37%|████████████                     | 38/104 [00:07<00:19,  3.41it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  38%|████████████▍                    | 39/104 [00:07<00:20,  3.23it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  38%|████████████▍                    | 39/104 [00:08<00:20,  3.23it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 5:  38%|████████████▋                    | 40/104 [00:08<00:19,  3.20it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 5:  38%|████████████▋                    | 40/104 [00:08<00:19,  3.20it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 5:  39%|█████████████                    | 41/104 [00:08<00:20,  3.09it/s, training_loss=0.005]\u001b[A\n",
      "Epoch 5:  39%|█████████████                    | 41/104 [00:08<00:20,  3.09it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  40%|█████████████▎                   | 42/104 [00:08<00:19,  3.11it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  40%|█████████████▎                   | 42/104 [00:09<00:19,  3.11it/s, training_loss=0.065]\u001b[A\n",
      "Epoch 5:  41%|█████████████▋                   | 43/104 [00:09<00:19,  3.07it/s, training_loss=0.065]\u001b[A\n",
      "Epoch 5:  41%|█████████████▋                   | 43/104 [00:09<00:19,  3.07it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  42%|█████████████▉                   | 44/104 [00:09<00:19,  3.05it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  42%|█████████████▉                   | 44/104 [00:09<00:19,  3.05it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 5:  43%|██████████████▎                  | 45/104 [00:09<00:19,  3.04it/s, training_loss=0.004]\u001b[A\n",
      "Epoch 5:  43%|██████████████▎                  | 45/104 [00:09<00:19,  3.04it/s, training_loss=0.121]\u001b[A\n",
      "Epoch 5:  44%|██████████████▌                  | 46/104 [00:09<00:19,  3.05it/s, training_loss=0.121]\u001b[A\n",
      "Epoch 5:  44%|██████████████▌                  | 46/104 [00:10<00:19,  3.05it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 5:  45%|██████████████▉                  | 47/104 [00:10<00:19,  2.99it/s, training_loss=0.009]\u001b[A\n",
      "Epoch 5:  45%|██████████████▉                  | 47/104 [00:10<00:19,  2.99it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  46%|███████████████▏                 | 48/104 [00:10<00:18,  3.03it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  46%|███████████████▏                 | 48/104 [00:10<00:18,  3.03it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 5:  47%|███████████████▌                 | 49/104 [00:10<00:18,  3.04it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 5:  47%|███████████████▌                 | 49/104 [00:11<00:18,  3.04it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  48%|███████████████▊                 | 50/104 [00:11<00:17,  3.00it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  48%|███████████████▊                 | 50/104 [00:11<00:17,  3.00it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  49%|████████████████▏                | 51/104 [00:11<00:17,  2.98it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  49%|████████████████▏                | 51/104 [00:12<00:17,  2.98it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 5:  50%|████████████████▌                | 52/104 [00:12<00:17,  2.98it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 5:  50%|████████████████▌                | 52/104 [00:12<00:17,  2.98it/s, training_loss=0.091]\u001b[A\n",
      "Epoch 5:  51%|████████████████▊                | 53/104 [00:12<00:17,  2.93it/s, training_loss=0.091]\u001b[A\n",
      "Epoch 5:  51%|████████████████▊                | 53/104 [00:12<00:17,  2.93it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 5:  52%|█████████████████▏               | 54/104 [00:12<00:16,  2.96it/s, training_loss=0.046]\u001b[A\n",
      "Epoch 5:  52%|█████████████████▏               | 54/104 [00:13<00:16,  2.96it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 5:  53%|█████████████████▍               | 55/104 [00:13<00:16,  2.98it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 5:  53%|█████████████████▍               | 55/104 [00:13<00:16,  2.98it/s, training_loss=0.130]\u001b[A\n",
      "Epoch 5:  54%|█████████████████▊               | 56/104 [00:13<00:15,  3.00it/s, training_loss=0.130]\u001b[A\n",
      "Epoch 5:  54%|█████████████████▊               | 56/104 [00:13<00:15,  3.00it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  55%|██████████████████               | 57/104 [00:13<00:15,  3.03it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  55%|██████████████████               | 57/104 [00:14<00:15,  3.03it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 5:  56%|██████████████████▍              | 58/104 [00:14<00:15,  3.04it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 5:  56%|██████████████████▍              | 58/104 [00:14<00:15,  3.04it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 5:  57%|██████████████████▋              | 59/104 [00:14<00:14,  3.05it/s, training_loss=0.021]\u001b[A\n",
      "Epoch 5:  57%|██████████████████▋              | 59/104 [00:14<00:14,  3.05it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 5:  58%|███████████████████              | 60/104 [00:14<00:14,  3.06it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 5:  58%|███████████████████              | 60/104 [00:14<00:14,  3.06it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 5:  59%|███████████████████▎             | 61/104 [00:14<00:14,  3.02it/s, training_loss=0.105]\u001b[A\n",
      "Epoch 5:  59%|███████████████████▎             | 61/104 [00:15<00:14,  3.02it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 5:  60%|███████████████████▋             | 62/104 [00:15<00:13,  3.06it/s, training_loss=0.013]\u001b[A\n",
      "Epoch 5:  60%|███████████████████▋             | 62/104 [00:15<00:13,  3.06it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  61%|███████████████████▉             | 63/104 [00:15<00:13,  3.06it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  61%|███████████████████▉             | 63/104 [00:15<00:13,  3.06it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  62%|████████████████████▎            | 64/104 [00:15<00:13,  3.05it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  62%|████████████████████▎            | 64/104 [00:16<00:13,  3.05it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  62%|████████████████████▋            | 65/104 [00:16<00:13,  2.98it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  62%|████████████████████▋            | 65/104 [00:16<00:13,  2.98it/s, training_loss=0.037]\u001b[A\n",
      "Epoch 5:  63%|████████████████████▉            | 66/104 [00:16<00:12,  3.01it/s, training_loss=0.037]\u001b[A\n",
      "Epoch 5:  63%|████████████████████▉            | 66/104 [00:16<00:12,  3.01it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 5:  64%|█████████████████████▎           | 67/104 [00:16<00:12,  3.05it/s, training_loss=0.137]\u001b[A\n",
      "Epoch 5:  64%|█████████████████████▎           | 67/104 [00:17<00:12,  3.05it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 5:  65%|█████████████████████▌           | 68/104 [00:17<00:11,  3.05it/s, training_loss=0.017]\u001b[A\n",
      "Epoch 5:  65%|█████████████████████▌           | 68/104 [00:17<00:11,  3.05it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  66%|█████████████████████▉           | 69/104 [00:17<00:11,  2.98it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  66%|█████████████████████▉           | 69/104 [00:17<00:11,  2.98it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  67%|██████████████████████▏          | 70/104 [00:17<00:11,  3.03it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  67%|██████████████████████▏          | 70/104 [00:18<00:11,  3.03it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  68%|██████████████████████▌          | 71/104 [00:18<00:10,  3.03it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  68%|██████████████████████▌          | 71/104 [00:18<00:10,  3.03it/s, training_loss=0.204]\u001b[A\n",
      "Epoch 5:  69%|██████████████████████▊          | 72/104 [00:18<00:10,  3.00it/s, training_loss=0.204]\u001b[A\n",
      "Epoch 5:  69%|██████████████████████▊          | 72/104 [00:18<00:10,  3.00it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  70%|███████████████████████▏         | 73/104 [00:18<00:10,  3.01it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  70%|███████████████████████▏         | 73/104 [00:19<00:10,  3.01it/s, training_loss=0.075]\u001b[A\n",
      "Epoch 5:  71%|███████████████████████▍         | 74/104 [00:19<00:10,  2.97it/s, training_loss=0.075]\u001b[A\n",
      "Epoch 5:  71%|███████████████████████▍         | 74/104 [00:19<00:10,  2.97it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  72%|███████████████████████▊         | 75/104 [00:19<00:09,  2.97it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  72%|███████████████████████▊         | 75/104 [00:19<00:09,  2.97it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 5:  73%|████████████████████████         | 76/104 [00:19<00:09,  3.00it/s, training_loss=0.045]\u001b[A\n",
      "Epoch 5:  73%|████████████████████████         | 76/104 [00:20<00:09,  3.00it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  74%|████████████████████████▍        | 77/104 [00:20<00:08,  3.03it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  74%|████████████████████████▍        | 77/104 [00:20<00:08,  3.03it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  75%|████████████████████████▊        | 78/104 [00:20<00:08,  3.01it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  75%|████████████████████████▊        | 78/104 [00:20<00:08,  3.01it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  76%|█████████████████████████        | 79/104 [00:20<00:08,  3.03it/s, training_loss=0.003]\u001b[A\n",
      "Epoch 5:  76%|█████████████████████████        | 79/104 [00:21<00:08,  3.03it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  77%|█████████████████████████▍       | 80/104 [00:21<00:07,  3.04it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  77%|█████████████████████████▍       | 80/104 [00:21<00:07,  3.04it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  78%|█████████████████████████▋       | 81/104 [00:21<00:07,  3.07it/s, training_loss=0.002]\u001b[A\n",
      "Epoch 5:  78%|█████████████████████████▋       | 81/104 [00:21<00:07,  3.07it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  79%|██████████████████████████       | 82/104 [00:21<00:07,  3.05it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  79%|██████████████████████████       | 82/104 [00:22<00:07,  3.05it/s, training_loss=0.014]\u001b[A\n",
      "Epoch 5:  80%|██████████████████████████▎      | 83/104 [00:22<00:06,  3.05it/s, training_loss=0.014]\u001b[A\n",
      "Epoch 5:  80%|██████████████████████████▎      | 83/104 [00:22<00:06,  3.05it/s, training_loss=0.113]\u001b[A\n",
      "Epoch 5:  81%|██████████████████████████▋      | 84/104 [00:22<00:06,  3.06it/s, training_loss=0.113]\u001b[A\n",
      "Epoch 5:  81%|██████████████████████████▋      | 84/104 [00:22<00:06,  3.06it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  82%|██████████████████████████▉      | 85/104 [00:22<00:06,  3.04it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  82%|██████████████████████████▉      | 85/104 [00:23<00:06,  3.04it/s, training_loss=0.029]\u001b[A\n",
      "Epoch 5:  83%|███████████████████████████▎     | 86/104 [00:23<00:05,  3.04it/s, training_loss=0.029]\u001b[A\n",
      "Epoch 5:  83%|███████████████████████████▎     | 86/104 [00:23<00:05,  3.04it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  84%|███████████████████████████▌     | 87/104 [00:23<00:05,  3.03it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  84%|███████████████████████████▌     | 87/104 [00:23<00:05,  3.03it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  85%|███████████████████████████▉     | 88/104 [00:23<00:05,  3.00it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  85%|███████████████████████████▉     | 88/104 [00:24<00:05,  3.00it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  86%|████████████████████████████▏    | 89/104 [00:24<00:04,  3.03it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  86%|████████████████████████████▏    | 89/104 [00:24<00:04,  3.03it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  87%|████████████████████████████▌    | 90/104 [00:24<00:04,  3.01it/s, training_loss=0.000]\u001b[A\n",
      "Epoch 5:  87%|████████████████████████████▌    | 90/104 [00:24<00:04,  3.01it/s, training_loss=0.065]\u001b[A\n",
      "Epoch 5:  88%|████████████████████████████▉    | 91/104 [00:24<00:04,  2.99it/s, training_loss=0.065]\u001b[A\n",
      "Epoch 5:  88%|████████████████████████████▉    | 91/104 [00:25<00:04,  2.99it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 5:  88%|█████████████████████████████▏   | 92/104 [00:25<00:04,  2.97it/s, training_loss=0.055]\u001b[A\n",
      "Epoch 5:  88%|█████████████████████████████▏   | 92/104 [00:25<00:04,  2.97it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  89%|█████████████████████████████▌   | 93/104 [00:25<00:03,  3.01it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  89%|█████████████████████████████▌   | 93/104 [00:25<00:03,  3.01it/s, training_loss=0.145]\u001b[A\n",
      "Epoch 5:  90%|█████████████████████████████▊   | 94/104 [00:25<00:03,  3.03it/s, training_loss=0.145]\u001b[A\n",
      "Epoch 5:  90%|█████████████████████████████▊   | 94/104 [00:26<00:03,  3.03it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  91%|██████████████████████████████▏  | 95/104 [00:26<00:02,  3.04it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  91%|██████████████████████████████▏  | 95/104 [00:26<00:02,  3.04it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  92%|██████████████████████████████▍  | 96/104 [00:26<00:02,  3.05it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  92%|██████████████████████████████▍  | 96/104 [00:26<00:02,  3.05it/s, training_loss=0.130]\u001b[A\n",
      "Epoch 5:  93%|██████████████████████████████▊  | 97/104 [00:26<00:02,  3.00it/s, training_loss=0.130]\u001b[A\n",
      "Epoch 5:  93%|██████████████████████████████▊  | 97/104 [00:27<00:02,  3.00it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  94%|███████████████████████████████  | 98/104 [00:27<00:01,  3.01it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  94%|███████████████████████████████  | 98/104 [00:27<00:01,  3.01it/s, training_loss=0.042]\u001b[A\n",
      "Epoch 5:  95%|███████████████████████████████▍ | 99/104 [00:27<00:01,  3.04it/s, training_loss=0.042]\u001b[A\n",
      "Epoch 5:  95%|███████████████████████████████▍ | 99/104 [00:27<00:01,  3.04it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 5:  96%|██████████████████████████████▊ | 100/104 [00:27<00:01,  3.00it/s, training_loss=0.007]\u001b[A\n",
      "Epoch 5:  96%|██████████████████████████████▊ | 100/104 [00:28<00:01,  3.00it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  97%|███████████████████████████████ | 101/104 [00:28<00:00,  3.03it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  97%|███████████████████████████████ | 101/104 [00:28<00:00,  3.03it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  98%|███████████████████████████████▍| 102/104 [00:28<00:00,  3.06it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 5:  98%|███████████████████████████████▍| 102/104 [00:28<00:00,  3.06it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 5:  99%|███████████████████████████████▋| 103/104 [00:28<00:00,  3.05it/s, training_loss=0.006]\u001b[A\n",
      "Epoch 5:  99%|███████████████████████████████▋| 103/104 [00:29<00:00,  3.05it/s, training_loss=0.026]\u001b[A\n",
      "Epoch 5: 100%|████████████████████████████████| 104/104 [00:29<00:00,  3.30it/s, training_loss=0.026]\u001b[A\n",
      "Epoch Progress:  12%|██████▏                                          | 5/40 [01:39<12:41, 21.77s/it]\u001b[A\n",
      "Epoch 6:   0%|                                                               | 0/104 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:   0%|                                          | 0/104 [00:00<?, ?it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 6:   1%|▎                                 | 1/104 [00:00<00:37,  2.77it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 6:   1%|▎                                 | 1/104 [00:00<00:37,  2.77it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 6:   2%|▋                                 | 2/104 [00:00<00:35,  2.86it/s, training_loss=0.022]\u001b[A\n",
      "Epoch 6:   2%|▋                                 | 2/104 [00:01<00:35,  2.86it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 6:   3%|▉                                 | 3/104 [00:01<00:34,  2.96it/s, training_loss=0.078]\u001b[A\n",
      "Epoch 6:   3%|▉                                 | 3/104 [00:01<00:34,  2.96it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 6:   4%|█▎                                | 4/104 [00:01<00:33,  3.03it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 6:   4%|█▎                                | 4/104 [00:01<00:33,  3.03it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 6:   5%|█▋                                | 5/104 [00:01<00:33,  2.98it/s, training_loss=0.180]\u001b[A\n",
      "Epoch 6:   5%|█▋                                | 5/104 [00:01<00:33,  2.98it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 6:   6%|█▉                                | 6/104 [00:02<00:32,  3.06it/s, training_loss=0.001]\u001b[A\n",
      "Epoch 6:   6%|█▉                                | 6/104 [00:02<00:32,  3.06it/s, training_loss=0.043]\u001b[A\n",
      "Epoch 6:   7%|██▎                               | 7/104 [00:02<00:32,  3.03it/s, training_loss=0.043]\u001b[A"
     ]
    }
   ],
   "source": [
    " #install packages\n",
    "!pip install transformers\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import logging\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def custom_train_test_split(df, test_size=0.2, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df[['ade', 'soc_code']]\n",
    "    y = df['label']\n",
    "    \n",
    "    # Identify classes and their counts\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    # Identify small classes\n",
    "    small_classes = classes[counts < 5]\n",
    "    \n",
    "    # Initialize lists for train and test sets\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    # Handle small classes separately\n",
    "    for cls in small_classes:\n",
    "        cls_mask = (y == cls)\n",
    "        cls_X = X[cls_mask]\n",
    "        cls_y = y[cls_mask]\n",
    "        cls_idx = df.index[cls_mask].tolist()\n",
    "        \n",
    "        if len(cls_X) == 1:\n",
    "            # If only one instance, put it in test set\n",
    "            test_indices.append(cls_idx[0])\n",
    "        else:\n",
    "            # Randomly choose one instance for testing\n",
    "            test_idx = np.random.choice(len(cls_X))\n",
    "            test_indices.append(cls_idx[test_idx])\n",
    "            \n",
    "            # Remaining instances go to training\n",
    "            train_indices.extend(np.delete(cls_idx, test_idx))\n",
    "    \n",
    "    # Combine the small class data into test and train sets\n",
    "    test_indices = np.array(test_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "    \n",
    "    X_test = df.loc[test_indices]\n",
    "    y_test = X_test['label']\n",
    "    \n",
    "    X_train = df.loc[train_indices]\n",
    "    y_train = X_train['label']\n",
    "    \n",
    "    # Handle large classes with stratified split\n",
    "    large_class_mask = ~np.isin(y, small_classes)\n",
    "    X_large = X[large_class_mask]\n",
    "    y_large = y[large_class_mask]\n",
    "    \n",
    "    X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "        X_large, y_large, test_size=test_size, random_state=random_state, stratify=y_large\n",
    "    )\n",
    "    \n",
    "    # Combine large class data with the small class data\n",
    "    X_train = pd.concat([X_train, X_train_large], axis=0)\n",
    "    y_train = pd.concat([y_train, y_train_large], axis=0)\n",
    "    \n",
    "    X_test = pd.concat([X_test, X_test_large], axis=0)\n",
    "    y_test = pd.concat([y_test, y_test_large], axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "#evaluation\n",
    "def accuracy_per_class(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    accuracy_dict = {}\n",
    "    count_dict = {}\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = pred_flat[labels_flat == label]\n",
    "        y_true = labels_flat[labels_flat == label]\n",
    "        accuracy_dict[label] = np.sum(y_preds == y_true) / len(y_true) if len(y_true) > 0 else 0\n",
    "        count_dict[label] = len(y_true)\n",
    "\n",
    "    return accuracy_dict, count_dict\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "\n",
    "# Function to calculate precision, recall, and F1 for each label\n",
    "def calculate_metrics(predictions, true_vals):\n",
    "    pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score per label\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels_flat, pred_flat, average=None, labels=np.unique(labels_flat))\n",
    "    \n",
    "    return precision, recall, f1\n",
    "    \n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='combine_top3_training_40ep_16bs_5e-5lr_log_Ev_SMM.txt', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class TQDMLoggingWrapper(tqdm):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logger = logger\n",
    "\n",
    "    def display(self, msg=None, pos=None):\n",
    "        if msg is not None:\n",
    "            self.logger.info(msg)\n",
    "        super().display(msg, pos)\n",
    "\n",
    "    def update(self, n=1):\n",
    "        super().update(n)\n",
    "        desc = self.format_dict.get('desc', 'No description')\n",
    "        postfix = self.format_dict.get('postfix', '')\n",
    "        self.logger.info(f'{desc} - {postfix}')\n",
    "\n",
    "    def set_description(self, desc=None, refresh=True):\n",
    "        super().set_description(desc, refresh)\n",
    "        if desc:\n",
    "            self.logger.info(f'Set description: {desc}')\n",
    "\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "#Read data from git:\n",
    "#https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\n",
    "# URL of the CSV file\n",
    "cadec_csv_url = \"https://raw.githubusercontent.com/FANMISUA/TweetAENormalization/main/ADENormalization/Data/CADEC/3.csv\"\n",
    "# read data from smm4h\n",
    "smm4h_csv_url = \"https://raw.githubusercontent.com/FANMISUA/ADE_Norm/main/Data/smm4h_soc.tsv\"\n",
    "\n",
    "top3SMM4H = [10037175, 10018065,10029205]\n",
    "top3label_dict = {\n",
    "    10037175: 0,\n",
    "    10018065: 1,\n",
    "    10029205: 2\n",
    "}\n",
    "\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"ade\", \"soc_code\"]\n",
    "smm4h_all = pd.read_csv(smm4h_csv_url,names=column_names, sep = '\\t', header=None)\n",
    "\n",
    "smm4h_all = smm4h_all[smm4h_all['soc_code'] != 0]\n",
    "smm4h_all['soc_code'] = pd.to_numeric(smm4h_all['soc_code'], errors='coerce').astype('Int64')\n",
    "smm4h_unique = smm4h_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# print(\"smm4h data:\",smm4h_all.shape)\n",
    "smm4h_soc_code_counts = smm4h_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",smm4h_soc_code_counts)\n",
    "# Filter DataFrame\n",
    "smm4h_filtered_data3 = smm4h_unique[smm4h_unique['soc_code'].isin(top3SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "top3inSMM4H = smm4h_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "top3inSMM4H.loc[:, 'label'] = top3inSMM4H['soc_code'].map(top3label_dict)\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "column_names = [\"TT\", \"llt_code\", \"ade\", \"soc_code\"]\n",
    "cadec_all = pd.read_csv(cadec_csv_url,names=column_names, header=None)\n",
    "\n",
    "# Remove duplicate rows based on the 'ade' column\n",
    "cadec_unique = cadec_all.drop_duplicates(subset='ade')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(\"clean cadec data:\",cadec_unique.shape)\n",
    "# Count occurrences of each 'soc_code'\n",
    "cadec_soc_code_counts = cadec_unique['soc_code'].value_counts()\n",
    "# Sort the counts from high to low and print the result\n",
    "# print(\"SOC count in CADEC: \",cadec_soc_code_counts)\n",
    "\n",
    "\n",
    "# Filter DataFrame\n",
    "cadec_filtered_data3 = cadec_unique[cadec_unique['soc_code'].isin(top3SMM4H)]\n",
    "# filtered_data6 = cadec_unique[cadec_unique['soc_code'].isin(top6SMM4H)]\n",
    "\n",
    "# Select only the Term and SOC columns\n",
    "CADECtop3inSMM4H = cadec_filtered_data3[['ade', 'soc_code']]\n",
    "# CADECtop6inSMM4H = filtered_data6[['ade', 'soc_code']]\n",
    "\n",
    "\n",
    "# print(\"CADEC top3 in SMM4H:\",CADECtop3inSMM4H)\n",
    "\n",
    "# For SMM4H data\n",
    "df1 = top3inSMM4H.copy()\n",
    "df1.loc[:, 'label'] = df1['soc_code'].map(top3label_dict)\n",
    "\n",
    "# For CADEC data\n",
    "df2 = CADECtop3inSMM4H.copy()\n",
    "df2.loc[:, 'label'] = df2['soc_code'].map(top3label_dict)\n",
    "\n",
    "print(\"SMM4H top 3\",df1)\n",
    "print(\"CADEC top 3\",df2)\n",
    "\n",
    "\n",
    "# Define the random seeds and other parameters\n",
    "seed_values = list(range(2, 42, 2))\n",
    "batch_size = 16\n",
    "epochs = 40\n",
    "learningrate = 5e-5\n",
    "\n",
    "# Placeholder for accuracies\n",
    "all_accuracies = {label: [] for label in range(len(top3label_dict))}\n",
    "\n",
    "# Initialize dictionaries to hold metrics for each seed\n",
    "# seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': []} for seed_val in seed_values}\n",
    "seed_metrics = {seed_val: {'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'confusion_matrix': []} for seed_val in seed_values}\n",
    "\n",
    "\n",
    "# Main loop over seed values\n",
    "for seed_val in seed_values:\n",
    "    # Set seeds\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Data preparation\n",
    "    # Apply the custom train-test split\n",
    "    # X_train, X_val, y_train, y_val = custom_train_test_split(df, test_size=0.2, random_state=seed_val)\n",
    "    \n",
    "    # # Add data_type column\n",
    "    # df['data_type'] = 'not_set'\n",
    "    # df.loc[X_train.index, 'data_type'] = 'train'\n",
    "    # df.loc[X_val.index, 'data_type'] = 'val'\n",
    "\n",
    "    # logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    # print(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    # Perform train-test split on df1\n",
    "    X_train_idx1, X_val_idx1, y_train1, y_val1 = custom_train_test_split(df1, test_size=0.2, random_state=seed_val)\n",
    "\n",
    "    # Perform train-test split on df2\n",
    "    X_train_idx2, X_val_idx2, y_train2, y_val2 = custom_train_test_split(df2, test_size=0.2, random_state=seed_val)\n",
    "\n",
    "    #  set the 'data_type' column for df1 and df2\n",
    "    df1['data_type'] = 'not_set'\n",
    "    df2['data_type'] = 'not_set'\n",
    "\n",
    "    df1.loc[df1.index.isin(X_train_idx1.index), 'data_type'] = 'train'\n",
    "    df1.loc[df1.index.isin(X_val_idx1.index), 'data_type'] = 'val'\n",
    "\n",
    "    df2.loc[df2.index.isin(X_train_idx2.index), 'data_type'] = 'train'\n",
    "    df2.loc[df2.index.isin(X_val_idx2.index), 'data_type'] = 'val2'\n",
    "\n",
    "\n",
    "    # If you want to combine df1 and df2 into a single dataframe:\n",
    "    df = pd.concat([df1, df2])\n",
    "    print(\"df: \",df)\n",
    "    logger.info(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "    print(df.groupby(['soc_code', 'label', 'data_type']).count())\n",
    "\n",
    "    \n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'train'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == 'val'].ade.values,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type == 'train'].label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type == 'val'].label.values)\n",
    "\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(top3label_dict), output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n",
    "    dataloader_validation = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=batch_size)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learningrate, eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train) * epochs)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    logger.info(f\"Device used: {device}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in TQDMLoggingWrapper(range(1, epochs+1), desc='Epoch Progress'):\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = TQDMLoggingWrapper(dataloader_train, desc=f'Epoch {epoch}', leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "\n",
    "        # torch.save(model.state_dict(), f'./ADENorm_top3_epoch_{epoch}.model')\n",
    "\n",
    "        logger.info(f'\\nEpoch {epoch}')\n",
    "        loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "        logger.info(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_vals.flatten(), np.argmax(predictions, axis=1).flatten(), average=None, labels=np.unique(true_vals.flatten()))\n",
    "\n",
    "     # Ensure that you use `true_vals` for the true labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1).flatten()\n",
    "    true_labels = true_vals.flatten()\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    seed_metrics[seed_val]['accuracy'] = accuracy\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=np.unique(true_labels))\n",
    "    seed_metrics[seed_val]['confusion_matrix'] = conf_matrix\n",
    "    \n",
    "    for label in np.unique(true_vals):\n",
    "        seed_metrics[seed_val]['precision'].append((label, precision[label]))\n",
    "        seed_metrics[seed_val]['recall'].append((label, recall[label]))\n",
    "        seed_metrics[seed_val]['f1'].append((label, f1[label]))\n",
    "\n",
    "# Write the precision, recall, F1 scores, and seed values to a file\n",
    "with open('combine_top3_20times_results_with_seeds_Ev_SMM.txt', 'w') as f:\n",
    "    f.write('Seed\\tLabel\\tPrecision\\tRecall\\tF1\\tAccuracy\\n')\n",
    "    for seed_val in seed_values:\n",
    "        for label, precision_val in seed_metrics[seed_val]['precision']:\n",
    "            recall_val = next(val for lbl, val in seed_metrics[seed_val]['recall'] if lbl == label)\n",
    "            f1_val = next(val for lbl, val in seed_metrics[seed_val]['f1'] if lbl == label)\n",
    "            accuracy = seed_metrics[seed_val]['accuracy']\n",
    "            f.write(f'{seed_val}\\t{label}\\t{precision_val:.4f}\\t{recall_val:.4f}\\t{f1_val:.4f}\\t{accuracy:.4f}\\n')\n",
    "\n",
    "        # Save the confusion matrix\n",
    "        f.write(f'\\nConfusion Matrix for Seed {seed_val}:\\n')\n",
    "        f.write(np.array2string(seed_metrics[seed_val]['confusion_matrix'], separator=', '))\n",
    "        f.write('\\n')\n",
    "    \n",
    "# Initialize lists to hold precision, recall, and f1 values for each label\n",
    "precision_dict, recall_dict, f1_dict = {}, {}, {}\n",
    "\n",
    "# Collect metrics across seeds\n",
    "for seed in seed_metrics:\n",
    "    for label, value in seed_metrics[seed]['precision']:\n",
    "        precision_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['recall']:\n",
    "        recall_dict.setdefault(label, []).append(value)\n",
    "    for label, value in seed_metrics[seed]['f1']:\n",
    "        f1_dict.setdefault(label, []).append(value)\n",
    "\n",
    "# Compute mean and std for precision, recall, and f1\n",
    "labels = sorted(precision_dict.keys())\n",
    "precision_mean = [np.mean(precision_dict[label]) for label in labels]\n",
    "precision_std = [np.std(precision_dict[label]) for label in labels]\n",
    "recall_mean = [np.mean(recall_dict[label]) for label in labels]\n",
    "recall_std = [np.std(recall_dict[label]) for label in labels]\n",
    "f1_mean = [np.mean(f1_dict[label]) for label in labels]\n",
    "f1_std = [np.std(f1_dict[label]) for label in labels]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(labels))  # label indices\n",
    "width = 0.25  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Bar plots with mean values\n",
    "bars_precision = ax.bar(x - width, precision_mean, width, label='Precision', color='b')\n",
    "bars_recall = ax.bar(x, recall_mean, width, label='Recall', color='g')\n",
    "bars_f1 = ax.bar(x + width, f1_mean, width, label='F1 Score', color='r')\n",
    "\n",
    "# Annotate bars with mean and std values\n",
    "# Annotate bars with mean and std values, with smaller font size\n",
    "for bars, means, stds in zip([bars_precision, bars_recall, bars_f1],\n",
    "                             [precision_mean, recall_mean, f1_mean],\n",
    "                             [precision_std, recall_std, f1_std]):\n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height,\n",
    "                f'{mean:.2f}\\n±{std:.2f}', ha='center', va='bottom', fontsize=8)  # Smaller font size\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_title('Mean and Standard Deviation of Precision, Recall, and F1 Score by Label')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Set y-axis limit to [0, 1]\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Move legend outside the plot\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the plot to fit the legend\n",
    "plt.savefig('combine_top3_20times_results_plot_Ev_SMM.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea03ac-e3ce-45d7-ad9a-448c51fc523d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
